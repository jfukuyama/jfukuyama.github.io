<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Julia Fukuyama" />
  <meta name="date" content="2018-10-25" />
  <title>Stat 470/670 Lecture 20: Model-building with a moderate number of variables</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Stat 470/670 Lecture 20: Model-building with a moderate number of variables</h1>
  <p class="author">
Julia Fukuyama
  </p>
  <p class="date">October 25, 2018</p>
</div>
<div id="section" class="slide section level2">
<h1></h1>
<ul>
<li><p>Over the next couple of weeks we'll get back to building models, and we'll look at models for different kinds of responses (binary, count, categorical).</p></li>
<li><p>We won't have that much more on high-dimensional data, although it will come up in a couple of later lectures.</p></li>
<li><p>Today we'll look at model-building with a moderate number of variables. Today will be linear models, but the ideas carry over to logistic regression and the generalized linear models we'll talk about next week.</p></li>
</ul>
</div>
<div id="data-prostate-cancer" class="slide section level2">
<h1>Data: Prostate Cancer</h1>
<p>We have a data set containing clinical data on patients who were about to receive a radical prostatectomy. The relevant variables are:</p>
<ul>
<li><p><code>lcavol</code>: log cancer volume</p></li>
<li><p><code>lweight</code>: log prostate weight</p></li>
<li><p><code>age</code>: Age in years</p></li>
<li><p><code>lbph</code>: log of the amount of benign prostatic hyperplasia</p></li>
<li><p><code>svi</code>: Seminal vesicle invasion, a measure of how advanced the cancer is.</p></li>
<li><p><code>lcp</code>: log of capsular penetration</p></li>
<li><p><code>gleason</code>: A numeric vector</p></li>
<li><p><code>pgg45</code>: Percent of Gleason score 4 or 5</p></li>
<li><p><code>lpsa</code>: log of the concentration of prostate-stimulating antigen.</p></li>
</ul>
<p>We are primarily interested in <code>lpsa</code>, which is used as a marker for prostate cancer. We would like to know whether and how it is related to the other variables we have available to us.</p>
</div>
<div id="first-we-look-at-the-data" class="slide section level2">
<h1>First we look at the data</h1>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ElemStatLearn)
<span class="kw">data</span>(prostate)
<span class="kw">summary</span>(prostate)</code></pre>
<pre><code>##      lcavol           lweight           age             lbph        
##  Min.   :-1.3471   Min.   :2.375   Min.   :41.00   Min.   :-1.3863  
##  1st Qu.: 0.5128   1st Qu.:3.376   1st Qu.:60.00   1st Qu.:-1.3863  
##  Median : 1.4469   Median :3.623   Median :65.00   Median : 0.3001  
##  Mean   : 1.3500   Mean   :3.629   Mean   :63.87   Mean   : 0.1004  
##  3rd Qu.: 2.1270   3rd Qu.:3.876   3rd Qu.:68.00   3rd Qu.: 1.5581  
##  Max.   : 3.8210   Max.   :4.780   Max.   :79.00   Max.   : 2.3263  
##       svi              lcp             gleason          pgg45       
##  Min.   :0.0000   Min.   :-1.3863   Min.   :6.000   Min.   :  0.00  
##  1st Qu.:0.0000   1st Qu.:-1.3863   1st Qu.:6.000   1st Qu.:  0.00  
##  Median :0.0000   Median :-0.7985   Median :7.000   Median : 15.00  
##  Mean   :0.2165   Mean   :-0.1794   Mean   :6.753   Mean   : 24.38  
##  3rd Qu.:0.0000   3rd Qu.: 1.1787   3rd Qu.:7.000   3rd Qu.: 40.00  
##  Max.   :1.0000   Max.   : 2.9042   Max.   :9.000   Max.   :100.00  
##       lpsa           train        
##  Min.   :-0.4308   Mode :logical  
##  1st Qu.: 1.7317   FALSE:30       
##  Median : 2.5915   TRUE :67       
##  Mean   : 2.4784                  
##  3rd Qu.: 3.0564                  
##  Max.   : 5.5829</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">prostate =<span class="st"> </span>prostate %&gt;%<span class="st"> </span><span class="kw">select</span>(-<span class="st">&quot;train&quot;</span>)
<span class="kw">ggpairs</span>(prostate, <span class="dt">progress =</span> <span class="ot">FALSE</span>)</code></pre>
<div class="figure">
<img src="lecture-20-fig/unnamed-chunk-1-1.png" />
</div>
<p>From the <code>ggpairs</code> plot, we see that the distributions of the variables are reasonable symmetrical, not that skewed, and that there is at least some relationship between most of the variables and <code>lpsa</code>.</p>
</div>
<div class="slide section level2">

<p>We can also use principal components as a visualization of the variables:</p>
<pre class="sourceCode r"><code class="sourceCode r">prostate_prcomp =<span class="st"> </span><span class="kw">prcomp</span>(prostate, <span class="dt">scale. =</span> <span class="ot">TRUE</span>)
<span class="kw">ggbiplot</span>(prostate_prcomp)</code></pre>
<div class="figure">
<img src="lecture-20-fig/unnamed-chunk-2-1.png" />
</div>
<p>From the principal components plot, we see that there are two groups of variables, one group of which are all highly correlated with <code>lpsa</code>.</p>
</div>
<div class="slide section level2">

<p>And a side note: this data set has already been transformed for you. Many of the variables are logged versions of what were presumably the raw measurements. This is actually a really important step: models with the logged variables perform substantially better than models with the raw variables, as we can see if we un-transform:</p>
<pre class="sourceCode r"><code class="sourceCode r">prostate_unlogged =<span class="st"> </span>prostate %&gt;%
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">cavol =</span> <span class="kw">exp</span>(lcavol), <span class="dt">weight =</span> <span class="kw">exp</span>(lweight), <span class="dt">bph =</span> <span class="kw">exp</span>(lbph), <span class="dt">cp =</span> <span class="kw">exp</span>(lcp)) %&gt;%
<span class="st">    </span><span class="kw">select</span>(-<span class="st">&quot;lcavol&quot;</span>, -<span class="st">&quot;lweight&quot;</span>, -<span class="st">&quot;lbph&quot;</span>, -<span class="st">&quot;lcp&quot;</span>)
<span class="kw">summary</span>(<span class="kw">lm</span>(lpsa ~<span class="st"> </span>., <span class="dt">data =</span> prostate))</code></pre>
<pre><code>## 
## Call:
## lm(formula = lpsa ~ ., data = prostate)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.76644 -0.35510 -0.00328  0.38087  1.55770 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.181561   1.320568   0.137  0.89096    
## lcavol       0.564341   0.087833   6.425 6.55e-09 ***
## lweight      0.622020   0.200897   3.096  0.00263 ** 
## age         -0.021248   0.011084  -1.917  0.05848 .  
## lbph         0.096713   0.057913   1.670  0.09848 .  
## svi          0.761673   0.241176   3.158  0.00218 ** 
## lcp         -0.106051   0.089868  -1.180  0.24115    
## gleason      0.049228   0.155341   0.317  0.75207    
## pgg45        0.004458   0.004365   1.021  0.31000    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6995 on 88 degrees of freedom
## Multiple R-squared:  0.6634, Adjusted R-squared:  0.6328 
## F-statistic: 21.68 on 8 and 88 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(lpsa ~<span class="st"> </span>., <span class="dt">data =</span> prostate_unlogged))</code></pre>
<pre><code>## 
## Call:
## lm(formula = lpsa ~ ., data = prostate_unlogged)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.78876 -0.40705 -0.00634  0.46725  1.82792 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.759219   1.210444   0.627  0.53214    
## age         -0.008545   0.012087  -0.707  0.48144    
## svi          0.769166   0.270646   2.842  0.00557 ** 
## gleason      0.130183   0.166701   0.781  0.43694    
## pgg45        0.005412   0.004567   1.185  0.23927    
## cavol        0.073661   0.014479   5.087 2.03e-06 ***
## weight       0.012511   0.005322   2.351  0.02095 *  
## bph          0.056512   0.034547   1.636  0.10546    
## cp          -0.040047   0.034216  -1.170  0.24499    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.764 on 88 degrees of freedom
## Multiple R-squared:  0.5985, Adjusted R-squared:  0.562 
## F-statistic:  16.4 on 8 and 88 DF,  p-value: 1.329e-14</code></pre>
</div>
<div class="slide section level2">

<p>If we had started off with the raw data, we would have seen that we should log-transform some of the variables by looking at their marginal distributions: the variables that were transformed started off quite skewed, and the transformation got rid of the skewness.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggpairs</span>(prostate_unlogged, <span class="dt">progress =</span> <span class="ot">FALSE</span>)</code></pre>
<div class="figure">
<img src="lecture-20-fig/unnamed-chunk-4-1.png" />
</div>
</div>
<div id="linear-model-with-all-the-predictors" class="slide section level2">
<h1>Linear model with all the predictors</h1>
<p>As a first step, we can fit a linear model with all the predictors and look at the results. We see that a lot of the coefficients are within the margin of error of zero, which suggests to us that a model with fewer predictors would do better.</p>
<pre class="sourceCode r"><code class="sourceCode r">prostate_lm =<span class="st"> </span><span class="kw">lm</span>(lpsa ~<span class="st"> </span>., <span class="dt">data =</span> prostate)
prostate_coefs =<span class="st"> </span><span class="kw">tidy</span>(prostate_lm, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)
<span class="kw">ggplot</span>(prostate_coefs[-<span class="dv">1</span>, ], <span class="kw">aes</span>(<span class="dt">x =</span> estimate, <span class="dt">y =</span> term, <span class="dt">xmin =</span> conf.low, <span class="dt">xmax =</span> conf.high)) +
<span class="st">    </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_errorbarh</span>() +<span class="st"> </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>)</code></pre>
<div class="figure">
<img src="lecture-20-fig/unnamed-chunk-5-1.png" />
</div>
</div>
<div id="best-subset" class="slide section level2">
<h1>Best subset</h1>
<p>There are a lot of ways of doing variable selection for linear models.</p>
<p>You might have heard of forward stepwise or backward stepwise regression, where predictors are added to or subtracted from the model one at a time, stopping when adding a new predictor doesn't seem to help (for forward stepwise) or when subtracting an existing predictor seems to hurt too much (for backward stepwise).</p>
<p>Part of the reason why people used forward or backward stepwise regression was due to computational cost though, and what they really wanted was to find the set of predictors that gave the best model.</p>
<p>With eight predictors and a laptop, we can actually just look through all the subsets and see which model performs best.</p>
<p>This is what the <code>regsubsets</code> in the package <code>leaps</code> does for you.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(leaps)
prostate_leaps =<span class="st"> </span><span class="kw">regsubsets</span>(lpsa ~<span class="st"> </span>., <span class="dt">data =</span> prostate)
<span class="kw">summary</span>(prostate_leaps)$which</code></pre>
<pre><code>##   (Intercept) lcavol lweight   age  lbph   svi   lcp gleason pgg45
## 1        TRUE   TRUE   FALSE FALSE FALSE FALSE FALSE   FALSE FALSE
## 2        TRUE   TRUE    TRUE FALSE FALSE FALSE FALSE   FALSE FALSE
## 3        TRUE   TRUE    TRUE FALSE FALSE  TRUE FALSE   FALSE FALSE
## 4        TRUE   TRUE    TRUE FALSE  TRUE  TRUE FALSE   FALSE FALSE
## 5        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE   FALSE FALSE
## 6        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE   FALSE  TRUE
## 7        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE   FALSE  TRUE
## 8        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE    TRUE  TRUE</code></pre>
<p>This shows us that the best one-predictor model uses <code>lcavol</code>, the best two-predictor model uses <code>lcavol</code> and <code>lweight</code>, the best three-predictor model uses <code>lcavol</code>, <code>lweight</code>, and <code>svi</code>, and so on.</p>
<p>We can use this set of models as guidance for what variables to include, and build up an interpretable model using some of the tools we've seen earlier in the course.</p>
</div>
<div class="slide section level2">

<p>We start off looking at the best one-predictor model, with <code>lcavol</code> predicting <code>lpsa</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(prostate, <span class="kw">aes</span>(<span class="dt">x =</span> lcavol, <span class="dt">y =</span> lpsa)) +<span class="st"> </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_smooth</span>()</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<div class="figure">
<img src="lecture-20-fig/unnamed-chunk-7-1.png" />
</div>
<p>We see that the relationship is pretty close to linear (a line would go through the entire confidence band of the smoother), and so we're ok with using a linear function of <code>lcavol</code> to predict <code>lpsa</code>.</p>
<p>If there had been a major non-linearity here, we would have wanted to ditch the linear modeling approach and do something non-parametric, maybe loess.</p>
</div>
<div class="slide section level2">

<p>Then we can move to the two-predictor model, and see what the relationship between <code>lcavol</code>, <code>lweight</code>, and <code>lpsa</code> looks like.</p>
<p>We can make a coplot to examine the relationship between <code>lpsa</code> and <code>lcavol</code>, with <code>lweight</code>as the given variable</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(prostate, <span class="kw">aes</span>(<span class="dt">x =</span> lcavol, <span class="dt">y =</span> lpsa)) +<span class="st"> </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="dv">1</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">facet_grid</span>(~<span class="kw">cut_number</span>(lweight, <span class="dt">n =</span> <span class="dv">3</span>))</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<div class="figure">
<img src="lecture-20-fig/unnamed-chunk-8-1.png" />
</div>
<p>Here it seems like there is some non-linearity, but only for the observations with a high value of <code>lweight</code>.</p>
<p>If we had a lot more observations, this might prompt us to move to loess, but since the non-linearity is based on just a few points, we want to see first whether the non-linearity shows up in other graphs as well.</p>
</div>
<div class="slide section level2">

<p>The best three-predictor model identified by <code>leaps</code> included <code>svi</code> in addition to <code>lcavol</code> and <code>lweight</code>, so we next look at those four variables together.</p>
<p><code>svi</code> is binary and there are only 21 cases where <code>svi</code> is equal to 1, we can't make a lot of facets and we don't trust curves that much.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(prostate$svi)</code></pre>
<pre><code>## 
##  0  1 
## 76 21</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">prostate$svi =<span class="st"> </span><span class="kw">recode</span>(prostate$svi, <span class="st">`</span><span class="dt">1</span><span class="st">`</span> =<span class="st"> &quot;Yes&quot;</span>, <span class="st">`</span><span class="dt">0</span><span class="st">`</span> =<span class="st"> &quot;No&quot;</span>)
<span class="kw">ggplot</span>(prostate, <span class="kw">aes</span>(<span class="dt">x =</span> lcavol, <span class="dt">y =</span> lpsa, <span class="dt">group =</span> svi, <span class="dt">color =</span> svi)) +
<span class="st">    </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) +
<span class="st">    </span><span class="kw">facet_wrap</span>(~<span class="kw">cut_number</span>(lweight, <span class="dt">n =</span> <span class="dv">2</span>))</code></pre>
<div class="figure">
<img src="lecture-20-fig/unnamed-chunk-9-1.png" />
</div>
<p>The different slopes suggest an interaction, but again, we don't have that much data.</p>
</div>
<div class="slide section level2">

<p>Next we try looking at the predictors from the four-predictor model. This is pushing the limits of the number of variables we can look at all at once, but we will try.</p>
<p>We can look at the relationship between <code>lpsa</code>, <code>lcavol</code>, and <code>svi</code>, with <code>lweight</code> and <code>lbph</code> as the given variables.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(prostate, <span class="kw">aes</span>(<span class="dt">x =</span> lcavol, <span class="dt">y =</span> lpsa, <span class="dt">group =</span> svi, <span class="dt">color =</span> svi)) +
<span class="st">    </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) +
<span class="st">        </span><span class="kw">facet_grid</span>(<span class="kw">cut_number</span>(lweight, 
    <span class="dt">n =</span> <span class="dv">2</span>) ~<span class="st"> </span><span class="kw">cut</span>(lbph, <span class="dt">breaks =</span> <span class="kw">c</span>(-<span class="dv">2</span>, -<span class="dv">1</span>, <span class="dv">3</span>)))</code></pre>
<div class="figure">
<img src="lecture-20-fig/unnamed-chunk-10-1.png" />
</div>
<p>Based on this visualization, we might not be confident about an interaction: the blue lines vary in slope, but there’s based on very small samples. The red lines have different heights but are similar in slope.</p>
</div>
<div class="slide section level2">

<p>In EDA we’re not always required to find a “best” model, and even if we were we can decide on what best means subjectively.</p>
<p>If you wanted to fit a linear model with <code>lcavol</code>, <code>lweight</code>, and <code>svi</code> as predictors plus interactions, you’re free to do so and then call that &quot;best&quot; because of the complexity you can get out of a relatively small number of variables.</p>
<p>On the other hand, if you want a numerical decision for &quot;best&quot;, you can just find the model that optimizes your favorite criterion. For example, if you like <a href="https://en.wikipedia.org/wiki/Mallows%27s_Cp">Mallow’s Cp</a>, this is available in <code>leaps</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(prostate_leaps)$cp</code></pre>
<pre><code>## [1] 27.406210 14.747299  6.173546  6.185065  5.816804  6.466493  7.100428
## [8]  9.000000</code></pre>
<p>When we use Cp, we look for the model with the lowest value of Cp, so here the five-predictor model is &quot;best&quot;.</p>
<p>The idea behind the Cp statistic is that the RSS underestimates the prediction error in a linear model, and the Cp statistic is a corrected version of the RSS.</p>
</div>
</body>
</html>
