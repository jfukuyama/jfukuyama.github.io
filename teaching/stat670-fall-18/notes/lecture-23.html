<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Julia Fukuyama" />
  <title>Stat 470/670 Lecture 23: Count responses and Poisson regression</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Stat 470/670 Lecture 23: Count responses and Poisson regression</h1>
  <p class="author">
Julia Fukuyama
  </p>
  <p class="date">November 6, 2018</p>
</div>
<div id="stop-and-frisk-data" class="slide section level1">
<h1>Stop and frisk data</h1>
<p>Gelman and Hill have data on police stops in New York City in 1998--1999, during Giuliani's mayoralty. There have been accusations that some ethnic groups have been stopped at rates not justified by either their arrest rate or their location (as measured by precinct.)</p>
<p>The data, with noise added for confidentiality, is at <a href="http://www.stat.columbia.edu/~gelman/arm/examples/police/frisk_with_noise.dat">http://www.stat.columbia.edu/~gelman/arm/examples/police/frisk_with_noise.dat</a></p>
<p>The data gives counts of police stops for all combinations of</p>
<ul>
<li><p><code>precinct</code>: 75 total</p></li>
<li><p><code>eth</code>: Ethnicity of the person stopped, three possibilities (1 = black, 2 = Hispanic, 3 = white), and</p></li>
<li><p><code>crime</code>: The type of crie, four possibilities (1 = violent, 2 = weapons, 3 = property, and 4 = drug)</p></li>
</ul>
<p>This gives a total of <span class="math">\(75 \times 3 \times 4 = 900\)</span> rows.</p>
<p>There are two other variables in the data set:</p>
<ul>
<li><p><code>pop</code>: population of the ethnic group within the precinct, and</p></li>
<li><p><code>past.arrests</code>: the number of arrests of people in that ethnic group in that precinct for that type of crime in 1997.</p></li>
</ul>
</div>
<div class="slide section level1">

<p>The first few rows of this file are a description, so we tell R to skip these when reading the data.</p>
<pre class="sourceCode r"><code class="sourceCode r">frisk =<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;http://www.stat.columbia.edu/~gelman/arm/examples/police/frisk_with_noise.dat&quot;</span>, <span class="dt">skip =</span> <span class="dv">6</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)
<span class="kw">nrow</span>(frisk)</code></pre>
<pre><code>## [1] 900</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(frisk)</code></pre>
<pre><code>##      stops           pop          past.arrests       precinct       eth   
##  Min.   :   0   Min.   :   321   Min.   :   0.0   Min.   : 1   Min.   :1  
##  1st Qu.:  26   1st Qu.:  6844   1st Qu.:  53.0   1st Qu.:19   1st Qu.:1  
##  Median :  72   Median : 18004   Median : 124.0   Median :38   Median :2  
##  Mean   : 146   Mean   : 30105   Mean   : 262.8   Mean   :38   Mean   :2  
##  3rd Qu.: 173   3rd Qu.: 46669   3rd Qu.: 287.5   3rd Qu.:57   3rd Qu.:3  
##  Max.   :1755   Max.   :184345   Max.   :2655.0   Max.   :75   Max.   :3  
##      crime     
##  Min.   :1.00  
##  1st Qu.:1.75  
##  Median :2.50  
##  Mean   :2.50  
##  3rd Qu.:3.25  
##  Max.   :4.00</code></pre>
<p>Having numerical ethnicity is annoying, so recode:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
frisk$eth =<span class="st"> </span><span class="kw">recode_factor</span>(frisk$eth, <span class="st">`</span><span class="dt">1</span><span class="st">`</span> =<span class="st"> &quot;black&quot;</span>, <span class="st">`</span><span class="dt">2</span><span class="st">`</span> =<span class="st"> &quot;Hispanic&quot;</span>, <span class="st">`</span><span class="dt">3</span><span class="st">`</span> =<span class="st"> &quot;white&quot;</span>)</code></pre>
</div>
<div class="slide section level1">

<p>For the purposes of this lecture, we'll ignore the type of crime, and aggregate the number of stops and past arrests over all four types. If you're interested though, you should try a model that includes type of crime as well and see if anything changes.</p>
<p>Aggregating in this way gives us 225 rows (75 precincts by three ethnic groups):</p>
<pre class="sourceCode r"><code class="sourceCode r">frisk.sum =<span class="st"> </span>frisk %&gt;%
<span class="st">    </span><span class="kw">group_by</span>(precinct, eth) %&gt;%
<span class="st">    </span><span class="kw">summarise</span>(<span class="dt">stops =</span> <span class="kw">sum</span>(stops), <span class="dt">past.arrests =</span> <span class="kw">sum</span>(past.arrests), <span class="dt">pop =</span> <span class="kw">mean</span>(pop))
<span class="kw">nrow</span>(frisk.sum)</code></pre>
<pre><code>## [1] 225</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(frisk.sum)</code></pre>
<pre><code>##     precinct        eth         stops         past.arrests 
##  Min.   : 1   black   :75   Min.   :   7.0   Min.   :  16  
##  1st Qu.:19   Hispanic:75   1st Qu.: 133.0   1st Qu.: 312  
##  Median :38   white   :75   Median : 385.0   Median : 571  
##  Mean   :38                 Mean   : 584.1   Mean   :1051  
##  3rd Qu.:57                 3rd Qu.: 824.0   3rd Qu.:1467  
##  Max.   :75                 Max.   :2771.0   Max.   :5667  
##       pop        
##  Min.   :   321  
##  1st Qu.:  6844  
##  Median : 18004  
##  Mean   : 30105  
##  3rd Qu.: 46669  
##  Max.   :184345</code></pre>
</div>
<div class="slide section level1">

<p>Let's first draw some pictures.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(frisk.sum, <span class="kw">aes</span>(<span class="dt">x =</span> stops, <span class="dt">color =</span> eth, <span class="dt">fill =</span> eth)) +
<span class="st">    </span><span class="kw">geom_histogram</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">2800</span>, <span class="dv">50</span>)) +<span class="st"> </span><span class="kw">facet_wrap</span>(~eth, <span class="dt">ncol =</span> <span class="dv">1</span>)</code></pre>
<div class="figure">
<img src="lecture-23-fig/unnamed-chunk-4-1.png" />
</div>
<p>Quite clearly, the distributions of stops for black and Hispanic people are very different from the distribution for white people, though there may be multiple explanations for this.</p>
</div>
<div class="slide section level1">

<p>Let's look at the relationship of stops with past arrests. Because of skewness, we log both variables.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(frisk.sum, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">log</span>(past.arrests), <span class="dt">y =</span> <span class="kw">log</span>(stops), <span class="dt">color =</span> eth)) +<span class="st"> </span><span class="kw">geom_point</span>()</code></pre>
<div class="figure">
<img src="lecture-23-fig/unnamed-chunk-5-1.png" />
</div>
<p>There's certainly a relationship. The question is whether the relationship between the two variables is sufficient to explain the differences between the stops of the three ethnic groups. You could get at this just by adding smoother for the three groups:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(frisk.sum, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">log</span>(past.arrests), <span class="dt">y =</span> <span class="kw">log</span>(stops), <span class="dt">group =</span> eth, <span class="dt">color =</span> eth)) +<span class="st"> </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">degree =</span> <span class="dv">1</span>))</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<div class="figure">
<img src="lecture-23-fig/unnamed-chunk-6-1.png" />
</div>
<p>Since this is an important topic, however, we should be a bit more careful and construct a model.</p>
</div>
<div id="poisson-regression" class="slide section level1">
<h1>Poisson regression</h1>
<p>We'll model this data using (at first) <em>Poisson regression</em>, another form of generalized linear model.</p>
<p>Poisson regression is used instead of standard linear regression when the response variable is a count (0, 1, 2, etc.) instead of a real number.</p>
<p>You <em>could</em> use standard linear regression here (if you put the numbers into <code>lm</code> in R it will give you results), but Poisson regression can be better because counts tend to have a <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a>, and Poisson distributed variables have a fixed relationship between the mean and the variance.</p>
<p>If <span class="math">\(y \sim \text{Pois}(\lambda)\)</span>, then <span class="math">\(E(y) = \lambda\)</span> and <span class="math">\(\text{Var}(y) =\lambda\)</span>. This relationship is inconsistent with the homoskedasticity assumptions of linear regression.</p>
</div>
<div class="slide section level1">

<p>In a standard Poisson regression, the response has a Poisson distribution with the <em>log</em> of the expected value given by a linear function of the predictors.</p>
<p>In the single-variable case: <span class="math">\[
\log(E[Y \mid x]) = \beta_0 + \beta_1 x
\]</span> and <span class="math">\[
Y \sim \text{Pois}(E[Y \mid x])
\]</span></p>
</div>
<div class="slide section level1">

<p>We'll start off with a Poission regression model that's much too simple, and build up to a more useful one.</p>
<p>The simplest model just treats each number of stops as a realization of a Poisson random variable.</p>
<pre class="sourceCode r"><code class="sourceCode r">constant.glm =<span class="st"> </span><span class="kw">glm</span>(stops ~<span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> poisson, <span class="dt">data =</span> frisk.sum)
<span class="kw">summary</span>(constant.glm)</code></pre>
<pre><code>## 
## Call:
## glm(formula = stops ~ 1, family = poisson, data = frisk.sum)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -33.049  -22.552   -8.788    9.343   65.227  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) 6.370053   0.002758    2309   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 123333  on 224  degrees of freedom
## Residual deviance: 123333  on 224  degrees of freedom
## AIC: 125041
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div class="slide section level1">

<p>By now you might be sick of all the cruft that gets displayed when we use <code>summary()</code> on a GLM. Let's use Gelman et al.'s <code>display()</code> function in package <code>arm</code> instead.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install.packages(&quot;arm&quot;)</span>
<span class="kw">library</span>(arm)
<span class="kw">display</span>(constant.glm)</code></pre>
<pre><code>## glm(formula = stops ~ 1, family = poisson, data = frisk.sum)
##             coef.est coef.se
## (Intercept) 6.37     0.00   
## ---
##   n = 225, k = 1
##   residual deviance = 123332.5, null deviance = 123332.5 (difference = 0.0)</code></pre>
<p>This pares away most of the low value information. We see the coefficent estimate (on the log scale) is 6.37, which gives <span class="math">\(e^{6.37} = 584\)</span> on the original scale. That is, the number of stops for each ethnic group within each precinct is modeled as a random variable with distribution</p>
<p><span class="math">\[
\textrm{Poisson}(584).
\]</span></p>
<p>The other number to keep track of is the (residual) <em>deviance</em>. Low deviance is good, as long as you're not overfitting. In particular, every time you add a degree of freedom, you should expect to reduce the deviance by 1 if you're just adding random noise. So if you're not overfitting when you fit a complex model, you should expect to reduce the deviance by more than you increase the degrees of freedom.</p>
</div>
<div class="slide section level1">

<p>Now this model is obviously inadequate. We might, for example, think that the number of stops for an ethnic groups in a precinct should be proportional to the number of arrests for that ethnicity-precinct (though this is controversial.) In a GLM, we can model this using an <em>offset</em>:</p>
<pre class="sourceCode r"><code class="sourceCode r">offset.glm =<span class="st"> </span><span class="kw">glm</span>(stops ~<span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> poisson, <span class="dt">offset =</span> <span class="kw">log</span>(past.arrests), <span class="dt">data =</span> frisk.sum)
<span class="kw">display</span>(offset.glm)</code></pre>
<pre><code>## glm(formula = stops ~ 1, family = poisson, data = frisk.sum, 
##     offset = log(past.arrests))
##             coef.est coef.se
## (Intercept) -0.59     0.00  
## ---
##   n = 225, k = 1
##   residual deviance = 46120.3, null deviance = 46120.3 (difference = 0.0)</code></pre>
<p>Since the linear predictor is on the log scale, the offset also has to be logged. This gives the following model for each precinct/race combination:</p>
<p><span class="math">\[
\log[E(\textrm{stops}|\textrm{past arrests})] = -0.59 + \log(\textrm{past arrests})
\]</span> or (taking the exponential of both sides) <span class="math">\[
E(\textrm{stops}|\textrm{past arrests}) = e^{-0.59 + \log(\textrm{past arrests})} = 0.56 \times \textrm{past arrests}
\]</span></p>
</div>
<div class="slide section level1">

<p>To check this, we look at the predicted number of stops for precinct/race combinations with 10, 100, and 1000 past arrests respectively:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">augment</span>(offset.glm,
        <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">past.arrests =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>)),
        <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)</code></pre>
<pre><code>## # A tibble: 3 x 3
##   past.arrests .fitted .se.fit
## *        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1           10    5.56  0.0153
## 2          100   55.6   0.153 
## 3         1000  556.    1.53</code></pre>
<p>Our model has a much lower deviance than the constant model, so we've improved the fit by a lot.</p>
</div>
<div class="slide section level1">

<p>Now we want to see what happens if we add ethnic group as a predictor. Ethnic group is categorical, so we use it as a factor.</p>
<pre class="sourceCode r"><code class="sourceCode r">eth.glm =<span class="st"> </span><span class="kw">glm</span>(stops ~<span class="st"> </span>eth, <span class="dt">family =</span> poisson, <span class="dt">offset =</span> <span class="kw">log</span>(past.arrests), <span class="dt">data =</span> frisk.sum)
<span class="kw">display</span>(eth.glm)</code></pre>
<pre><code>## glm(formula = stops ~ eth, family = poisson, data = frisk.sum, 
##     offset = log(past.arrests))
##             coef.est coef.se
## (Intercept) -0.59     0.00  
## ethHispanic  0.07     0.01  
## ethwhite    -0.16     0.01  
## ---
##   n = 225, k = 3
##   residual deviance = 45437.4, null deviance = 46120.3 (difference = 682.9)</code></pre>
<p>Note that &quot;past arrests&quot; doesn't have a coefficient: the model assumes that expected stops are proportional to past arrests (where the constant of proportionality may depend on other stuff.) The deviance has dropped substantially again. On the log scale, we have additive terms for the offset and for ethnicity (relative to black, which is taken as the baseline due to alphabetical order.) On the original scale, the terms are multiplicative, and we can combine the offset and ethnicity terms to get a coefficient for each ethnicity. That is, the model is now</p>
<p><span class="math">\[
E(\textrm{stops}) = \textrm{multiplier for ethnic group} \times \textrm{past arrests}
\]</span></p>
<p>where the multipliers are</p>
<pre class="sourceCode r"><code class="sourceCode r">eth.co =<span class="st"> </span><span class="kw">coefficients</span>(eth.glm)
multipliers =<span class="st"> </span><span class="kw">exp</span>(<span class="kw">c</span>(eth.co[<span class="dv">1</span>], eth.co[<span class="dv">1</span>] +<span class="st"> </span>eth.co[<span class="dv">2</span>], eth.co[<span class="dv">1</span>] +<span class="st"> </span>eth.co[<span class="dv">3</span>]))
<span class="kw">print</span>(multipliers)</code></pre>
<pre><code>## (Intercept) (Intercept) (Intercept) 
##   0.5553894   0.5957836   0.4725238</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">eth.coef =<span class="st"> </span><span class="kw">tidy</span>(eth.glm)$estimate

multipliers =<span class="st"> </span><span class="kw">exp</span>(<span class="kw">c</span>(eth.coef[<span class="dv">1</span>],
    eth.coef[<span class="dv">1</span>] +<span class="st"> </span>eth.coef[<span class="dv">2</span>],
    eth.coef[<span class="dv">1</span>] +<span class="st"> </span>eth.coef[<span class="dv">3</span>]))
multipliers</code></pre>
<pre><code>## [1] 0.5553894 0.5957836 0.4725238</code></pre>
<p>for black, Hispanic, and white respectively. We can check this using <code>augment()</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">augment</span>(eth.glm,
        <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">past.arrests =</span> <span class="dv">1000</span>, <span class="dt">eth =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;Hispanic&quot;</span>, <span class="st">&quot;white&quot;</span>)),
        <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)</code></pre>
<pre><code>## # A tibble: 3 x 4
##   past.arrests eth      .fitted .se.fit
## *        &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;
## 1         1000 black       555.    2.10
## 2         1000 Hispanic    596.    2.82
## 3         1000 white       473.    3.63</code></pre>
</div>
<div class="slide section level1">

<p>So far we have shown that black and Hispanic people were stopped at a proportionately higher fraction of their arrest rate compared to white people.</p>
<p>However, as the data isn't from a randomized experiment, there may be confounding --- it could be that black and Hispanic people generally live in precincts with higher stop rates. (Whether this is in itself evidence of bias is again, controversial.)</p>
<p>Since this is exploratory work, we won't attempt to prove cause-and-effect, but we'll see whether we can simply explain the results by including a precinct variable. If we can, then the NYPD might argue that minorities are only stopped more often because they, perhaps coincidentally, tend to live in precincts with high stop rates.</p>
<pre class="sourceCode r"><code class="sourceCode r">precinct.glm =<span class="st"> </span><span class="kw">glm</span>(stops ~<span class="st"> </span>eth +<span class="st"> </span><span class="kw">factor</span>(precinct), <span class="dt">family =</span> poisson, <span class="dt">offset =</span> <span class="kw">log</span>(past.arrests), <span class="dt">data =</span> frisk.sum)</code></pre>
<p>We won't print out the full results because we now have a coefficient for each precinct. Let's just first check the deviance has gone down significantly:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">deviance</span>(eth.glm)</code></pre>
<pre><code>## [1] 45437.35</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">deviance</span>(precinct.glm)</code></pre>
<pre><code>## [1] 3427.14</code></pre>
</div>
<div class="slide section level1">

<p>Now look at the first few coefficients (and their standard errors):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tidy</span>(precinct.glm)</code></pre>
<pre><code>## # A tibble: 77 x 5
##    term              estimate std.error statistic   p.value
##    &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1 (Intercept)        -1.38     0.0510     -27.0  7.21e-161
##  2 ethHispanic         0.0102   0.00680      1.50 1.34e-  1
##  3 ethwhite           -0.419    0.00943    -44.4  0.       
##  4 factor(precinct)2  -0.149    0.0740      -2.01 4.41e-  2
##  5 factor(precinct)3   0.560    0.0568       9.87 5.87e- 23
##  6 factor(precinct)4   1.21     0.0575      21.0  3.03e- 98
##  7 factor(precinct)5   0.283    0.0568       4.98 6.34e-  7
##  8 factor(precinct)6   1.14     0.0580      19.7  1.72e- 86
##  9 factor(precinct)7   0.218    0.0643       3.39 6.96e-  4
## 10 factor(precinct)8  -0.391    0.0569      -6.87 6.51e- 12
## # ... with 67 more rows</code></pre>
<p>After controlling for precinct, the differences between the white and minority coefficients becomes even bigger.</p>
</div>
<div id="checking-the-model" class="slide section level1">
<h1>Checking the model</h1>
<p>As usual, our first plot for checking the model is to plot the residuals against the fitted values on the response (original) scale, and see what happens.</p>
<pre class="sourceCode r"><code class="sourceCode r">precinct.glm.df =<span class="st"> </span><span class="kw">augment</span>(precinct.glm, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">type.residuals =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">ggplot</span>(precinct.glm.df, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .resid)) +<span class="st"> </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="dv">1</span>, <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">degree =</span> <span class="dv">1</span>))</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<div class="figure">
<img src="lecture-23-fig/unnamed-chunk-18-1.png" />
</div>
<p>The smoother we added isn't flat, but that could just be because the residuals are heteroskedastic: they spread out dramatically. The heteroskedasticity is not a bug: Poissons are supposed to be heteroskedastic. Recall that a Poisson<span class="math">\((\lambda)\)</span> random variable has variance <span class="math">\(\lambda\)</span> and standard deviation <span class="math">\(\sqrt{\lambda}\)</span>. So the typical size of the residuals should go up as the square root of the fitted value.</p>
</div>
<div class="slide section level1">

<p>To hopefully remove this effect, we create <em>standardized residuals</em> by dividing the raw residuals by the square root of the fitted value. We plot these against the log fitted values to reduce the distortions caused by skewness.</p>
<pre class="sourceCode r"><code class="sourceCode r">precinct.std.resid =<span class="st"> </span>precinct.glm.df$.resid /<span class="st"> </span><span class="kw">sqrt</span>(precinct.glm.df$.fitted)
precinct.glm.df$.std.resid =<span class="st"> </span>precinct.std.resid
<span class="kw">ggplot</span>(precinct.glm.df, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">log</span>(.fitted), <span class="dt">y =</span> .std.resid)) +<span class="st"> </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="dv">1</span>, <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">degree =</span> <span class="dv">1</span>))</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<div class="figure">
<img src="lecture-23-fig/unnamed-chunk-19-1.png" />
</div>
<p>Note that there are other kinds of residuals that you can plot for glms, (try <code>?residuals.glm</code> to see them). The idea behind the other types is roughly the same as what we've done here, they aim to standardize residuals according to the variance expected in the residuals by the fit.</p>
<p>This is better, though far from perfect. There's still some nonlinearity left in the smoother, though the amount is relatively small. If prediction was the goal, a nonparametric model would probably provide an improvement.</p>
</div>
<div id="overdispersion" class="slide section level1">
<h1>Overdispersion</h1>
<p>If we care about more than just the conditional expectation, however, we find a bigger problem. If the Poisson model were correct, the standardized residuals should be on a similar scale to the standard normal -- that is, the vast majority should be within <span class="math">\(\pm 2\)</span>. From the previous graph, that's clearly not the case.</p>
<p>We need to measure the <em>overdispersion</em> in the data. We could do a formal <span class="math">\(\chi^2\)</span> test for overdispersion, but instead, let's calculate the typical size of the squared residuals. (When we &quot;average&quot;, we divide the sum by the residual degrees of freedom.) If the Poisson model is correct, this should be close to 1. If it's much more than 1, we need a better model.</p>
<pre class="sourceCode r"><code class="sourceCode r">overdispersion =<span class="st"> </span><span class="kw">sum</span>(precinct.std.resid^<span class="dv">2</span>) /<span class="st"> </span><span class="kw">df.residual</span>(precinct.glm)
overdispersion</code></pre>
<pre><code>## [1] 21.88505</code></pre>
<p>This is much more than 1. In fact, this happens a lot with counts -- the data is often more dispersed than the Poisson model.</p>
</div>
<div id="how-bad-is-it" class="slide section level1">
<h1>How bad is it?</h1>
<p>We know there are problems with our model. But are they so bad that we can't draw conclusions from it?</p>
<p>One simple way of checking is to simulate a fake set of data, and see if it closely resembles the actual set.</p>
<p>For a Poisson model, this is easy. We know according to the model, each observation is a realization of a Poisson random variable, whose parameter is given by the fitted value. Then we can use <code>rpois()</code> to do simulation and do numerical summaries and plots.</p>
<pre class="sourceCode r"><code class="sourceCode r">precinct.fits =<span class="st"> </span><span class="kw">augment</span>(precinct.glm, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)$.fitted
sim1 =<span class="st"> </span><span class="kw">rpois</span>(<span class="kw">nrow</span>(frisk.sum), <span class="dt">lambda =</span> precinct.fits)
<span class="kw">summary</span>(frisk.sum$stops)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     7.0   133.0   385.0   584.1   824.0  2771.0</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(sim1)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     9.0   158.0   379.0   583.6   817.0  2728.0</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">sim.df =<span class="st"> </span><span class="kw">data.frame</span>(frisk.sum, sim1)
<span class="kw">library</span>(tidyr)
sim.long =<span class="st"> </span>sim.df %&gt;%<span class="st"> </span><span class="kw">gather</span>(type, number, <span class="kw">c</span>(<span class="st">&quot;stops&quot;</span>, <span class="st">&quot;sim1&quot;</span>))
<span class="kw">ggplot</span>(sim.long, <span class="kw">aes</span>(<span class="dt">x =</span> number)) +<span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">2800</span>, <span class="dv">50</span>)) +<span class="st"> </span><span class="kw">facet_wrap</span>(~type, <span class="dt">ncol =</span> <span class="dv">1</span>)</code></pre>
<div class="figure">
<img src="lecture-23-fig/unnamed-chunk-21-1.png" />
</div>
</div>
<div class="slide section level1">

<p>If we look at the histograms, there doesn't seem to be much difference. But what happens if we fit a model to the simulated data and look at its residuals? We'll find these and do a two-sample QQ plot of them against the original residuals.</p>
<pre class="sourceCode r"><code class="sourceCode r">precinct.sim =<span class="st"> </span><span class="kw">glm</span>(sim1 ~<span class="st"> </span>eth +<span class="st"> </span><span class="kw">factor</span>(precinct), <span class="dt">family =</span> poisson, <span class="dt">offset =</span> <span class="kw">log</span>(past.arrests), <span class="dt">data =</span> sim.df)
resid.df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">real.resid =</span> <span class="kw">augment</span>(precinct.glm, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)$.resid,
                      <span class="dt">sim.resid =</span> <span class="kw">augment</span>(precinct.sim, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)$.resid)
<span class="kw">ggplot</span>(resid.df) +
<span class="st">    </span><span class="kw">stat_qq</span>(<span class="kw">aes</span>(<span class="dt">sample =</span> sim.resid),
            <span class="dt">distribution =</span> function(p) <span class="kw">quantile</span>(resid.df$real.resid, <span class="dt">probs =</span> p)) +
<span class="st">        </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>) +
<span class="st">        </span><span class="kw">xlab</span>(<span class="st">&quot;real residual quantiles&quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;simulation residual quantiles&quot;</span>)</code></pre>
<div class="figure">
<img src="lecture-23-fig/unnamed-chunk-22-1.png" />
</div>
<p>If the model were correct, this QQ plot should be close to a line through the origin with slope 1, and this is not. The real residuals are much more spread out/have much larger variance than the residuals in the simulation.</p>
<p>The simulation here is overkill, since we understand the Poisson fairly well and already know the data is overdispersed. However, the more complicated your model gets, the more useful this kind of simulation is as a sanity check.</p>
</div>
<div id="fixing-overdispersion" class="slide section level1">
<h1>Fixing overdispersion</h1>
<p>The quickest fix is to use the quasipoisson family instead of the Poisson.</p>
<pre class="sourceCode r"><code class="sourceCode r">precinct.quasi =<span class="st"> </span><span class="kw">glm</span>(stops ~<span class="st"> </span>eth +<span class="st"> </span><span class="kw">factor</span>(precinct), <span class="dt">family =</span> quasipoisson, <span class="dt">offset =</span> <span class="kw">log</span>(past.arrests), <span class="dt">data =</span> frisk.sum)
<span class="kw">tidy</span>(precinct.quasi)</code></pre>
<pre><code>## # A tibble: 77 x 5
##    term              estimate std.error statistic  p.value
##    &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 (Intercept)        -1.38      0.239     -5.78  4.33e- 8
##  2 ethHispanic         0.0102    0.0318     0.320 7.49e- 1
##  3 ethwhite           -0.419     0.0441    -9.49  5.49e-17
##  4 factor(precinct)2  -0.149     0.346     -0.430 6.68e- 1
##  5 factor(precinct)3   0.560     0.266      2.11  3.66e- 2
##  6 factor(precinct)4   1.21      0.269      4.50  1.38e- 5
##  7 factor(precinct)5   0.283     0.266      1.06  2.89e- 1
##  8 factor(precinct)6   1.14      0.272      4.21  4.35e- 5
##  9 factor(precinct)7   0.218     0.301      0.725 4.70e- 1
## 10 factor(precinct)8  -0.391     0.266     -1.47  1.44e- 1
## # ... with 67 more rows</code></pre>
<p>Note that the coefficients look the same as they were in the standard Poisson case. However, their standard errors have been inflated by the square root of their overdispersion. We can confirm that the fitted values haven't changed:</p>
<pre class="sourceCode r"><code class="sourceCode r">precinct.fitted =<span class="st"> </span><span class="kw">augment</span>(precinct.glm, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)$.fitted
quasi.fitted =<span class="st"> </span><span class="kw">augment</span>(precinct.quasi, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)$.fitted
<span class="kw">summary</span>(quasi.fitted -<span class="st"> </span>precinct.fitted)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       0       0       0       0       0       0</code></pre>
<p>So the quasipoission doesn't change the fit, only the variance and the standard errors.</p>
</div>
<div class="slide section level1">

<p>For interpretation, it may be useful to refit the model changing the order of levels in <code>eth</code> to use whites as a baseline.</p>
<pre class="sourceCode r"><code class="sourceCode r">frisk.sum.relev =<span class="st"> </span>frisk.sum %&gt;%<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">eth.releveled =</span> <span class="kw">factor</span>(eth, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;white&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;Hispanic&quot;</span>)))
precinct.quasi2 =<span class="st"> </span><span class="kw">glm</span>(stops ~<span class="st"> </span>eth.releveled +<span class="st"> </span><span class="kw">factor</span>(precinct),
    <span class="dt">family =</span> quasipoisson, <span class="dt">offset =</span> <span class="kw">log</span>(past.arrests),
    <span class="dt">data =</span> frisk.sum.relev)
<span class="kw">tidy</span>(precinct.quasi2)</code></pre>
<pre><code>## # A tibble: 77 x 5
##    term                  estimate std.error statistic  p.value
##    &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 (Intercept)             -1.80     0.241     -7.46  6.81e-12
##  2 eth.releveledblack       0.419    0.0441     9.49  5.49e-17
##  3 eth.releveledHispanic    0.429    0.0449     9.57  3.49e-17
##  4 factor(precinct)2       -0.149    0.346     -0.430 6.68e- 1
##  5 factor(precinct)3        0.560    0.266      2.11  3.66e- 2
##  6 factor(precinct)4        1.21     0.269      4.50  1.38e- 5
##  7 factor(precinct)5        0.283    0.266      1.06  2.89e- 1
##  8 factor(precinct)6        1.14     0.272      4.21  4.35e- 5
##  9 factor(precinct)7        0.218    0.301      0.725 4.70e- 1
## 10 factor(precinct)8       -0.391    0.266     -1.47  1.44e- 1
## # ... with 67 more rows</code></pre>
</div>
<div class="slide section level1">

<p>We now back-transform to get intervals for the stop rates of blacks and Hispanics relative to whites, after adjusting for arrest rates and precinct.</p>
<pre class="sourceCode r"><code class="sourceCode r">## 2 and 3 are the rows corresponding to the black and hispanic coefficients
eth.co =<span class="st"> </span><span class="kw">tidy</span>(precinct.quasi2)[<span class="dv">2</span>:<span class="dv">3</span>,]
eth.co$ethnicity =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Black&quot;</span>, <span class="st">&quot;Hispanic&quot;</span>)
eth.co.plotting =<span class="st"> </span>eth.co %&gt;%
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">estimate_rescaled =</span> <span class="kw">exp</span>(estimate),
           <span class="dt">lower =</span> <span class="kw">exp</span>(estimate -<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>std.error),
           <span class="dt">upper =</span> <span class="kw">exp</span>(estimate +<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>std.error))
<span class="kw">ggplot</span>(eth.co.plotting) +
<span class="st">    </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">x =</span> ethnicity, <span class="dt">y =</span> estimate_rescaled, <span class="dt">ymin =</span> lower, <span class="dt">ymax =</span> upper)) +
<span class="st">    </span><span class="kw">ylim</span>(<span class="dv">1</span>,<span class="dv">2</span>) +
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">1</span>, <span class="dt">slope =</span> <span class="dv">0</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)+
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Ratio of stop rate to that of whites,</span><span class="ch">\n</span><span class="st">adjusted for past arrests and precinct&quot;</span>) +
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Approximate 95% confidence intervals</span><span class="ch">\n</span><span class="st">for NYPD stop rates of minorities&quot;</span>) +
<span class="st">    </span><span class="kw">coord_flip</span>()</code></pre>
<div class="figure">
<img src="lecture-23-fig/unnamed-chunk-26-1.png" />
</div>
<p>The confidence intervals don't include 1. This would be consistent with a hypothesis of bias against minorities, though we should think very carefully about other confounding variables before drawing a firm conclusion (e.g. type of crime, which we ignored.) You should check your model very thoroughly. A definitive answer here requires subject matter knowledge in conjunction with statistics.</p>
</div>
<div id="other-fixes" class="slide section level1">
<h1>Other fixes</h1>
<p>There are lots of alternative approaches:</p>
<ul>
<li>Negative binomial regression is an alternative to the quasipoisson when the count data is overdispersed.</li>
<li>Nonparametric approaches like loess and GAM can give you a better fit for the conditional expectation, at the cost of making inference much more complicated.</li>
<li>A multilevel model has appeal here because of the large number of precincts. It can deal with overdispersion as well as regularize the estimates for the precincts. These get complicated very quickly though..</li>
</ul>
</div>
</body>
</html>
