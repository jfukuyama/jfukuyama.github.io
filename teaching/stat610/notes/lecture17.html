<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>lecture17</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="newtons-method" class="slide section level2">
<h1>Newton’s method</h1>
<p>Agenda today</p>
<ul class="incremental">
<li><p>Newton’s method</p></li>
<li><p>Review method of moments</p></li>
</ul>
<p>Reading:</p>
<ul class="incremental">
<li>Kenneth Lange, Numerical Analysis for Statisticians, Section 11.2, 13.3</li>
</ul>
<p>Logistics:</p>
<ul class="incremental">
<li><p>Homework due today</p></li>
<li><p>Next homework posted, due in two weeks (Tuesday November 3)</p></li>
</ul>
</div>
<div id="newtons-method-1" class="slide section level2">
<h1>Newton’s method</h1>
<ul class="incremental">
<li><p>Iterative method for finding local minimum/maximum of a function.</p></li>
<li><p>Also known as Newton-Raphson, after Isaac Newton and Joseph Raphson (Raphson published in 1690, Newton wrote a similar method in 1671 but didn’t publish until 1736)</p></li>
<li><p>Initial description is for finding zeros of a function</p></li>
<li><p>This turns out to be equivalent to an optimization problem/finding maxima of functions, as we need.</p></li>
</ul>
</div>
<div id="notation" class="slide section level2">
<h1>Notation</h1>
<p>Start off with a one-dimenisonal parameter:</p>
<ul class="incremental">
<li><p><span class="math inline">\(\theta\)</span>: The parameter, a scalar</p></li>
<li><p><span class="math inline">\(\ell (\theta)\)</span>: The log likelihood at <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><span class="math inline">\(\ell&#39;(\theta)\)</span>: The first derivative of the log likelihood at <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><span class="math inline">\(\ell&#39;&#39;(\theta)\)</span>: The second derivative of the log likelihood.</p></li>
</ul>
</div>
<div id="what" class="slide section level2">
<h1>What</h1>
<p>Our goal: Find the value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(\ell(\theta)\)</span>.</p>
<div class="incremental">
<p>Given that we are at a point <span class="math inline">\(\theta_n\)</span>, one Newton step is given by</p>
<p><span class="math display">\[
\theta_{n+1} = \theta_n - \ell&#39;&#39;(\theta_n)^{-1} \ell&#39;(\theta_n)
\]</span></p>
</div>
<div class="incremental">
<p>Newton’s method algorithm:</p>
<ul class="incremental">
<li><p>Start at a point <span class="math inline">\(\theta_0\)</span></p></li>
<li><p>Iterate <span class="math inline">\(\theta_{n+1} = \theta_n - \ell&#39;&#39;(\theta_n)^{-1} \ell&#39;(\theta_n)\)</span> until some stopping criterion is reached.</p></li>
<li><p>Usually stop when the derivative, <span class="math inline">\(\ell&#39;(\theta_n)\)</span> is sufficiently close to zero.</p></li>
</ul>
</div>
<div class="incremental">
<p>Notice:</p>
<ul class="incremental">
<li><p>Any stationary point of <span class="math inline">\(\ell(\theta)\)</span> is a fixed point of Newton’s method.</p></li>
<li><p>Not necessarily an ascent algorithm</p></li>
</ul>
</div>
</div>
<div id="why" class="slide section level2">
<h1>Why</h1>
<p>Suppose we want to maximze a quadratic:</p>
<p><span class="math display">\[
f(\theta) = a + b \theta + c \theta^2
\]</span></p>
<div class="incremental">
<p>We can solve for the maximum/minimum analytically by setting the first derivative equal to 0:</p>
<p><span class="math display">\[
f&#39;(\theta) = b + 2 c \theta
\]</span></p>
</div>
<div class="incremental">
<p>If we want <span class="math inline">\(b + 2c \theta^\star = 0\)</span>, we take <span class="math inline">\(\theta^\star = -\frac{b }{2c}\)</span></p>
</div>
</div>
<div class="slide section level2">

<div class="incremental">
<p>Recast this result as a “step” from <span class="math inline">\(\theta_0\)</span> instead of a single optimization:</p>
<ul class="incremental">
<li><p>We start at <span class="math inline">\(\theta_0\)</span></p></li>
<li><p><span class="math inline">\(\theta_1\)</span> should be <span class="math inline">\(-\frac{b}{2c}\)</span></p></li>
<li><p>We want to write <span class="math inline">\(\theta_1 = \theta_0 + ???\)</span> <br><br> <span class="math display">\[
\begin{align*}
\theta_1 &amp;= \theta_0 + (\theta_1 - \theta_0) \\
&amp;=\theta_0 -\frac{b}{2c} - \theta_0 \\
&amp;=\theta_0 -\frac{b + 2c \theta_0}{2c} \\
&amp;=\theta_0 -\frac{ f&#39;(\theta_0)}{f&#39;&#39;(\theta_0)}
\end{align*}
\]</span> <br><br> since <span class="math inline">\(f&#39;(\theta_0) = b + 2c\theta_0\)</span> and <span class="math inline">\(f&#39;&#39;(\theta_0) = 2c\)</span></p></li>
</ul>
</div>
</div>
<div class="slide section level2">

<p>Intuition for general, not-necessarily-quadratic functions:</p>
<ul class="incremental">
<li><p>We are not only dealing with quadratic functions, but we can approximate smooth, differentiable functions by quadratic functions.</p></li>
<li><p>Taylor approximation of <span class="math inline">\(\ell(\theta)\)</span> around <span class="math inline">\(\theta_0\)</span>: <span class="math display">\[
\ell(\theta) \approx \ell(\theta_0) + \ell&#39;(\theta_0)(\theta - \theta_0) + \frac{1}{2} \ell&#39;&#39;(\theta_0) (\theta - \theta_0)^2
\]</span></p></li>
<li><p>A Newton step finds an extreme point for the approximation.</p></li>
</ul>
</div>
<div id="example-1-normal-mean" class="slide section level2">
<h1>Example 1: Normal mean</h1>
<p><span class="math display">\[
x_1,\ldots, x_n \sim N(\theta, 1)
\]</span></p>
<div class="incremental">
<p>Likelihood: <span class="math display">\[
L(\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp \left(-\frac{1}{2} (x_i - \theta)^2 \right)
\]</span></p>
</div>
<div class="incremental">
<p>Log likelihood: <span class="math display">\[
\ell(\theta) = \sum_{i=1}^n \left[ -\frac{1}{2} \log(2\pi)- \frac{1}{2} (x_i - \theta)^2\right]
\]</span></p>
</div>
<div class="incremental">
<p>First derivative: <span class="math display">\[
\ell&#39;(\theta) = \sum_{i=1}^n (x_i - \theta)
\]</span></p>
</div>
<div class="incremental">
<p>Second derivative <span class="math display">\[
\ell&#39;&#39;(\theta) = \sum_{i=1}^n (-1) = -n
\]</span></p>
</div>
</div>
<div class="slide section level2">

<p>Newton “step”:</p>
<p><span class="math display">\[
\begin{align*}
\theta_1 &amp;= \theta_0 -\ell&#39;(\theta_0) / \ell&#39;&#39;(\theta_0)\\
&amp;= \theta_0 + \sum_{i=1}^n (x_i - \theta_0) / n \\
&amp;= \sum_{i=1}^n x_i / n
\end{align*}
\]</span></p>
<div class="incremental">
<p>This puts us at the maximum, as you can check by evaluating first and second derivatives.</p>
</div>
</div>
<div class="slide section level2">

<p>Let’s look at the log likelihood and the Taylor approximation of the log likelihood:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="fl">1.1</span>, <span class="fl">2.3</span>, <span class="dv">3</span>, <span class="fl">1.5</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a>theta_vec =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dt">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a>log_lik =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, <span class="cf">function</span>(theta) <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dnorm</span>(x, <span class="dt">mean =</span> theta, <span class="dt">sd =</span> <span class="dv">1</span>))))</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-1-1.png" /></p>
</div>
<div class="slide section level2">

<p>Taylor approximation:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>theta_<span class="dv">0</span> =<span class="st"> </span><span class="dv">0</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>taylor_approx =<span class="st"> </span><span class="cf">function</span>(theta, theta_<span class="dv">0</span>, loglik, dloglik, d2loglik, x) {</span>
<span id="cb2-3"><a href="#cb2-3"></a>    <span class="kw">loglik</span>(theta_<span class="dv">0</span>, x) <span class="op">+</span><span class="st"> </span><span class="kw">dloglik</span>(theta_<span class="dv">0</span>, x) <span class="op">*</span><span class="st"> </span>(theta <span class="op">-</span><span class="st"> </span>theta_<span class="dv">0</span>) <span class="op">+</span><span class="st"> </span><span class="fl">.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">d2loglik</span>(theta_<span class="dv">0</span>, x) <span class="op">*</span><span class="st"> </span>(theta <span class="op">-</span><span class="st"> </span>theta_<span class="dv">0</span>)<span class="op">^</span><span class="dv">2</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>}</span>
<span id="cb2-5"><a href="#cb2-5"></a>loglik_norm =<span class="st"> </span><span class="cf">function</span>(theta, x) <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dnorm</span>(x, <span class="dt">mean =</span> theta, <span class="dt">sd =</span> <span class="dv">1</span>)))</span>
<span id="cb2-6"><a href="#cb2-6"></a>dloglik_norm =<span class="st"> </span><span class="cf">function</span>(theta, x) <span class="kw">sum</span>(x <span class="op">-</span><span class="st"> </span>theta)</span>
<span id="cb2-7"><a href="#cb2-7"></a>d2loglik_norm =<span class="st"> </span><span class="cf">function</span>(theta, x) <span class="op">-</span><span class="kw">length</span>(x)</span>
<span id="cb2-8"><a href="#cb2-8"></a>taylor_approx_vector =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, taylor_approx, <span class="dt">theta_0 =</span> <span class="dv">0</span>, <span class="dt">loglik =</span> loglik_norm, <span class="dt">dloglik =</span> dloglik_norm, <span class="dt">d2loglik =</span> d2loglik_norm, <span class="dt">x =</span> x)</span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">pch =</span> <span class="dv">2</span>)</span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="kw">points</span>(taylor_approx_vector <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-2-1.png" /></p>
</div>
<div id="example-2-negative-binomial" class="slide section level2">
<h1>Example 2: Negative binomial</h1>
<p><span class="math display">\[
x_1,\ldots, x_n \sim NB(.5, \theta)
\]</span></p>
<div class="incremental">
<p>Likelihood: <span class="math display">\[
\begin{align*}
L(\theta) &amp;= \prod_{i=1}^n \frac{(x_i + \theta - 1)!}{(\theta-1)! x_i!} \left( \frac{1}{2} \right)^{x_i + \theta} \\
&amp;= \prod_{i=1}^n \frac{\Gamma(x_i + \theta)}{\Gamma(\theta) x_i!}  \left( \frac{1}{2} \right)^{x_i + \theta}
\end{align*}
\]</span></p>
</div>
<div class="incremental">
<p>Log likelihood: <span class="math display">\[
\ell(\theta) = \sum_{i=1}^n \log \Gamma(x_i + \theta) - \log(x_i!) - \log \Gamma(\theta) + (x_i + \theta) \log(1/2)
\]</span></p>
</div>
<div class="incremental">
<p>First derivative: <span class="math display">\[
\ell&#39;(\theta) = \sum_{i=1}^n \psi^{(0)}(x_i + \theta)- \psi^{(0)} (\theta) + \log(1/2)
\]</span></p>
</div>
<div class="incremental">
<p>Second derivative <span class="math display">\[
\ell&#39;&#39;(\theta) = \sum_{i=1}^n \psi^{(1)} (x_i + \theta) - \psi^{(1)} (\theta)
\]</span></p>
</div>
</div>
<div class="slide section level2">

<p>Newton step:</p>
<p><span class="math display">\[
\begin{align*}
\theta_{m+1} &amp;= \theta_m -\ell&#39;(\theta_m) / \ell&#39;&#39;(\theta_m)\\
&amp;= \theta_m - \frac{\sum_{i=1}^n(\psi^{(0)}(x_i + \theta_m)- \psi^{(0)} (\theta_m) + \log(1/2))}{\sum_{i=1}^n (\psi^{(1)} (x_i + \theta_m) - \psi^{(1)} (\theta_m)) }
\end{align*}
\]</span></p>
</div>
<div class="slide section level2">

<p>Let’s look at the log likelihood and the Taylor approximation of the log likelihood starting at <span class="math inline">\(\theta_0 = 7\)</span>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb3-2"><a href="#cb3-2"></a>x =<span class="st"> </span><span class="kw">rnbinom</span>(<span class="dt">n =</span> <span class="dv">5</span>, <span class="dt">size =</span> <span class="dv">4</span>, <span class="dt">prob =</span> <span class="fl">.5</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a>theta_vec =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dt">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb3-4"><a href="#cb3-4"></a>log_lik =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, <span class="cf">function</span>(theta) <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dnbinom</span>(x, <span class="dt">size =</span> theta, <span class="dt">prob =</span> <span class="fl">.5</span>))))</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-3-1.png" /></p>
</div>
<div class="slide section level2">

<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>loglik_nbinom =<span class="st"> </span><span class="cf">function</span>(theta, x) <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dnbinom</span>(x, <span class="dt">size =</span> theta, <span class="dt">prob =</span> <span class="fl">.5</span>)))</span>
<span id="cb4-2"><a href="#cb4-2"></a>dloglik_nbinom =<span class="st"> </span><span class="cf">function</span>(theta, x) <span class="kw">sum</span>(<span class="kw">digamma</span>(x <span class="op">+</span><span class="st"> </span>theta) <span class="op">-</span><span class="st"> </span><span class="kw">digamma</span>(theta) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(.<span class="dv">5</span>))</span>
<span id="cb4-3"><a href="#cb4-3"></a>d2loglik_nbinom =<span class="st"> </span><span class="cf">function</span>(theta, x) <span class="kw">sum</span>(<span class="kw">trigamma</span>(x <span class="op">+</span><span class="st"> </span>theta) <span class="op">-</span><span class="st"> </span><span class="kw">trigamma</span>(theta))</span>
<span id="cb4-4"><a href="#cb4-4"></a>taylor_approx_vector =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, taylor_approx, <span class="dt">theta_0 =</span> <span class="dv">7</span>, <span class="dt">loglik =</span> loglik_nbinom, <span class="dt">dloglik =</span> dloglik_nbinom, <span class="dt">d2loglik =</span> d2loglik_nbinom, <span class="dt">x =</span> x)</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">20</span>, <span class="dv">-8</span>))</span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="kw">points</span>(taylor_approx_vector <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-4-1.png" /></p>
</div>
<div class="slide section level2">

<p>What is the maximizing point?</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>theta_vec[<span class="kw">which.max</span>(taylor_approx_vector)]</span></code></pre></div>
<pre><code>## [1] 2.306306</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>theta_<span class="dv">0</span> =<span class="st"> </span><span class="dv">7</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>theta_<span class="dv">1</span> =<span class="st"> </span>theta_<span class="dv">0</span> <span class="op">-</span><span class="st"> </span><span class="kw">dloglik_nbinom</span>(theta_<span class="dv">0</span>, x) <span class="op">/</span><span class="st"> </span><span class="kw">d2loglik_nbinom</span>(theta_<span class="dv">0</span>, x)</span>
<span id="cb7-3"><a href="#cb7-3"></a>theta_<span class="dv">1</span></span></code></pre></div>
<pre><code>## [1] 2.307302</code></pre>
</div>
<div class="slide section level2">

<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>taylor_approx_vector =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, taylor_approx, <span class="dt">theta_0 =</span> theta_<span class="dv">1</span>, <span class="dt">loglik =</span> loglik_nbinom, <span class="dt">dloglik =</span> dloglik_nbinom, <span class="dt">d2loglik =</span> d2loglik_nbinom, <span class="dt">x =</span> x)</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">20</span>, <span class="dv">-8</span>))</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="kw">points</span>(taylor_approx_vector <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-6-1.png" /></p>
</div>
<div class="slide section level2">

<p>Another Newton step:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>theta_<span class="dv">2</span> =<span class="st"> </span>theta_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">dloglik_nbinom</span>(theta_<span class="dv">1</span>, x) <span class="op">/</span><span class="st"> </span><span class="kw">d2loglik_nbinom</span>(theta_<span class="dv">1</span>, x)</span>
<span id="cb10-2"><a href="#cb10-2"></a>theta_<span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 3.393864</code></pre>
<div class="incremental">
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>taylor_approx_vector =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, taylor_approx, <span class="dt">theta_0 =</span> theta_<span class="dv">2</span>, <span class="dt">loglik =</span> loglik_nbinom, <span class="dt">dloglik =</span> dloglik_nbinom, <span class="dt">d2loglik =</span> d2loglik_nbinom, <span class="dt">x =</span> x)</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">20</span>, <span class="dv">-8</span>))</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="kw">points</span>(taylor_approx_vector <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-8-1.png" /></p>
</div>
</div>
<div id="example-3-mixture-model" class="slide section level2">
<h1>Example 3: Mixture model</h1>
<p>Mixture model</p>
<ul class="incremental">
<li><p><span class="math inline">\(x_1, \ldots, x_n\)</span> come from a distribution with cumulative distribution function <span class="math inline">\(\theta G + (1 - \theta)H\)</span>, where <span class="math inline">\(G\)</span> and <span class="math inline">\(H\)</span> are two fixed, distributions (for example, two normal distributions with known means and variances, or two Poisson distributions with different means).</p></li>
<li><p>Let <span class="math inline">\(\xi\)</span> denote the mean of <span class="math inline">\(G\)</span> and <span class="math inline">\(\eta\)</span> denote the mean of <span class="math inline">\(H\)</span>.</p></li>
<li><p>We want to estimate the mixing parameter <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
</div>
<div class="slide section level2">

<p>For example, we can visualize the density for a mixture of a <span class="math inline">\(N(0, .5)\)</span> and a <span class="math inline">\(N(3, 2.5)\)</span> distribution with mixing parameter <span class="math inline">\(\theta = .2\)</span>:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>mean_G =<span class="st"> </span><span class="dv">0</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>mean_H =<span class="st"> </span><span class="dv">3</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>sd_G =<span class="st"> </span><span class="fl">.5</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>sd_H =<span class="st"> </span><span class="fl">2.5</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>q_seq =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>, <span class="dt">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb13-6"><a href="#cb13-6"></a>g =<span class="st"> </span><span class="kw">dnorm</span>(q_seq, <span class="dt">mean =</span> mean_G, <span class="dt">sd =</span> sd_G)</span>
<span id="cb13-7"><a href="#cb13-7"></a>h =<span class="st"> </span><span class="kw">dnorm</span>(q_seq, <span class="dt">mean =</span> mean_H, <span class="dt">sd =</span> sd_H)</span>
<span id="cb13-8"><a href="#cb13-8"></a>theta =<span class="st"> </span><span class="fl">.2</span></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="kw">plot</span>(theta <span class="op">*</span><span class="st"> </span>g <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">*</span><span class="st"> </span>h <span class="op">~</span><span class="st"> </span>q_seq, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-9-1.png" /></p>
</div>
<div class="slide section level2">

<p>Let <span class="math inline">\(\phi_1(x)\)</span> denote the density of a <span class="math inline">\(N(0, .5)\)</span> and <span class="math inline">\(\phi_2(x)\)</span> denote the density of a <span class="math inline">\(N(3,2.5)\)</span> distribution.</p>
<p>Then we have</p>
<p><span class="math display">\[
x_1,\ldots, x_n \sim \theta \phi_1 + (1 - \theta) \phi_2
\]</span></p>
<div class="incremental">
<p>Likelihood: <span class="math display">\[
L(\theta) = \prod_{i=1}^n (\theta \phi_1(x_i) + (1 - \theta)\phi_2(x_i))
\]</span></p>
</div>
<div class="incremental">
<p>Log likelihood: <span class="math display">\[
\ell(\theta) = \sum_{i=1}^n \log(\theta \phi_1(x_i) + (1 - \theta)\phi_2(x_i))
\]</span></p>
</div>
<div class="incremental">
<p>First derivative: <span class="math display">\[
\ell&#39;(\theta) = \sum_{i=1}^n \frac{\phi_1(x_i) - \phi_2(x_i)}{\theta \phi_1(x_i) + (1 - \theta) \phi_2(x_i)}
\]</span></p>
</div>
<div class="incremental">
<p>Second derivative <span class="math display">\[
\ell&#39;&#39;(\theta) = -\sum_{i=1}^n\frac{ (\phi_1(x_i) - \phi_2(x_i))^2}{(\theta \phi_1(x_i) + (1 - \theta) \phi_2(x_i))^2}
\]</span></p>
</div>
</div>
<div class="slide section level2">

<p>Newton step:</p>
<p><span class="math display">\[
\begin{align*}
\theta_{m+1} &amp;= \theta_m -\ell&#39;(\theta_m) / \ell&#39;&#39;(\theta_m)\\
&amp;= \theta_m + \frac{\sum_{i=1}^n \frac{\phi_1(x_i) - \phi_2(x_i)}{\theta_m \phi_1(x_i) + (1 - \theta_m) \phi_2(x_i)}}{\sum_{i=1}^n\frac{ (\phi_1(x_i) - \phi_2(x_i))^2}{(\theta_m \phi_1(x_i) + (1 - \theta_m) \phi_2(x_i))^2}}
\end{align*}
\]</span></p>
</div>
<div class="slide section level2">

<p>Again: log likelihood and the Taylor approximation of the log likelihood</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span>.<span class="dv">5</span>,.<span class="dv">2</span>,.<span class="dv">2</span>,.<span class="dv">1</span>,.<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a>theta_vec =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb14-3"><a href="#cb14-3"></a>phi1 =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">dnorm</span>(x, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">.5</span>)</span>
<span id="cb14-4"><a href="#cb14-4"></a>phi2 =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">dnorm</span>(x, <span class="dt">mean =</span> <span class="dv">3</span>, <span class="dt">sd =</span> <span class="fl">2.5</span>)</span>
<span id="cb14-5"><a href="#cb14-5"></a>mixture_density =<span class="st"> </span><span class="cf">function</span>(theta, x)</span>
<span id="cb14-6"><a href="#cb14-6"></a>    theta <span class="op">*</span><span class="st"> </span><span class="kw">phi1</span>(x) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">*</span><span class="st"> </span><span class="kw">phi2</span>(x)</span>
<span id="cb14-7"><a href="#cb14-7"></a>log_lik =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, <span class="cf">function</span>(theta) <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">mixture_density</span>(theta, x))))</span>
<span id="cb14-8"><a href="#cb14-8"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">40</span>, <span class="dv">-15</span>))</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-10-1.png" /></p>
</div>
<div class="slide section level2">

<div class="incremental">
<p>We’ll start at a pretty bad point, <span class="math inline">\(\theta_0 = .95\)</span>:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>loglik_mixture =<span class="st"> </span><span class="cf">function</span>(theta, x)</span>
<span id="cb15-2"><a href="#cb15-2"></a>    <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">mixture_density</span>(theta, x)))</span>
<span id="cb15-3"><a href="#cb15-3"></a>dloglik_mixture =<span class="st"> </span><span class="cf">function</span>(theta, x)</span>
<span id="cb15-4"><a href="#cb15-4"></a>    <span class="kw">sum</span>((<span class="kw">phi1</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">phi2</span>(x)) <span class="op">/</span><span class="st"> </span>(theta <span class="op">*</span><span class="st"> </span><span class="kw">phi1</span>(x) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">*</span><span class="st"> </span><span class="kw">phi2</span>(x)))</span>
<span id="cb15-5"><a href="#cb15-5"></a>d2loglik_mixture =<span class="st"> </span><span class="cf">function</span>(theta, x)</span>
<span id="cb15-6"><a href="#cb15-6"></a>    <span class="op">-</span><span class="kw">sum</span>((<span class="kw">phi1</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">phi2</span>(x))<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(theta <span class="op">*</span><span class="st"> </span><span class="kw">phi1</span>(x) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">*</span><span class="st"> </span><span class="kw">phi2</span>(x))<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb15-7"><a href="#cb15-7"></a>taylor_approx_vector =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, taylor_approx, <span class="dt">theta_0 =</span> <span class="fl">.95</span>, <span class="dt">loglik =</span> loglik_mixture, <span class="dt">dloglik =</span> dloglik_mixture, <span class="dt">d2loglik =</span> d2loglik_mixture, <span class="dt">x =</span> x)</span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">40</span>, <span class="dv">-15</span>))</span>
<span id="cb15-9"><a href="#cb15-9"></a><span class="kw">points</span>(taylor_approx_vector <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-11-1.png" /></p>
</div>
</div>
<div class="slide section level2">

<p>What is the maximizing point?</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>theta_vec[<span class="kw">which.max</span>(taylor_approx_vector)]</span></code></pre></div>
<pre><code>## [1] 0.9029029</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a>theta_<span class="dv">0</span> =<span class="st"> </span><span class="fl">.95</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>theta_<span class="dv">1</span> =<span class="st"> </span>theta_<span class="dv">0</span> <span class="op">-</span><span class="st"> </span><span class="kw">dloglik_mixture</span>(theta_<span class="dv">0</span>, x) <span class="op">/</span><span class="st"> </span><span class="kw">d2loglik_mixture</span>(theta_<span class="dv">0</span>, x)</span>
<span id="cb18-3"><a href="#cb18-3"></a>theta_<span class="dv">1</span></span></code></pre></div>
<pre><code>## [1] 0.9024169</code></pre>
</div>
<div class="slide section level2">

<p>Then do a Taylor approximation around <span class="math inline">\(\theta_1\)</span>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>taylor_approx_vector =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, taylor_approx, <span class="dt">theta_0 =</span> theta_<span class="dv">1</span>, <span class="dt">loglik =</span> loglik_mixture, <span class="dt">dloglik =</span> dloglik_mixture, <span class="dt">d2loglik =</span> d2loglik_mixture, <span class="dt">x =</span> x)</span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">40</span>, <span class="dv">-15</span>))</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="kw">points</span>(taylor_approx_vector <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-13-1.png" /></p>
</div>
<div class="slide section level2">

<p>And another Newton step:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>theta_<span class="dv">2</span> =<span class="st"> </span>theta_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">dloglik_mixture</span>(theta_<span class="dv">1</span>, x) <span class="op">/</span><span class="st"> </span><span class="kw">d2loglik_mixture</span>(theta_<span class="dv">1</span>, x)</span>
<span id="cb21-2"><a href="#cb21-2"></a>theta_<span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.8148403</code></pre>
</div>
<div class="slide section level2">

<p>Another Taylor approximation/Newton step:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a>taylor_approx_vector =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, taylor_approx, <span class="dt">theta_0 =</span> theta_<span class="dv">2</span>, <span class="dt">loglik =</span> loglik_mixture, <span class="dt">dloglik =</span> dloglik_mixture, <span class="dt">d2loglik =</span> d2loglik_mixture, <span class="dt">x =</span> x)</span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="kw">plot</span>(log_lik <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">40</span>, <span class="dv">-15</span>))</span>
<span id="cb23-3"><a href="#cb23-3"></a><span class="kw">points</span>(taylor_approx_vector <span class="op">~</span><span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-15-1.png" /></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a>theta_<span class="dv">3</span> =<span class="st"> </span>theta_<span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">dloglik_mixture</span>(theta_<span class="dv">2</span>, x) <span class="op">/</span><span class="st"> </span><span class="kw">d2loglik_mixture</span>(theta_<span class="dv">2</span>, x)</span>
<span id="cb24-2"><a href="#cb24-2"></a>theta_<span class="dv">3</span></span></code></pre></div>
<pre><code>## [1] 0.6714755</code></pre>
</div>
<div id="potential-issues" class="slide section level2">
<h1>Potential issues</h1>
<ul class="incremental">
<li><p>How do you choose starting values?</p></li>
<li><p>What if the function has no derivative/second derivative?</p></li>
<li><p>What if the function has no curvature in some regions?</p></li>
<li><p>How do you know you will converge to a maximum and not a minimum?</p></li>
</ul>
</div>
<div id="multiple-parameters-notation" class="slide section level2">
<h1>Multiple parameters: notation</h1>
<p>Suppose we have <span class="math inline">\(p\)</span> paramaters.</p>
<ul class="incremental">
<li><p><span class="math inline">\(\theta\)</span>: The parameters, a vector in <span class="math inline">\(\mathbb R^p\)</span>.</p></li>
<li><p><span class="math inline">\(\ell (\theta)\)</span>: The log likelihood at <span class="math inline">\(\theta\)</span>, an element of <span class="math inline">\(\mathbb R\)</span>.</p></li>
<li><p><span class="math inline">\(d \ell(\theta)\)</span>: A vector in <span class="math inline">\(\mathbb R^p\)</span> of first derivatives of the log likelihood at <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><span class="math inline">\(d^2 \ell(\theta)\)</span>: A matrix in <span class="math inline">\(\mathbb R^{p\times p}\)</span> containing the second partial derivatives of <span class="math inline">\(\ell\)</span>.</p></li>
</ul>
</div>
<div id="multiple-parameters-algorithm" class="slide section level2">
<h1>Multiple parameters: algorithm</h1>
<p>Our goal: Find the value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(\ell(\theta)\)</span>.</p>
<p>Given that we are at a point <span class="math inline">\(\theta_n\)</span>, one Newton step is given by</p>
<p><span class="math display">\[
\theta_{n+1} = \theta_n - d^2 \ell(\theta_n)^{-1} d \ell(\theta_n)
\]</span></p>
<div class="incremental">
<p>Newton’s method algorithm:</p>
<ul class="incremental">
<li><p>Start at a point <span class="math inline">\(\theta_0\)</span></p></li>
<li><p>Iterate <span class="math inline">\(\theta_{n+1} = \theta_n - d^2 \ell(\theta_n)^{-1} d \ell(\theta_n)\)</span> until some stopping criterion is reached.</p></li>
<li><p>Usually stop when the derivative is sufficiently close to zero.</p></li>
</ul>
</div>
<div class="incremental">
<p>Same derivation and intuition as the one-parameter case: we make a second-order Taylor approximation to the log likelihood and maximize that.</p>
</div>
</div>
<div class="slide section level2">

<p>Newton’s method for multiple parameters has the same issues as with one parameter:</p>
<ul class="incremental">
<li><p>How do you choose a starting point?</p></li>
<li><p>What if the second derivative matrix doesn’t exist?</p></li>
<li><p>What if the second derivative matrix isn’t invertible?</p></li>
<li><p>How do you know the function converges to a maximum and not a minimum?</p></li>
</ul>
<p>In addition, we have the problem of inverting the Hessian, this scales badly with <span class="math inline">\(p\)</span>, the number of parameters. (Usually think of as <span class="math inline">\(O(p^3)\)</span>, but computer scientists have worked really hard and gotten <span class="math inline">\(O(p^{2.4ish})\)</span>)</p>
<p>Several ways of choosing starting points, but one way that often works is <em>method of moments</em>.</p>
</div>
<div id="method-of-moments" class="slide section level2">
<h1>Method of moments</h1>
<p>Same problem as maximum likelihood: we have a family of probability models, indexed by a scalar or vector <span class="math inline">\(\theta\)</span>, and we need to choose one to describe the data.</p>
<p>Idea:</p>
<ul class="incremental">
<li><p>If <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(k\)</span>-dimensional vector (we have <span class="math inline">\(k\)</span> parameters to estimate), derive expressions for the first <span class="math inline">\(k\)</span> moments of the data, <span class="math inline">\(E_\theta(X^r)\)</span>, <span class="math inline">\(r = 1,\ldots, k\)</span></p></li>
<li><p>Compute the first <span class="math inline">\(k\)</span> empirical moments of the data: <br><br> <span class="math display">\[
\frac{1}{n} \sum_{i=1}^n x_i^r, \quad r = 1,\ldots, k
\]</span></p></li>
<li><p><span class="math inline">\(\hat \theta\)</span> is the value of <span class="math inline">\(\theta\)</span> such that the empirical moments match the theoretical moments: <br><br> <span class="math display">\[
E_{\hat \theta}(X^r) = \sum_{i=1}^n x_i^r, \quad r = 1,\ldots, k
\]</span></p></li>
</ul>
</div>
<div id="example-moment-estimator-for-normal-family" class="slide section level2">
<h1>Example: moment estimator for normal family</h1>
<p>Our family of distributions is <span class="math inline">\(N(\mu, \sigma^2)\)</span>, so that <span class="math inline">\(\theta = (\mu, \sigma)\)</span>.</p>
<div class="incremental">
<p>The first two moments are:</p>
<ul class="incremental">
<li><p><span class="math inline">\(E_{\mu, \sigma}(X) = \mu\)</span></p></li>
<li><p><span class="math inline">\(E_{\mu, \sigma}(X^2) = \mu^2 + \sigma^2\)</span></p></li>
</ul>
</div>
</div>
<div class="slide section level2">

<p>Equate the first theoretical moment to the first data moment tells us that <span class="math inline">\(\hat \mu\)</span> should satisfy</p>
<p><span class="math display">\[
E_{\hat \mu,\hat \sigma}(X) = \hat \mu = \frac{1}{n} \sum_{i=1}^n x_i
\]</span> and so <span class="math inline">\(\hat \mu = \frac{1}{n} \sum_{i=1}^n x_i\)</span></p>
<div class="incremental">
<p>Then equating the second theoretical moments to the second data moment tells us that <span class="math inline">\(\hat \mu\)</span> and <span class="math inline">\(\hat \sigma\)</span> should satisfy <span class="math display">\[
E_{\mu, \sigma}(X^2) = \mu^2 + \sigma^2 = \frac{1}{n} \sum_{i=1}^n x_i^2
\]</span> Plugging in <span class="math inline">\(\hat \mu = \sum_{i=1}^n x_i\)</span> and solving gives us <span class="math display">\[
E_{\hat \mu, \hat \sigma}(X^2) = \hat \mu^2 + \hat \sigma^2 \\
= (\frac{1}{n} \sum_{i=1}^n x_i)^2 + \hat \sigma^2\\
=\frac{1}{n}\sum_{i=1}^n x_i^2
\]</span> and so <span class="math inline">\(\hat \sigma^2 = \frac{1}{n}\sum_{i=1}^n x_i^2 - (\frac{1}{n}\sum_{i=1}^n x_i)^2\)</span>.</p>
<p>If you do a little more algebra, you can see that this is a standard estimate of the variance.</p>
</div>
</div>
<div id="example-moment-estimator-for-mixture-models" class="slide section level2">
<h1>Example: moment estimator for mixture models</h1>
<p>Mixture model</p>
<ul class="incremental">
<li><p><span class="math inline">\(x_1, \ldots, x_n\)</span> come from a distribution with cumulative distribution function <span class="math inline">\(\theta G + (1 - \theta)H\)</span>, where <span class="math inline">\(G\)</span> and <span class="math inline">\(H\)</span> are two fixed, distributions (for example, two normal distributions with known means and variances, or two Poisson distributions with different means).</p></li>
<li><p>Let <span class="math inline">\(\xi\)</span> denote the mean of <span class="math inline">\(G\)</span> and <span class="math inline">\(\eta\)</span> denote the mean of <span class="math inline">\(H\)</span>.</p></li>
<li><p>We want to estimate the mixing parameter <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
</div>
<div class="slide section level2">

<p>For example, we can visualize the density for a mixture of a <span class="math inline">\(N(0, .5)\)</span> and a <span class="math inline">\(N(3, 2.5)\)</span> distribution with mixing parameter <span class="math inline">\(\theta = .2\)</span>:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a>mean_G =<span class="st"> </span><span class="dv">0</span></span>
<span id="cb26-2"><a href="#cb26-2"></a>mean_H =<span class="st"> </span><span class="dv">3</span></span>
<span id="cb26-3"><a href="#cb26-3"></a>sd_G =<span class="st"> </span><span class="fl">.5</span></span>
<span id="cb26-4"><a href="#cb26-4"></a>sd_H =<span class="st"> </span><span class="fl">2.5</span></span>
<span id="cb26-5"><a href="#cb26-5"></a>q_seq =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>, <span class="dt">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb26-6"><a href="#cb26-6"></a>g =<span class="st"> </span><span class="kw">dnorm</span>(q_seq, <span class="dt">mean =</span> mean_G, <span class="dt">sd =</span> sd_G)</span>
<span id="cb26-7"><a href="#cb26-7"></a>h =<span class="st"> </span><span class="kw">dnorm</span>(q_seq, <span class="dt">mean =</span> mean_H, <span class="dt">sd =</span> sd_H)</span>
<span id="cb26-8"><a href="#cb26-8"></a>theta =<span class="st"> </span><span class="fl">.2</span></span>
<span id="cb26-9"><a href="#cb26-9"></a><span class="kw">plot</span>(theta <span class="op">*</span><span class="st"> </span>g <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">*</span><span class="st"> </span>h <span class="op">~</span><span class="st"> </span>q_seq, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="lecture-17-fig/unnamed-chunk-17-1.png" /></p>
</div>
<div class="slide section level2">

<p>We have one parameter, so we compute the first theoretical moment: <span class="math display">\[
E_\theta(X) = \theta \xi + (1 - \theta) \eta
\]</span></p>
<div class="incremental">
<p>Then we equate that moment to the first data moment to get our estimate: <span class="math display">\[
\hat \theta \xi + (1 - \hat \theta) \eta = \frac{1}{n} \sum_{i=1}^n x_i\\
\hat \theta = \frac{\frac{1}{n} \sum_{i=1}^n x_i - \eta}{\xi - \eta}
\]</span></p>
</div>
</div>
<div class="slide section level2">

<p>There isn’t anything particularly important about the first <span class="math inline">\(k\)</span> moments, can match other aspects of the data</p>
<ul class="incremental">
<li><p>Median</p></li>
<li><p>Other quantiles</p></li>
<li><p>Centered moments instead of raw moments</p></li>
</ul>
<p>We are thinking of these as starting values for maximum likelihood estimation, but they are usually reasonable estimators in their own right.</p>
<p>The idea of matching data statistics to expected values of statistics will come up again in approximate Bayesian computation.</p>
</div>
</body>
</html>
