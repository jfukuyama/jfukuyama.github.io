<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>lecture27</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="metropolis-hastings" class="slide section level2">
<h1>Metropolis Hastings</h1>
<p>Today: Metropolis Hastings</p>
<p>Reading:</p>
<ul>
<li><p>Lange, Chapter 24.1, 24.2</p></li>
<li><p>More fun: <a href="https://math.uchicago.edu/~shmuel/Network-course-readings/MCMCRev.pdf">Section 1 of this paper has an interesting example</a></p></li>
<li><p>Some practical advice about using Markov chains by Charles Geyer: <a href="http://users.stat.umn.edu/~geyer/mcmc/one.html">One long run</a>, <a href="http://users.stat.umn.edu/~geyer/mcmc/burn.html">Burn in is unnecessary</a>, <a href="http://users.stat.umn.edu/~geyer/mcmc/diag.html">Bogosity of MCMC diagnostics</a></p></li>
</ul>
</div>
<div id="our-goals" class="slide section level2">
<h1>Our goals</h1>
<ul>
<li><p>Sample from any probability distribution <span class="math inline">\(\pi\)</span>.</p></li>
<li><p>Compute expected value of functions of random variables drawn from these distributions, <span class="math inline">\(E_{X \sim \pi}(f(X))\)</span></p></li>
</ul>
<div class="incremental">
<ul>
<li><p>Last time we saw that if a Markov chain has a stationary distribution <span class="math inline">\(\pi\)</span>, we can estimate <span class="math inline">\(E_{X \sim \pi}(f(X))\)</span> as <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n f(X_i)\)</span>, where <span class="math inline">\(X_1,X_2, \ldots, X_n\)</span> are drawn from the Markov chain.</p></li>
<li><p>Metropolis-Hastings will let us specify a stationary distribution <span class="math inline">\(\pi\)</span> and build a Markov chain having <span class="math inline">\(\pi\)</span> as its stationary distribution.</p></li>
</ul>
</div>
</div>
<div id="metropolis-hastings-the-idea" class="slide section level2">
<h1>Metropolis-Hastings: The Idea</h1>
<ul>
<li><p>Start off with a Markov chain that has the wrong stationary distribution, e.g., a random walk</p></li>
<li><p>Modify the chain so that it spends more time in regions of high probability under the target distribution.</p></li>
</ul>
</div>
<div id="metropolis-hastings-algorithm-with-a-symmetric-proposal-distribution" class="slide section level2">
<h1>Metropolis-Hastings: Algorithm with a symmetric proposal distribution</h1>
<p>Given:</p>
<ul>
<li><p>A proposal distribution <span class="math inline">\(q\)</span> such that <span class="math inline">\(q(y \mid x) = q(x \mid y)\)</span></p></li>
<li><p>A target stationary distribution <span class="math inline">\(\pi\)</span></p></li>
</ul>
<div class="incremental">
<p>Pick a starting value for the chain <span class="math inline">\(X_0\)</span>.</p>
<p>For <span class="math inline">\(i = 1, \ldots, n\)</span>:</p>
<ul>
<li><p>Pick a proposed move from <span class="math inline">\(Y \sim q(\cdot \mid X_{i-1})\)</span></p></li>
<li><p>Compute the acceptance probability: <span class="math display">\[
a = \text{min} \left \{ \frac{\pi(Y)}{\pi(X_{i-1})}, 1 \right\}
\]</span></p></li>
<li><p>Let <span class="math inline">\(X_i\)</span> be <span class="math display">\[
X_i = \begin{cases}
Y &amp; \text{w.p. } a \\
X_{i-1} &amp; \text{w.p. }1 - a
\end{cases}
\]</span></p></li>
</ul>
</div>
<div class="incremental">
<p>Note:</p>
<ul>
<li><p>If the proposed state has a higher probability than the current state, we move there deterministically.</p></li>
<li><p>If the proposed state has a lower probability than the current state, we move there with probability proportional to the ratio of the probabilities.</p></li>
</ul>
</div>
</div>
<div id="aside-reversible-chains-and-detailed-balance" class="slide section level2">
<h1>Aside: Reversible chains and detailed balance</h1>
<ul>
<li><p>Last time we talked about conditions for a Markov chain to have an invariant distribution.</p></li>
<li><p>Some Markov chains are “reversible”. This is a stronger condition than the chain simply having an invariant distribution.</p></li>
<li><p>To check whether a chain is reversible, we check the “detailed balance” condition: A Markov chain with transition probability matrix <span class="math inline">\(P\)</span> and invariant distribution <span class="math inline">\(\pi\)</span> satisfies the <em>detailed balance</em> equations and is <em>reversible</em> if <span class="math display">\[
\pi_i P_{ij} = \pi_j P_{ji}
\]</span></p></li>
</ul>
</div>
<div id="checking-that-the-metropolis-algorithm-has-the-correct-invariant-distribution" class="slide section level2">
<h1>Checking that the Metropolis algorithm has the correct invariant distribution</h1>
<p>Let <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> be any two elements in the state space and let <span class="math inline">\(a_{ij}\)</span> be the acceptance probability given that we start at <span class="math inline">\(X_i\)</span> and propose <span class="math inline">\(X_j\)</span>.</p>
<p>Suppose further that <span class="math inline">\(\pi(X_j) \le \pi(X_i)\)</span> so that <span class="math inline">\(a_{ij} \le 1\)</span></p>
<p>We can check that <span class="math inline">\(\pi\)</span> is the invariant distribution by checking detailed balance:</p>
<p><span class="math display">\[
\begin{align*}
\pi(X_i)q(X_j \mid X_i) a_{ij} &amp;= \pi(X_i) q(X_j \mid X_i) \frac{\pi(X_j)}{\pi(X_i)} \\ 
&amp;= q(X_j \mid X_i) \pi(X_j) \\
&amp;= \pi(X_j) q(X_j \mid X_i) a_{ji}
\end{align*}
\]</span></p>
<p>The last line follows because <span class="math inline">\(a_{ji} = 1\)</span> and the proposal distribution is symmetric.</p>
</div>
<div id="metropolis-hastings-with-non-symmetric-proposal-distribution" class="slide section level2">
<h1>Metropolis-Hastings with non-symmetric proposal distribution</h1>
<p>Given:</p>
<ul>
<li><p>A proposal distribution <span class="math inline">\(q\)</span>, not necessarily such that <span class="math inline">\(q(y \mid x) = q(x \mid y)\)</span></p></li>
<li><p>A target stationary distribution <span class="math inline">\(\pi\)</span></p></li>
</ul>
<div class="incremental">
<p>Pick a starting value for the chain <span class="math inline">\(X_0\)</span>.</p>
<p>For <span class="math inline">\(i = 1, \ldots, n\)</span>:</p>
<ul>
<li><p>Pick a proposed move from <span class="math inline">\(Y \sim q(\cdot \mid X_{i-1})\)</span></p></li>
<li><p>Compute the acceptance probability: <span class="math display">\[
a = \text{min} \left \{ \frac{\pi(Y)q(X_{i-1} \mid Y)}{\pi(X_{i-1})q(Y \mid X_{i-1})}, 1 \right\}
\]</span></p></li>
<li><p>Let <span class="math inline">\(X_i\)</span> be <span class="math display">\[
X_i = \begin{cases}
Y &amp; \text{w.p. } a \\
X_{i-1} &amp; \text{w.p. }1 - a
\end{cases}
\]</span></p></li>
</ul>
</div>
<div class="incremental">
<p>Notes:</p>
<ul>
<li><p>The difference in the acceptance probability accounts for the fact that the proposal distribution favors some parts of the space over others.</p></li>
<li><p>You can derive the acceptance probability from the detailed balance equations or check the detailed balance equations with the acceptance probabilities given here.</p></li>
</ul>
</div>
</div>
<div id="simple-example-normal-distribution" class="slide section level2">
<h1>Simple Example: Normal Distribution</h1>
<ul>
<li><p>Proposal distribution: <span class="math inline">\(q(x \mid y) = N(y, .3)\)</span></p></li>
<li><p>Target distribution: <span class="math inline">\(\pi(x) = N(0,1)\)</span></p></li>
<li><p>Start at <span class="math inline">\(X_0 = -10\)</span></p></li>
</ul>
<div class="incremental">
<ul>
<li>Notice that <span class="math inline">\(q\)</span> is symmetric, and so the acceptance probability for moving from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> is <span class="math inline">\(\text{min}\{\pi(Y) / \pi(X), 1\}\)</span></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>sample_with_symmetric_proposal =</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="st">    </span><span class="cf">function</span>(proposal_function, target_distribution, current_state) {</span>
<span id="cb1-3"><a href="#cb1-3"></a>        proposal =<span class="st"> </span><span class="kw">proposal_function</span>(current_state)</span>
<span id="cb1-4"><a href="#cb1-4"></a>        acceptance_probability =<span class="st"> </span><span class="kw">min</span>(<span class="dv">1</span>, <span class="kw">target_distribution</span>(proposal) <span class="op">/</span><span class="st"> </span><span class="kw">target_distribution</span>(current_state))</span>
<span id="cb1-5"><a href="#cb1-5"></a>        <span class="cf">if</span>(<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&lt;=</span><span class="st"> </span>acceptance_probability) {</span>
<span id="cb1-6"><a href="#cb1-6"></a>            <span class="kw">return</span>(proposal)</span>
<span id="cb1-7"><a href="#cb1-7"></a>        } <span class="cf">else</span> {</span>
<span id="cb1-8"><a href="#cb1-8"></a>            <span class="kw">return</span>(current_state)</span>
<span id="cb1-9"><a href="#cb1-9"></a>        }   </span>
<span id="cb1-10"><a href="#cb1-10"></a>}</span></code></pre></div>
</div>
</div>
<div class="slide section level2">

<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co">## The proposal distribution is normal, centered at the current state, standard deviation .3</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>proposal_function =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">mean =</span> x, <span class="dt">sd =</span> <span class="fl">.3</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co">## The target distribution is N(0,1)</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>target_distribution =<span class="st"> </span>dnorm</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co">## check the sampling:</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="kw">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, <span class="dv">-10</span>)</span></code></pre></div>
<pre><code>## [1] -10</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>n_samples =<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>chain_output =<span class="st"> </span><span class="kw">numeric</span>(n_samples)</span>
<span id="cb4-3"><a href="#cb4-3"></a>chain_output[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">-10</span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n_samples) {</span>
<span id="cb4-5"><a href="#cb4-5"></a>    chain_output[i] =<span class="st"> </span><span class="kw">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb4-6"><a href="#cb4-6"></a>}</span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="kw">plot</span>(chain_output)</span></code></pre></div>
<p><img src="lecture-27-fig/unnamed-chunk-2-1.png" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">mean</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] -0.9162604</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">sd</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 1.871455</code></pre>
</div>
<div class="slide section level2">

<p>Notice:</p>
<ul>
<li><p>We see that before about 200 steps, the chain has not converged to its stationary distribution.</p></li>
<li><p>Even after it has converged, the elements in the chain are not independent.</p></li>
</ul>
</div>
<div class="slide section level2">

<p>Let’s run the chain longer and discard the first 200 steps as “burn-in”</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>n_samples =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>chain_output =<span class="st"> </span><span class="kw">numeric</span>(n_samples)</span>
<span id="cb9-3"><a href="#cb9-3"></a>chain_output[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">-10</span></span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n_samples) {</span>
<span id="cb9-5"><a href="#cb9-5"></a>    chain_output[i] =<span class="st"> </span><span class="kw">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb9-6"><a href="#cb9-6"></a>}</span>
<span id="cb9-7"><a href="#cb9-7"></a>chain_no_burnin =<span class="st"> </span>chain_output[<span class="dv">201</span><span class="op">:</span>n_samples]</span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="kw">plot</span>(chain_no_burnin)</span></code></pre></div>
<p><img src="lecture-27-fig/unnamed-chunk-3-1.png" /></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">mean</span>(chain_no_burnin)</span></code></pre></div>
<pre><code>## [1] -0.03140313</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">sd</span>(chain_no_burnin)</span></code></pre></div>
<pre><code>## [1] 1.019056</code></pre>
</div>
<div class="slide section level2">

<p>Note:</p>
<ul>
<li><p>This chain looks like it has reached its stationary distribution, and we can confirm that by checking that the sample expected values match what they should be for a <span class="math inline">\(N(0,1)\)</span> distribution.</p></li>
<li><p>The ergodic theorem doesn’t require that we discard the burn-in period, but people often do anyway.</p></li>
</ul>
</div>
<div id="example-2-mixture-distributions" class="slide section level2">
<h1>Example 2: Mixture distributions</h1>
<ul>
<li><p>Proposal distribution: <span class="math inline">\(q(x \mid y) = N(y, .3)\)</span></p></li>
<li><p>Target distribution: Let <span class="math inline">\(\phi_{\mu, \sigma}(x)\)</span> be the pdf for a <span class="math inline">\(N(\mu, \sigma)\)</span> random variable. Our target distribution is <span class="math inline">\(\pi(x) = .25 \phi_{1, 1}(x) + .75 \phi_{5,.2}(x)\)</span>.</p></li>
<li><p>Start at <span class="math inline">\(X_0 = -10\)</span></p></li>
</ul>
<div class="incremental">
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="co">## The proposal distribution is normal, centered at the current state, standard deviation .3</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>proposal_function =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">mean =</span> x, <span class="dt">sd =</span> <span class="fl">.3</span>)</span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="co">## The target distribution is N(0,1)</span></span>
<span id="cb14-4"><a href="#cb14-4"></a>target_distribution =<span class="st"> </span><span class="cf">function</span>(x) <span class="fl">.25</span> <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(x, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="fl">.75</span> <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(x, <span class="dt">mean =</span> <span class="dv">5</span>, <span class="dt">sd =</span> <span class="fl">.2</span>)</span>
<span id="cb14-5"><a href="#cb14-5"></a>n_samples =<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb14-6"><a href="#cb14-6"></a>chain_output =<span class="st"> </span><span class="kw">numeric</span>(n_samples)</span>
<span id="cb14-7"><a href="#cb14-7"></a>chain_output[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">-10</span></span>
<span id="cb14-8"><a href="#cb14-8"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n_samples) {</span>
<span id="cb14-9"><a href="#cb14-9"></a>    chain_output[i] =<span class="st"> </span><span class="kw">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb14-10"><a href="#cb14-10"></a>}</span>
<span id="cb14-11"><a href="#cb14-11"></a><span class="kw">plot</span>(chain_output)</span></code></pre></div>
<p><img src="lecture-27-fig/unnamed-chunk-4-1.png" /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="kw">mean</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 0.3720607</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="kw">sd</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 2.266502</code></pre>
</div>
<div class="incremental">
<p>Notice:</p>
<ul>
<li><p>Still takes about 200 steps to get to something that looks like a stationary distribution</p></li>
<li><p>No samples with values above 2!</p></li>
</ul>
</div>
</div>
<div class="slide section level2">

<p>Let’s try running the chain a lot longer:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>n_samples =<span class="st"> </span><span class="dv">100000</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>chain_output =<span class="st"> </span><span class="kw">numeric</span>(n_samples)</span>
<span id="cb19-3"><a href="#cb19-3"></a>chain_output[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">-10</span></span>
<span id="cb19-4"><a href="#cb19-4"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n_samples) {</span>
<span id="cb19-5"><a href="#cb19-5"></a>    chain_output[i] =<span class="st"> </span><span class="kw">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb19-6"><a href="#cb19-6"></a>}</span>
<span id="cb19-7"><a href="#cb19-7"></a><span class="kw">plot</span>(chain_output)</span></code></pre></div>
<p><img src="lecture-27-fig/unnamed-chunk-5-1.png" /></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">mean</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 4.042973</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="kw">sd</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 1.780207</code></pre>
<p>What’s happening?</p>
</div>
<div class="slide section level2">

<p>Another way to fix this: change the proposal distribution.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a>proposal_function =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">mean =</span> x, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb24-2"><a href="#cb24-2"></a>n_samples =<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>chain_output =<span class="st"> </span><span class="kw">numeric</span>(n_samples)</span>
<span id="cb24-4"><a href="#cb24-4"></a>chain_output[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">-10</span></span>
<span id="cb24-5"><a href="#cb24-5"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n_samples) {</span>
<span id="cb24-6"><a href="#cb24-6"></a>    chain_output[i] =<span class="st"> </span><span class="kw">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb24-7"><a href="#cb24-7"></a>}</span>
<span id="cb24-8"><a href="#cb24-8"></a><span class="kw">plot</span>(chain_output)</span></code></pre></div>
<p><img src="lecture-27-fig/unnamed-chunk-6-1.png" /></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="kw">mean</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 3.376655</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="kw">sd</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 2.2654</code></pre>
</div>
<div class="slide section level2">

<p>Why not always have a really diffuse proposal distribution?</p>
<ul>
<li><p>Tradeoff between exploring the space well and proposing high probability moves.</p></li>
<li><p>With a diffuse proposal distribution, many of the proposals are to low-probability areas, and the chain stays in the same place a lot.</p></li>
</ul>
<div class="incremental">
<p>Plot below shows how far the chain moved on each step when we used the diffuse proposal distribution:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="kw">plot</span>(<span class="kw">diff</span>(chain_output))</span></code></pre></div>
<p><img src="lecture-27-fig/unnamed-chunk-7-1.png" /></p>
</div>
</div>
<div class="slide section level2">

<p>Overall:</p>
<ul>
<li><p>Choosing the proposal distribution involves balancing between moving long distances and having a high proportion of the moves accepted.</p></li>
<li><p>This will be problem-specific, and you often have to experiment with different proposal distributions.</p></li>
<li><p>There are more formal diagnostics for assessing convergence, but you should really look at the plots of the parameters.</p></li>
<li><p>You can never be sure that your chain isn’t completely missing part of the space.</p></li>
</ul>
</div>
<div id="summing-up" class="slide section level2">
<h1>Summing up</h1>
<p>Metropolis Hastings is a simple, general-purpose method for creating a Markov chain with a specified stationary distribution. It is particularly useful when:</p>
<ul>
<li><p>You only know the target distribution up to a constant of proportionality.</p></li>
<li><p>All the regions of high density in the target distribution are connected to each other.</p></li>
<li><p>The target distribution is high dimensional.</p></li>
</ul>
<div class="incremental">
<p>You should be scared because:</p>
<ul>
<li><p>You can never be sure that your chain has explored the space adequately</p></li>
<li><p>There is theory on convergence times, but they tend to say you have to run your chain past the heat death of the universe.</p></li>
</ul>
</div>
<div class="incremental">
<p>Next time:</p>
<ul>
<li>More interesting examples</li>
</ul>
</div>
</div>
</body>
</html>
