<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>lecture25</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="metropolis-hastings" class="slide section level2">
<h1>Metropolis Hastings</h1>
<p>Today: Metropolis Hastings</p>
<p>Reading:</p>
<ul class="incremental">
<li><p>Lange, Chapter 24.1, 24.2</p></li>
<li><p>More fun: <a
href="https://math.uchicago.edu/~shmuel/Network-course-readings/MCMCRev.pdf">Section
1 of this paper has an interesting example</a></p></li>
<li><p>Some practical advice about using Markov chains by Charles Geyer:
<a href="http://users.stat.umn.edu/~geyer/mcmc/one.html">One long
run</a>, <a href="http://users.stat.umn.edu/~geyer/mcmc/burn.html">Burn
in is unnecessary</a>, <a
href="http://users.stat.umn.edu/~geyer/mcmc/diag.html">Bogosity of MCMC
diagnostics</a></p></li>
</ul>
</div>
<div id="our-goals" class="slide section level2">
<h1>Our goals</h1>
<ul class="incremental">
<li><p>Sample from any probability distribution <span
class="math inline">\(\pi\)</span>.</p></li>
<li><p>Compute expected value of functions of random variables drawn
from these distributions, <span class="math inline">\(E_{X \sim
\pi}(f(X))\)</span></p></li>
</ul>
<div class="incremental">
<ul class="incremental">
<li><p>Last time we saw that if a Markov chain has a invariant
distribution <span class="math inline">\(\pi\)</span>, we can estimate
<span class="math inline">\(E_{X \sim \pi}(f(X))\)</span> as <span
class="math inline">\(\frac{1}{n} \sum_{i=1}^n f(X_i)\)</span>, where
<span class="math inline">\(X_1,X_2, \ldots, X_n\)</span> are drawn from
the Markov chain.</p></li>
<li><p>Metropolis-Hastings will let us specify a invariant distribution
<span class="math inline">\(\pi\)</span> and build a Markov chain having
<span class="math inline">\(\pi\)</span> as its invariant
distribution.</p></li>
</ul>
</div>
</div>
<div id="metropolis-hastings-the-idea" class="slide section level2">
<h1>Metropolis-Hastings: The Idea</h1>
<ul class="incremental">
<li><p>Start off with a Markov chain that has the wrong invariant
distribution, e.g., a random walk</p></li>
<li><p>Modify the chain so that it spends more time in regions of high
probability under the target distribution.</p></li>
</ul>
</div>
<div
id="metropolis-hastings-algorithm-with-a-symmetric-proposal-distribution"
class="slide section level2">
<h1>Metropolis-Hastings: Algorithm with a symmetric proposal
distribution</h1>
<p>Given:</p>
<ul class="incremental">
<li><p>A proposal distribution <span class="math inline">\(q\)</span>
such that <span class="math inline">\(q(y \mid x) = q(x \mid
y)\)</span></p></li>
<li><p>A target invariant distribution <span
class="math inline">\(\pi\)</span></p></li>
</ul>
<div class="incremental">
<p>Pick a starting value for the chain <span
class="math inline">\(X_0\)</span>.</p>
<p>For <span class="math inline">\(i = 1, \ldots, n\)</span>:</p>
<ul class="incremental">
<li><p>Pick a proposed move from <span class="math inline">\(Y \sim
q(\cdot \mid X_{i-1})\)</span></p></li>
<li><p>Compute the acceptance probability: <span class="math display">\[
a = \text{min} \left \{ \frac{\pi(Y)}{\pi(X_{i-1})}, 1 \right\}
\]</span></p></li>
<li><p>Let <span class="math inline">\(X_i\)</span> be <span
class="math display">\[
X_i = \begin{cases}
Y &amp; \text{w.p. } a \\
X_{i-1} &amp; \text{w.p. }1 - a
\end{cases}
\]</span></p></li>
</ul>
</div>
<div class="incremental">
<p>Note:</p>
<ul class="incremental">
<li><p>If the proposed state has a higher probability than the current
state, we move there deterministically.</p></li>
<li><p>If the proposed state has a lower probability than the current
state, we move there with probability proportional to the ratio of the
probabilities.</p></li>
</ul>
</div>
</div>
<div id="aside-reversible-chains-and-detailed-balance"
class="slide section level2">
<h1>Aside: Reversible chains and detailed balance</h1>
<ul class="incremental">
<li><p>Last time we talked about conditions for a Markov chain to have
an invariant distribution.</p></li>
<li><p>Some Markov chains are “reversible”. This is a stronger condition
than the chain simply having an invariant distribution.</p></li>
<li><p>To check whether a chain is reversible, we check the “detailed
balance” condition: A Markov chain with transition probability matrix
<span class="math inline">\(P\)</span> and invariant distribution <span
class="math inline">\(\pi\)</span> satisfies the <em>detailed
balance</em> equations and is <em>reversible</em> if <span
class="math display">\[
\pi_i P_{ij} = \pi_j P_{ji}
\]</span></p></li>
</ul>
</div>
<div
id="checking-that-the-metropolis-algorithm-has-the-correct-invariant-distribution"
class="slide section level2">
<h1>Checking that the Metropolis algorithm has the correct invariant
distribution</h1>
<p>Let <span class="math inline">\(X_i\)</span> and <span
class="math inline">\(X_j\)</span> be any two elements in the state
space and let <span class="math inline">\(a_{ij}\)</span> be the
acceptance probability given that we start at <span
class="math inline">\(X_i\)</span> and propose <span
class="math inline">\(X_j\)</span>.</p>
<p>Suppose further that <span class="math inline">\(\pi(X_j) \le
\pi(X_i)\)</span> so that <span class="math inline">\(a_{ij} \le
1\)</span></p>
<p>We can check that <span class="math inline">\(\pi\)</span> is the
invariant distribution by checking detailed balance:</p>
<p><span class="math display">\[
\begin{align*}
\pi(X_i)q(X_j \mid X_i) a_{ij} &amp;= \pi(X_i) q(X_j \mid X_i)
\frac{\pi(X_j)}{\pi(X_i)} \\
&amp;= q(X_j \mid X_i) \pi(X_j) \\
&amp;= \pi(X_j) q(X_i \mid X_j) a_{ji}
\end{align*}
\]</span></p>
<p>The last line follows because <span class="math inline">\(a_{ji} =
1\)</span> and the proposal distribution is symmetric.</p>
</div>
<div id="metropolis-hastings-with-non-symmetric-proposal-distribution"
class="slide section level2">
<h1>Metropolis-Hastings with non-symmetric proposal distribution</h1>
<p>Given:</p>
<ul class="incremental">
<li><p>A proposal distribution <span class="math inline">\(q\)</span>,
not necessarily such that <span class="math inline">\(q(y \mid x) = q(x
\mid y)\)</span></p></li>
<li><p>A target invariant distribution <span
class="math inline">\(\pi\)</span></p></li>
</ul>
<div class="incremental">
<p>Pick a starting value for the chain <span
class="math inline">\(X_0\)</span>.</p>
<p>For <span class="math inline">\(i = 1, \ldots, n\)</span>:</p>
<ul class="incremental">
<li><p>Pick a proposed move from <span class="math inline">\(Y \sim
q(\cdot \mid X_{i-1})\)</span></p></li>
<li><p>Compute the acceptance probability: <span class="math display">\[
a = \text{min} \left \{ \frac{\pi(Y)q(X_{i-1} \mid Y)}{\pi(X_{i-1})q(Y
\mid X_{i-1})}, 1 \right\}
\]</span></p></li>
<li><p>Let <span class="math inline">\(X_i\)</span> be <span
class="math display">\[
X_i = \begin{cases}
Y &amp; \text{w.p. } a \\
X_{i-1} &amp; \text{w.p. }1 - a
\end{cases}
\]</span></p></li>
</ul>
</div>
<div class="incremental">
<p>Notes:</p>
<ul class="incremental">
<li><p>The difference in the acceptance probability accounts for the
fact that the proposal distribution favors some parts of the space over
others.</p></li>
<li><p>You can derive the acceptance probability from the detailed
balance equations or check the detailed balance equations with the
acceptance probabilities given here.</p></li>
</ul>
</div>
</div>
<div id="simple-example-normal-distribution"
class="slide section level2">
<h1>Simple Example: Normal Distribution</h1>
<ul class="incremental">
<li><p>Proposal distribution: <span class="math inline">\(q(x \mid y) =
N(y, .3)\)</span></p></li>
<li><p>Target distribution: <span class="math inline">\(\pi(x) =
N(0,1)\)</span></p></li>
<li><p>Start at <span class="math inline">\(X_0 = -10\)</span></p></li>
</ul>
<div class="incremental">
<ul class="incremental">
<li>Notice that <span class="math inline">\(q\)</span> is symmetric, and
so the acceptance probability for moving from <span
class="math inline">\(X\)</span> to <span
class="math inline">\(Y\)</span> is <span
class="math inline">\(\text{min}\{\pi(Y) / \pi(X), 1\}\)</span></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>sample_with_symmetric_proposal <span class="ot">&lt;-</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>    <span class="cf">function</span>(proposal_function, target_distribution, current_state) {</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>        proposal <span class="ot">&lt;-</span> <span class="fu">proposal_function</span>(current_state)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>        acceptance_probability <span class="ot">&lt;-</span> <span class="fu">min</span>(<span class="dv">1</span>, <span class="fu">target_distribution</span>(proposal) <span class="sc">/</span> <span class="fu">target_distribution</span>(current_state))</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>        <span class="cf">if</span>(<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;=</span> acceptance_probability) {</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>            <span class="fu">return</span>(proposal)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>            <span class="fu">return</span>(current_state)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>        }   </span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>}</span></code></pre></div>
</div>
</div>
<div class="slide section level2">

<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="do">## The proposal distribution is normal, centered at the current state, standard deviation .3</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>proposal_function <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, <span class="at">mean =</span> x, <span class="at">sd =</span> .<span class="dv">3</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="do">## The target distribution is N(0,1)</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>target_distribution <span class="ot">&lt;-</span> dnorm</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="do">## check the sampling:</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="fu">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, <span class="sc">-</span><span class="dv">10</span>)</span></code></pre></div>
<pre><code>## [1] -10</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>n_samples <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>chain_output <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_samples)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>chain_output[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n_samples) {</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    chain_output[i] <span class="ot">&lt;-</span> <span class="fu">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>}</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="fu">plot</span>(chain_output)</span></code></pre></div>
<p><img src="lecture-25-fig/unnamed-chunk-2-1.png" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="fu">mean</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] -0.9162604</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">sd</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 1.871455</code></pre>
</div>
<div class="slide section level2">

<p>Notice:</p>
<ul class="incremental">
<li><p>We see that before about 200 steps, the chain has not converged
to its invariant distribution.</p></li>
<li><p>Even after it has converged, the elements in the chain are not
independent.</p></li>
</ul>
</div>
<div class="slide section level2">

<p>Let’s run the chain longer and discard the first 200 steps as
“burn-in”</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>n_samples <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>chain_output <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_samples)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>chain_output[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n_samples) {</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    chain_output[i] <span class="ot">&lt;-</span> <span class="fu">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>}</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>chain_no_burnin <span class="ot">&lt;-</span> chain_output[<span class="dv">201</span><span class="sc">:</span>n_samples]</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="fu">plot</span>(chain_no_burnin)</span></code></pre></div>
<p><img src="lecture-25-fig/unnamed-chunk-3-1.png" /></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">mean</span>(chain_no_burnin)</span></code></pre></div>
<pre><code>## [1] -0.03140313</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">sd</span>(chain_no_burnin)</span></code></pre></div>
<pre><code>## [1] 1.019056</code></pre>
</div>
<div class="slide section level2">

<p>Note:</p>
<ul class="incremental">
<li><p>This chain looks like it has reached its invariant distribution,
and we can confirm that by checking that the sample expected values
match what they should be for a <span
class="math inline">\(N(0,1)\)</span> distribution.</p></li>
<li><p>The ergodic theorem doesn’t require that we discard the burn-in
period, but people often do anyway.</p></li>
</ul>
</div>
<div id="example-2-mixture-distributions" class="slide section level2">
<h1>Example 2: Mixture distributions</h1>
<ul class="incremental">
<li><p>Proposal distribution: <span class="math inline">\(q(x \mid y) =
N(y, .3)\)</span></p></li>
<li><p>Target distribution: Let <span class="math inline">\(\phi_{\mu,
\sigma}(x)\)</span> be the pdf for a <span class="math inline">\(N(\mu,
\sigma)\)</span> random variable. Our target distribution is <span
class="math inline">\(\pi(x) = .25 \phi_{1, 1}(x) + .75
\phi_{5,.2}(x)\)</span>.</p></li>
<li><p>Start at <span class="math inline">\(X_0 = -10\)</span></p></li>
</ul>
<div class="incremental">
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="do">## The proposal distribution is normal, centered at the current state, standard deviation .3</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>proposal_function <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, <span class="at">mean =</span> x, <span class="at">sd =</span> .<span class="dv">3</span>)</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="do">## The target distribution is a mixture of N(1,1) and N(5,.2)</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>target_distribution <span class="ot">&lt;-</span> <span class="cf">function</span>(x) .<span class="dv">25</span> <span class="sc">*</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>) <span class="sc">+</span> .<span class="dv">75</span> <span class="sc">*</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> .<span class="dv">2</span>)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>n_samples <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>chain_output <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_samples)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>chain_output[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n_samples) {</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>    chain_output[i] <span class="ot">&lt;-</span> <span class="fu">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>}</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a><span class="fu">plot</span>(chain_output)</span></code></pre></div>
<p><img src="lecture-25-fig/unnamed-chunk-4-1.png" /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="fu">mean</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 0.3720607</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="fu">sd</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 2.266502</code></pre>
</div>
<div class="incremental">
<p>Notice:</p>
<ul class="incremental">
<li><p>Still takes about 200 steps to get to something that looks like a
invariant distribution</p></li>
<li><p>No samples with values above 2!</p></li>
</ul>
</div>
</div>
<div class="slide section level2">

<p>Let’s try running the chain a lot longer:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>n_samples <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>chain_output <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_samples)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>chain_output[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n_samples) {</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>    chain_output[i] <span class="ot">&lt;-</span> <span class="fu">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>}</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a><span class="fu">plot</span>(chain_output)</span></code></pre></div>
<p><img src="lecture-25-fig/unnamed-chunk-5-1.png" /></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="fu">mean</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 4.042973</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="fu">sd</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 1.780207</code></pre>
<p>What’s happening?</p>
</div>
<div class="slide section level2">

<p>Another way to fix this: change the proposal distribution.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>proposal_function <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, <span class="at">mean =</span> x, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>n_samples <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>chain_output <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_samples)</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>chain_output[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span></span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n_samples) {</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a>    chain_output[i] <span class="ot">&lt;-</span> <span class="fu">sample_with_symmetric_proposal</span>(proposal_function, target_distribution, chain_output[i<span class="dv">-1</span>])</span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>}</span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a><span class="fu">plot</span>(chain_output)</span></code></pre></div>
<p><img src="lecture-25-fig/unnamed-chunk-6-1.png" /></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="fu">mean</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 3.376655</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="fu">sd</span>(chain_output)</span></code></pre></div>
<pre><code>## [1] 2.2654</code></pre>
</div>
<div class="slide section level2">

<p>Why not always have a really diffuse proposal distribution?</p>
<ul class="incremental">
<li><p>Tradeoff between exploring the space well and proposing high
probability moves.</p></li>
<li><p>With a diffuse proposal distribution, many of the proposals are
to low-probability areas, and the chain stays in the same place a
lot.</p></li>
</ul>
<div class="incremental">
<p>Plot below shows how far the chain moved on each step when we used
the diffuse proposal distribution:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">diff</span>(chain_output))</span></code></pre></div>
<p><img src="lecture-25-fig/unnamed-chunk-7-1.png" /></p>
</div>
</div>
<div class="slide section level2">

<p>Overall:</p>
<ul class="incremental">
<li><p>Choosing the proposal distribution involves balancing between
moving long distances and having a high proportion of the moves
accepted.</p></li>
<li><p>This will be problem-specific, and you often have to experiment
with different proposal distributions.</p></li>
<li><p>There are more formal diagnostics for assessing convergence, but
you should really look at the plots of the parameters.</p></li>
<li><p>You can never be sure that your chain isn’t completely missing
part of the space.</p></li>
</ul>
</div>
<div id="summing-up" class="slide section level2">
<h1>Summing up</h1>
<p>Metropolis Hastings is a simple, general-purpose method for creating
a Markov chain with a specified invariant distribution. It is
particularly useful when:</p>
<ul class="incremental">
<li><p>You only know the target distribution up to a constant of
proportionality.</p></li>
<li><p>All the regions of high density in the target distribution are
connected to each other.</p></li>
<li><p>The target distribution is high dimensional.</p></li>
</ul>
<div class="incremental">
<p>You should be scared because:</p>
<ul class="incremental">
<li><p>You can never be sure that your chain has explored the space
adequately</p></li>
<li><p>There is theory on convergence times, but they tend to say you
have to run your chain past the heat death of the universe.</p></li>
</ul>
</div>
<div class="incremental">
<p>Next time:</p>
<ul class="incremental">
<li>More interesting examples</li>
</ul>
</div>
</div>
</body>
</html>
