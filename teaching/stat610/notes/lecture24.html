<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>lecture24</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="monte-carlo-methods-markov-chains" class="slide section level2">
<h1>Monte Carlo methods: Markov Chains</h1>
<p>Today: Markov Chains</p>
<p>Our goals:</p>
<ul class="incremental">
<li><p>Building towards Markov chain monte carlo</p></li>
<li><p>We will eventually use Markov chains to get samples from arbitrary probability distributions</p></li>
<li><p>Today we will just look at Markov chains, define them, establish some of their properties</p></li>
</ul>
<p>Reading: <a href="http://www.statslab.cam.ac.uk/~rrw1/markov/M.pdf">Notes from Richard Weber’s course on Markov chains</a>, or Lange, Chapter 23.</p>
<ul class="incremental">
<li><p>Weber’s notes have proofs and examples, and much more detail than we’ll use here. I’m following his notation.</p></li>
<li><p>Lange Chapter 23 is quite succinct.</p></li>
</ul>
</div>
<div id="a-silly-example" class="slide section level2">
<h1>A silly example</h1>
<p><img src="Frog.png" /></p>
<p>A frog hops around on some lily pads, choosing where to hop somewhat randomly.</p>
<p>We would like to be able to make probability statements about where the frog is.</p>
<ul class="incremental">
<li><p>After <span class="math inline">\(n\)</span> hops, what is the probability that the frog is on the 3rd lily pad?</p></li>
<li><p>As <span class="math inline">\(n \to \infty\)</span>, what is the probability that the frog is on the 3rd lily pad? Does the starting location of the frog matter?</p></li>
</ul>
</div>
<div id="definitions" class="slide section level2">
<h1>Definitions</h1>
<div class="incremental">
<ul class="incremental">
<li><p>Let <span class="math inline">\(I\)</span> be a countable set, <span class="math inline">\(\{ i, j, k, \ldots \}\)</span>.</p></li>
<li><p>Each <span class="math inline">\(i \in I\)</span> is called a <em>state</em> and <span class="math inline">\(I\)</span> is the <em>state space</em>.</p></li>
<li><p>Let <span class="math inline">\(\lambda = (\lambda_i, i \in I)\)</span> be a row vector with <span class="math inline">\(i\)</span>th element <span class="math inline">\(\lambda_i\)</span> such that <span class="math inline">\(\sum_{i} \lambda_i = 1\)</span>. <span class="math inline">\(\lambda\)</span> defines the <em>initial distribution</em> over <span class="math inline">\(I\)</span>.</p></li>
<li><p>Let <span class="math inline">\(P = (p_{ij}: i,j \in I)\)</span> with <span class="math inline">\(p_{ij} \ge 0\)</span> for all <span class="math inline">\(i,j\)</span> and such that <span class="math inline">\(\sum_{j \in I} p_{ij} = 1\)</span>. <span class="math inline">\(P\)</span> is a transition probability matrix.</p></li>
<li><p>A sequence of random variables, <span class="math inline">\((X_n)_{n \ge 0}\)</span> is a <em>Markov chain</em> with initial distribution <span class="math inline">\(\lambda\)</span> and transition matrix <span class="math inline">\(P\)</span> if for all <span class="math inline">\(n \ge 0\)</span>, and all <span class="math inline">\(i \in I\)</span>:</p></li>
<li><p><span class="math inline">\(P(X_0 = i) = \lambda_i\)</span></p></li>
<li><p><span class="math inline">\(P(X_{n+1}= i_{n+1}\mid X_0 = i_0, \ldots, X_n = i_n) = P(X_{n+1} = i_{n+1} \mid X_{n} = i_n) = p_{i_n, i_{n+1}}\)</span></p></li>
</ul>
</div>
</div>
<div id="a-silly-example-1" class="slide section level2">
<h1>A silly example</h1>
<p><img src="Frog.png" /></p>
<p>A frog hops around on some lily pads, choosing where to hop somewhat randomly.</p>
<ul class="incremental">
<li><p>State space: <span class="math inline">\(I = \{1,2,3\}\)</span> (the three lily pads)</p></li>
<li><p>Starting distribution <span class="math inline">\(\lambda = (1, 0,0)\)</span> (starts on lily pad 1)</p></li>
<li><p>Transition matrix: <span class="math inline">\(P = \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 0 &amp; .5 &amp; .5 \\ .5 &amp; 0 &amp; .5 \end{pmatrix}\)</span></p></li>
</ul>
</div>
<div id="n-step-transition-matrix" class="slide section level2">
<h1>n-step transition matrix</h1>
<p>Given a starting distribution <span class="math inline">\(\lambda\)</span>, we would like to know <span class="math inline">\(P(X_n = j)\)</span>.</p>
<p>We will use the notation <span class="math inline">\(P_i(A) = P(A \mid X_0 = i)\)</span> for <span class="math inline">\(A\)</span> any event.</p>
<div class="incremental">
<p>For <span class="math inline">\(n = 1\)</span>, we know that <span class="math display">\[
P(X_1 = j) = \sum_{i \in I} \lambda_i P_i(X_1 = j) = \sum_{i \in I} \lambda_i p_{ij} = (\lambda P)_j
\]</span></p>
</div>
<div class="incremental">
<p>For <span class="math inline">\(n = 2\)</span>, we can write <span class="math display">\[
P_i(X_2 = j) = \sum_k P_i(X_1 = k, X_2 = j) = \sum_k p_{ik} p_{kj} = (P^2)_{ij}
\]</span></p>
</div>
<div class="incremental">
<p><span class="math display">\[
P(X_2 = j) = \sum_{i,k} \lambda_i P_i(X_1 = k, X_2 = j) = \sum_{i,k} \lambda_i p_{ik} p_{kj} = (\lambda P^2)_j
\]</span></p>
</div>
<div class="incremental">
<p>So in general, if <span class="math inline">\(\delta_i\)</span> is the vector with a 1 at index <span class="math inline">\(i\)</span> and 0’s everywhere else, <span class="math display">\[
P_i(X_n = j) = (\delta_i P^n)_j = (P^n)_{ij}
\]</span> and <span class="math display">\[
P(X_n = j) = \sum_i \lambda_i P_i(X_n = j) = (\lambda P^n)_j
\]</span></p>
</div>
<div class="incremental">
<p>So the <span class="math inline">\(n\)</span>-step transition matrix is simply <span class="math inline">\(P^n\)</span>.</p>
</div>
</div>
<div id="example-of-n-step-transition-matrix-computations" class="slide section level2">
<h1>Example of n-step transition matrix computations</h1>
<p><img src="Frog.png" /></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>P =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">3</span>, <span class="dt">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a>P[<span class="dv">1</span>,<span class="dv">2</span>] =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>P[<span class="dv">2</span>,<span class="dv">2</span>] =<span class="st"> </span>P[<span class="dv">2</span>,<span class="dv">3</span>] =<span class="st"> </span>P[<span class="dv">3</span>,<span class="dv">1</span>] =<span class="st"> </span>P[<span class="dv">3</span>,<span class="dv">3</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>P</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  0.0  1.0  0.0
## [2,]  0.0  0.5  0.5
## [3,]  0.5  0.0  0.5</code></pre>
<div class="incremental">
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>lambda =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb3-2"><a href="#cb3-2"></a>lambda <span class="op">%*%</span><span class="st"> </span>P</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    0    1    0</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>lambda <span class="op">%*%</span><span class="st"> </span>P <span class="op">%*%</span><span class="st"> </span>P</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    0  0.5  0.5</code></pre>
</div>
</div>
<div class="slide section level2">

<p>What happens after a long time?</p>
<div class="incremental">
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="co">## computes P^(2^k)</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>pow_P =<span class="st"> </span><span class="cf">function</span>(P, k) {</span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</span>
<span id="cb7-4"><a href="#cb7-4"></a>        P =<span class="st"> </span>P <span class="op">%*%</span><span class="st"> </span>P</span>
<span id="cb7-5"><a href="#cb7-5"></a>    }</span>
<span id="cb7-6"><a href="#cb7-6"></a>    <span class="kw">return</span>(P)</span>
<span id="cb7-7"><a href="#cb7-7"></a>}</span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="co">## 8-step probabilities </span></span>
<span id="cb7-9"><a href="#cb7-9"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##          [,1]      [,2]      [,3]
## [1,] 0.203125 0.3984375 0.3984375</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co">## 16-step probabilities </span></span>
<span id="cb9-2"><a href="#cb9-2"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P, <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##           [,1]      [,2]      [,3]
## [1,] 0.2000122 0.3999939 0.3999939</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="co">## 32-step probabilities </span></span>
<span id="cb11-2"><a href="#cb11-2"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  0.2  0.4  0.4</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co">## 64-step probabilities </span></span>
<span id="cb13-2"><a href="#cb13-2"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  0.2  0.4  0.4</code></pre>
</div>
</div>
<div class="slide section level2">

<p>What if the frog starts at a different location?</p>
<div class="incremental">
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="co">## 2^6-step probabilities, starting at 2</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>lambda =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb15-3"><a href="#cb15-3"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  0.2  0.4  0.4</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="co">## 2^6-step probabilities, starting at 3</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>lambda =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb17-3"><a href="#cb17-3"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  0.2  0.4  0.4</code></pre>
</div>
</div>
<div class="slide section level2">

<p>In this case, no matter where the frog starts, we get the same probabilities:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="kw">pow_P</span>(P, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  0.2  0.4  0.4
## [2,]  0.2  0.4  0.4
## [3,]  0.2  0.4  0.4</code></pre>
</div>
<div id="another-example" class="slide section level2">
<h1>Another example</h1>
<p><img src="Frog-big.png" /></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>P_big =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">7</span>, <span class="dt">ncol =</span> <span class="dv">7</span>)</span>
<span id="cb21-2"><a href="#cb21-2"></a>P_big[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>] =<span class="st"> </span>P</span>
<span id="cb21-3"><a href="#cb21-3"></a>P_big[<span class="dv">4</span>,<span class="dv">3</span><span class="op">:</span><span class="dv">5</span>] =<span class="st"> </span><span class="kw">c</span>(.<span class="dv">25</span>, <span class="fl">.5</span>, <span class="fl">.25</span>)</span>
<span id="cb21-4"><a href="#cb21-4"></a>P_big[<span class="dv">5</span>, <span class="dv">6</span><span class="op">:</span><span class="dv">7</span>] =<span class="st"> </span><span class="kw">c</span>(.<span class="dv">5</span>, <span class="fl">.5</span>)</span>
<span id="cb21-5"><a href="#cb21-5"></a>P_big[<span class="dv">6</span>,<span class="dv">4</span>] =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb21-6"><a href="#cb21-6"></a>P_big[<span class="dv">7</span>,<span class="dv">7</span>] =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb21-7"><a href="#cb21-7"></a>P_big</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]  0.0  1.0 0.00  0.0 0.00  0.0  0.0
## [2,]  0.0  0.5 0.50  0.0 0.00  0.0  0.0
## [3,]  0.5  0.0 0.50  0.0 0.00  0.0  0.0
## [4,]  0.0  0.0 0.25  0.5 0.25  0.0  0.0
## [5,]  0.0  0.0 0.00  0.0 0.00  0.5  0.5
## [6,]  0.0  0.0 0.00  1.0 0.00  0.0  0.0
## [7,]  0.0  0.0 0.00  0.0 0.00  0.0  1.0</code></pre>
</div>
<div class="slide section level2">

<div class="incremental">
<p>Let’s do the same thing: if we start the frog at different locations, compute the probability of being at any of the lily pads after 64 steps.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="co">## 2^6-step probabilities, starting at 1</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>lambda =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>))</span>
<span id="cb23-3"><a href="#cb23-3"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P_big, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]  0.2  0.4  0.4    0    0    0    0</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="co">## 2^6-step probabilities, starting at 7</span></span>
<span id="cb25-2"><a href="#cb25-2"></a>lambda =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>), <span class="dv">1</span>)</span>
<span id="cb25-3"><a href="#cb25-3"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P_big, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    0    0    0    0    0    0    1</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="co">## 2^6-step probabilities, starting at 6</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>lambda =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb27-3"><a href="#cb27-3"></a>lambda <span class="op">%*%</span><span class="st"> </span><span class="kw">pow_P</span>(P_big, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##           [,1]      [,2]      [,3]         [,4]         [,5]         [,6]
## [1,] 0.1333333 0.2666667 0.2666667 1.905188e-09 6.499812e-10 4.435003e-10
##           [,7]
## [1,] 0.3333333</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="kw">round</span>(<span class="kw">pow_P</span>(P_big, <span class="dv">6</span>), <span class="dt">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,] 0.20 0.40 0.40    0    0    0 0.00
## [2,] 0.20 0.40 0.40    0    0    0 0.00
## [3,] 0.20 0.40 0.40    0    0    0 0.00
## [4,] 0.13 0.27 0.27    0    0    0 0.33
## [5,] 0.07 0.13 0.13    0    0    0 0.67
## [6,] 0.13 0.27 0.27    0    0    0 0.33
## [7,] 0.00 0.00 0.00    0    0    0 1.00</code></pre>
</div>
</div>
<div class="slide section level2">

<p>Our next goal is to find the conditions under which this convergence occurs.</p>
<p>We’ll need two concepts:</p>
<ul class="incremental">
<li><p>Communicating classes/irreducibility</p></li>
<li><p>Invariant distributions</p></li>
</ul>
</div>
<div id="class-structure" class="slide section level2">
<h1>Class structure</h1>
<p>We say state <span class="math inline">\(i\)</span> leads to <span class="math inline">\(j\)</span>, <span class="math inline">\(i \to j\)</span> if <span class="math display">\[
P_i(X_n = j \text{ for some } n \ge 0) &gt; 0
\]</span></p>
<div class="incremental">
<p>We say that <span class="math inline">\(i\)</span> communicates with <span class="math inline">\(j\)</span> and write <span class="math inline">\(i \leftrightarrow j\)</span> if <span class="math inline">\(i \to j\)</span> and <span class="math inline">\(j \to i\)</span>.</p>
</div>
</div>
<div id="communicating-classes" class="slide section level2">
<h1>Communicating classes</h1>
<p>From what we saw before, we can see that</p>
<ul class="incremental">
<li><span class="math inline">\(i \leftrightarrow j\)</span> and <span class="math inline">\(j \leftrightarrow k\)</span> implies <span class="math inline">\(i \leftrightarrow k\)</span></li>
</ul>
<p>If we add that <span class="math inline">\(i \leftrightarrow i\)</span>, <span class="math inline">\(\leftrightarrow\)</span> satisfies the conditions for an <a href="https://en.wikipedia.org/wiki/Equivalence_relation">equivalence relation</a>.</p>
<p>We use <span class="math inline">\(\leftrightarrow\)</span> to partition the state space <span class="math inline">\(I\)</span> into communicating clases.</p>
<div class="incremental">
<p><img src="Frog-big.png" /></p>
</div>
</div>
<div id="irreducibility" class="slide section level2">
<h1>Irreducibility</h1>
<p>We will want Markov chains that are <em>irreducible</em>.</p>
<p>A chain with transition matrix <span class="math inline">\(P\)</span> is <em>irreducible</em> if the state space <span class="math inline">\(I\)</span> forms a single class.</p>
<div class="incremental">
<p>Is the little frog Markov chain irreducible?</p>
<p><img src="Frog.png" /></p>
</div>
</div>
<div id="invariant-distributions" class="slide section level2">
<h1>Invariant distributions</h1>
<p>Definition:</p>
<ul class="incremental">
<li><p>Let <span class="math inline">\(\lambda \in \mathbb R^n\)</span> be a row vector such that <span class="math inline">\(\sum_i \lambda_i = 1\)</span>.</p></li>
<li><p><span class="math inline">\(\lambda\)</span> is an invariant distribution if <span class="math inline">\(\lambda P = \lambda\)</span>.</p></li>
</ul>
<div class="incremental">
<p>Interpretation:</p>
<ul class="incremental">
<li>If we start a chain by choosing <span class="math inline">\(X_0\)</span> with probabilities as given in <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(P(X_n = i) = \lambda_i\)</span> for any <span class="math inline">\(n, i\)</span>.</li>
</ul>
</div>
</div>
<div id="convergence-of-chains-to-equilibrium" class="slide section level2">
<h1>Convergence of chains to equilibrium</h1>
<p>(Theorem 9.8 in the linked notes)</p>
<p>Let <span class="math inline">\(P\)</span> be irreducible, positive recurrent, and aperiodic (we haven’t defined positive recurrent or aperiodic here, but you can look in the notes), with invariant distribution <span class="math inline">\(\pi\)</span>.</p>
<p>Then for any initial distribution, <span class="math inline">\(P(X_n = j) \to \pi_j\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
<div class="incremental">
<p>Interpretation: No matter where we start, the chain converges to a unique distribution.</p>
<ul class="incremental">
<li><p>We saw in the 3-lily-pad frog example that we got this convergence to an equilibrium distribution.</p></li>
<li><p>In the 7-lily-pad example, the starting position of the frog mattered.</p></li>
</ul>
</div>
</div>
<div id="ergodic-theorem" class="slide section level2">
<h1>Ergodic theorem</h1>
<p>Let <span class="math inline">\((X_n)_{0 \le n \le N}\)</span> be a Markov chain with transition matrix <span class="math inline">\(P\)</span>.</p>
<p>If <span class="math inline">\(P\)</span> is irreducible and positive recurrent, then for any bounded function <span class="math inline">\(f\)</span> we have <span class="math display">\[
P(\frac{1}{n} \sum_{k=1}^n f(X_k) \to \bar f \text{ as } n \to \infty) = 1
\]</span> where <span class="math display">\[
\bar f = \sum_{x} \pi_i f(i)
\]</span> and <span class="math inline">\(\pi\)</span> is the unique invariant distribution.</p>
<div class="incremental">
<p>Why is this useful?</p>
<ul class="incremental">
<li><p>We can use this like we use the law of large numbers.</p></li>
<li><p>If we want to compute <span class="math inline">\(E_{X \sim \pi}[(f(X))]\)</span>, we can run a Markov chain with invariant distribution <span class="math inline">\(\pi\)</span> and use <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n f(X_i)\)</span> as an estimate.</p></li>
</ul>
</div>
</div>
<div id="looking-forward" class="slide section level2">
<h1>Looking forward</h1>
<p>Now we know:</p>
<ul class="incremental">
<li><p>Markov chains, under certain conditions, have invariant distributions.</p></li>
<li><p>If we have a Markov chain and know its invariant distribution, we can use it to approximate expected values of random variables taken from that distribution.</p></li>
</ul>
<p>Next time:</p>
<ul class="incremental">
<li><p>Given a target distribution (usually a posterior), we will construct a Markov chain that has that target distribution as the invariant distribution.</p></li>
<li><p>The Markov chain won’t give us independent samples from the distribution, but it will allow us to approximate expected values of functions under that distribution.</p></li>
</ul>
<div class="incremental">
<p>By the way:</p>
<ul class="incremental">
<li><p>This semester we’re just using Markov chains to sample from probability distributions, but they are interesting models in their own right</p></li>
<li><p>The frog example seems silly, but it’s actually the model behind the initial version of google’s web page indexing algorithm. Lily pads are websites, connections between lily pads are links between websites. See the linked notes for a little more detail.</p></li>
</ul>
</div>
</div>
</body>
</html>
