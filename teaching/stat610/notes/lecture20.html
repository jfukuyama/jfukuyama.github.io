<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>lecture20</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="expectation-maximization" class="slide section level2">
<h1>Expectation Maximization</h1>
<p>Agenda today</p>
<ul class="incremental">
<li><p>Last day of optimization section</p></li>
<li><p>The EM algorithm</p></li>
</ul>
<p>Reading</p>
<ul class="incremental">
<li>Lange Chapter 10</li>
</ul>
</div>
<div id="problem-for-today" class="slide section level2">
<h1>Problem for today</h1>
<p>Goal, as usual: Maximize a likelihood</p>
<ul class="incremental">
<li>Particularly useful problems with “hidden data” or “missing
data”</li>
</ul>
<div class="incremental">
<p>The idea behind EM:</p>
<ul class="incremental">
<li><p>There are some likelihoods that would be easy to maximize if we
just had a little bit of extra data</p></li>
<li><p>We can guess at what the missing data should be and find the
parameters that maximize the likelihood based on our guess.</p></li>
<li><p>We then alternate between guessing at the missing data based on
the current parameter estimates and estimating the parameters based on
the guesses at the missing data.</p></li>
</ul>
</div>
</div>
<div id="for-example-clustering-with-normal-mixtures"
class="slide section level2">
<h1>For example: Clustering with normal mixtures</h1>
<p>Suppose we have a set of points measured on one variable.</p>
<p>We think that they come from two clusters, and we want to find the
centers of those clusters.</p>
<p><img src="lecture-20-fig/unnamed-chunk-1-1.png" /></p>
</div>
<div class="slide section level2">

<p>We can set up the following model for the data:</p>
<p><span class="math display">\[
\begin{align*}
Z_i &amp;=\begin{cases}
1 &amp; \text{w.p. } p\\
0 &amp; \text{w.p. } 1 - p
\end{cases}\\
Y_i \mid Z_i &amp;\sim N(\theta_{Z_i}, 1)
\end{align*}
\]</span></p>
<ul class="incremental">
<li><p><span class="math inline">\(Y_i\)</span>, <span
class="math inline">\(i = 1,\ldots, n\)</span> are the observed
data</p></li>
<li><p><span class="math inline">\(Z_i\)</span>, <span
class="math inline">\(i = 1,\ldots, n\)</span> are the cluster
assignments, or the “missing” or “unobserved” data.</p></li>
<li><p>The goal is to get maximum likelihood estimates of <span
class="math inline">\(\theta_0\)</span> and <span
class="math inline">\(\theta_1\)</span>, the means of the two clusters,
and <span class="math inline">\(p\)</span>, the probability of each
cluster.</p></li>
<li><p>If we observed the <span class="math inline">\(Z_i\)</span>’s,
this problem would be simple: <span class="math inline">\(\hat
\theta_0\)</span> would be the mean of the <span
class="math inline">\(Y_i\)</span>’s for which <span
class="math inline">\(Z_i = 0\)</span>, and <span
class="math inline">\(\hat \theta_1\)</span> would be the mean of the
<span class="math inline">\(Y_i\)</span>’s for which <span
class="math inline">\(Z_i = 1\)</span>.</p></li>
</ul>
</div>
<div id="likelihood-for-the-normal-mixture-example"
class="slide section level2">
<h1>Likelihood for the normal mixture example</h1>
<p>We will let <span class="math inline">\(\phi_\theta\)</span> be the
normal pdf function, <span class="math display">\[
\phi_\theta(y) = \frac{1}{2\pi} \exp \left(-\frac{(y - \theta)^2}{ 2}
\right)
\]</span> so that we don’t have to rewrite it every time.</p>
<p>Let <span class="math inline">\(y_i\)</span>, <span
class="math inline">\(i = 1,\ldots, n\)</span> be the observed data. In
this model, the observed-data likelihood for one point is: <span
class="math display">\[
p \phi_{\theta_1}(y_i) + (1 -p)\phi_{\theta_0}(y_i)
\]</span></p>
<p>And the overall observed-data log likelihood is <span
class="math display">\[
\log g(y \mid \theta) = \sum_{i=1}^n \log \left( p \phi_{\theta_1}(y_i)
+ (1 -p)\phi_{\theta_0}(y_i) \right)
\]</span></p>
<p>Note: this problem is not as well-behaved as the ones we have seen
before, the log likelihood in general has multiple local maxima.</p>
</div>
<div id="em-the-algorithm" class="slide section level2">
<h1>EM: The algorithm</h1>
<p>Suppose we have observed data <span class="math inline">\(Y\)</span>,
missing data <span class="math inline">\(Z\)</span>, complete data <span
class="math inline">\(X = (Y, Z)\)</span>, and parameters <span
class="math inline">\(\theta\)</span>.</p>
<p><span class="math inline">\(f(X\mid \theta)\)</span> is the
complete-data likelihood, <span class="math inline">\(g(Y \mid
\theta)\)</span> is the observed-data likelihood.</p>
<p>We would like to maximize <span class="math inline">\(g(Y \mid
\theta)\)</span> (or <span class="math inline">\(\log g(Y \mid
\theta)\)</span>)</p>
<ul class="incremental">
<li><p>Start with an initial estimate of the parameters <span
class="math inline">\(\theta^{(0)}\)</span></p></li>
<li><p>While you haven’t converged yet, repeat the following two
steps:</p></li>
<li><p>E step: compute <span class="math display">\[
Q(\theta \mid \theta^{(n)}) = E[\log f(X \mid \theta) \mid Y,
\theta^{(n)}]
\]</span></p></li>
<li><p>M step: Let <span class="math inline">\(\theta^{(n+1)}\)</span>
be the solution to <span class="math display">\[
\text{maximize}_\theta Q(\theta \mid \theta^{(n)})
\]</span></p></li>
</ul>
</div>
<div id="example-e-step-in-normal-mixtures"
class="slide section level2">
<h1>Example: E step in normal mixtures</h1>
<p>Our parameters are <span class="math inline">\(\theta\)</span> and
<span class="math inline">\(p\)</span>, with current estimates <span
class="math inline">\(\theta^{(n)}\)</span> and <span
class="math inline">\(p^{(n)}\)</span>. The complete-data log likelihood
is</p>
<p><span class="math display">\[
\log f(Y, Z \mid \theta, p) = \sum_{i=1}^n [(1 - z_i)
\log(\phi_{\theta_0}(y_i)) + z_i \log(\phi_{\theta_1}(y_i))] +
\sum_{i=1}^n [(1 - z_i) \log(1 - p) + z_i \log p]
\]</span></p>
<div class="incremental">
<p>In the E step, we compute the expectation of <span
class="math inline">\(\log f(y, z, \mid \theta)\)</span>, conditional on
the observed values of <span class="math inline">\(y\)</span> and the
current estimate of <span class="math inline">\(\theta\)</span>, <span
class="math inline">\(\theta^{(n)}\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
E[\log \;&amp;f(Y, Z \mid \theta, p)  \mid Y = y, \theta= \theta^{(n)},
p = p^{(n)}] \\
&amp;= \sum_{i=1}^n \left[(1 - E[z_i \mid Y = y, \theta=
\theta^{(n)}])\log(\phi_{\theta^{(n)}_0}(y_i)) + E[z_i \mid  Y = y,
\theta= \theta^{(n)}] \log(\phi_{\theta^{(n)}_1}(y_i))\right] +\\
&amp;\quad  \sum_{i=1}^n\left [(1 - E[z_i \mid Y = y, \theta=
\theta^{(n)}]) \log(1 - p^{(n)}) + E[z_i \mid Y = y, \theta=
\theta^{(n)}]\log p^{(n)}\right]
\end{align*}
\]</span></p>
</div>
<div class="incremental">
<p>Finally: <span class="math display">\[
E[z_i \mid Y = y, \theta= \theta^{(n)}]  =
\frac{p^{(n)}\phi_{\theta^{(n)}_1}(y_i)}{p^{(n)}\phi_{\theta^{(n)}_1}(y_i)
+ (1 - p^{(n)})\phi_{\theta^{(n)}_0}(y_i)}
\]</span></p>
</div>
</div>
<div class="slide section level2">

<p>Suppose our current estimates are <span
class="math inline">\(\theta_0^{(n)} = -1\)</span>, <span
class="math inline">\(\theta_2^{(n)} = 2\)</span>, and <span
class="math inline">\(p^{(n)} = .5\)</span></p>
<p>We can compute the quantities from the previous slide for the data we
generated:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>theta0 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>theta1 <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>expected_z <span class="ot">&lt;-</span> p <span class="sc">*</span> (<span class="fu">dnorm</span>(y, <span class="at">mean =</span> theta1)) <span class="sc">/</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    (p <span class="sc">*</span> (<span class="fu">dnorm</span>(y, <span class="at">mean =</span> theta1)) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> p) <span class="sc">*</span> <span class="fu">dnorm</span>(y, <span class="at">mean =</span> theta0))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(y, expected_z) <span class="sc">%&gt;%</span> <span class="fu">head</span>() <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##           y expected_z
## [1,] -0.488      0.049
## [2,] -1.610      0.002
## [3,]  2.379      0.996
## [4,]  0.785      0.702
## [5,] -0.875      0.016
## [6,]  2.955      0.999</code></pre>
</div>
<div id="example-m-step-in-normal-mixtures"
class="slide section level2">
<h1>Example: M step in normal mixtures</h1>
<p>You can either go through the analysis, or you can notice that
maximizing <span class="math inline">\(E[\log f(Y, Z \mid \theta, p)
\mid Y = y, \theta = \theta^{(n)}, p = p^{(n)}]\)</span> is simply a
problem of estimating the mean of a normal distribution with weights on
the samples.</p>
<p>If we let <span class="math inline">\(\gamma_i = E[z_i \mid Y = y,
\theta= \theta^{(n)}]\)</span>, then our new parameters are <span
class="math display">\[
\begin{align*}
\theta^{(n+1)}_0 &amp;= \frac{\sum_{i=1}^n (1 - \gamma_i)
y_i}{\sum_{i=1}^n (1 - \gamma_i)}\\
\theta^{(n+1)}_1 &amp;= \frac{\sum_{i=1}^n \gamma_i y_i}{\sum_{i=1}^n
\gamma_i}\\
p^{(n+1)} &amp;= \sum_{i=1}^n \gamma_i / n
\end{align*}
\]</span></p>
</div>
<div class="slide section level2">

<p>Let’s look at what the M step looks like on our data.</p>
<p>Remember our previous parameter estimates were <span
class="math inline">\(\theta_0^{(n)} = -1\)</span>, <span
class="math inline">\(\theta_2^{(n)} = 2\)</span>, and <span
class="math inline">\(p^{(n)} = .5\)</span>. The true centers are at
<span class="math inline">\(-2\)</span> and <span
class="math inline">\(3\)</span>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>(theta1_updated <span class="ot">&lt;-</span> <span class="fu">sum</span>(y <span class="sc">*</span> expected_z) <span class="sc">/</span> <span class="fu">sum</span>(expected_z))</span></code></pre></div>
<pre><code>## [1] 2.88475</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>(theta0_updated <span class="ot">&lt;-</span> <span class="fu">sum</span>(y <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> expected_z)) <span class="sc">/</span> <span class="fu">sum</span>(<span class="dv">1</span> <span class="sc">-</span> expected_z))</span></code></pre></div>
<pre><code>## [1] -1.599996</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>(p_updated <span class="ot">&lt;-</span> <span class="fu">sum</span>(expected_z) <span class="sc">/</span> n_samples)</span></code></pre></div>
<pre><code>## [1] 0.536838</code></pre>
</div>
<div id="example-2-allele-frequency-inference"
class="slide section level2">
<h1>Example 2: Allele frequency inference</h1>
<p>Blood typing has to do with the “ABO” locus.</p>
<ul class="incremental">
<li><p>Three alleles, A, B, and O</p></li>
<li><p>Every individual has two copies (so AA, BB, AO, BO, AB, OO are
the possible genotypes)</p></li>
<li><p>A and B are dominant over O, so we blood typing allows us to
observe the presence of A, the presence of B, the presence of both, or
the presence of neither (A, B, AB, O are the possible
phenotypes)</p></li>
<li><p>If we see phenotype A, the genotype could be either AO or AA, if
we see phenotype B, the genotype could be either BO or BB, phenotype AB
always corresponds to genotype AB, and phenotype O always corresponds to
genotype O.</p></li>
</ul>
</div>
<div class="slide section level2">

<p>Our goal: estimate the population proportions <span
class="math inline">\(p_A\)</span>, <span
class="math inline">\(p_B\)</span>, and <span
class="math inline">\(p_O\)</span>.</p>
<ul class="incremental">
<li><p>If we observed genotypes, this would be easy: count the number of
alleles of each type and divide by the total number of alleles.</p></li>
<li><p>We can’t see genotypes, only phenotypes, so we consider the
genotypes missing data and use EM.</p></li>
<li><p><span class="math inline">\(X\)</span> is the complete data
(number of individuals with AA, BB, AO, BO, AB, OO genotypes)</p></li>
<li><p><span class="math inline">\(Y\)</span> is the observed data
(number of individuals with A, B, AB, O phenotypes)</p></li>
</ul>
</div>
<div class="slide section level2">

<p>Step 1: Write down the complete data log likelihood</p>
<p>Hardy-Weinberg (a law from population biology) tells us the
probability of each genotype given the population allele proportions
under the assumption of random assortative mating.</p>
<ul class="incremental">
<li><p><span class="math inline">\(p_{AA} = p_A^2\)</span></p></li>
<li><p><span class="math inline">\(p_{BB} = p_B^2\)</span></p></li>
<li><p><span class="math inline">\(p_{AO} = 2 p_A p_O\)</span></p></li>
<li><p><span class="math inline">\(p_{BO} = 2 p_B p_O\)</span></p></li>
<li><p><span class="math inline">\(p_{AB} = 2 p_A p_B\)</span></p></li>
<li><p><span class="math inline">\(p_{OO} = p_O^2\)</span></p></li>
</ul>
<p>Therefore, the complete-data log likelihood is: <span
class="math display">\[
\log f(X \mid p_A, p_B, p_O) = n_{AA} \log p_A^2 + n_{AO} \log(2 p_A
p_O) + n_{BB} \log p_B^2 + n_{BO} \log (2 p_B p_O) + n_{AB} \log (2p_A
p_B) + n_O \log p_O^2 + C
\]</span></p>
</div>
<div class="slide section level2">

<p>Step 2: Write down expectation of complete data log likelihood</p>
<p><span class="math inline">\(E(\log f(X \mid p_A, p_B, p_O))\)</span>
will involve the expected genotype counts given the observed data. These
are:</p>
<ul class="incremental">
<li><p><span class="math inline">\(E(n_{AB} \mid Y, p_A, p_B, p_O) =
n_{AB}\)</span></p></li>
<li><p><span class="math inline">\(E(n_{OO} \mid Y, p_A, p_B, p_O) =
n_{O}\)</span></p></li>
<li><p><span class="math inline">\(E(n_{AA} \mid Y, p_A, p_B, p_O) = n_A
\frac{p_A^2}{p_A^2 + 2 p_A p_O}\)</span></p></li>
<li><p><span class="math inline">\(E(n_{AO} \mid Y, p_A, p_B, p_O) = n_A
\frac{2 p_A p_O}{p_A^2 + 2 p_A p_O}\)</span></p></li>
</ul>
<p>And similarly for <span class="math inline">\(n_{BB}\)</span> and
<span class="math inline">\(n_{BO}\)</span>.</p>
</div>
<div class="slide section level2">

<p>Step 3: Write down the values of <span
class="math inline">\(p_A\)</span>, <span
class="math inline">\(p_B\)</span>, and <span
class="math inline">\(p_O\)</span> that maximize the expected complete
data log likelihood</p>
<ul class="incremental">
<li><p><span class="math inline">\(p_A = \frac{2n_{AA} + n_{AO} +
n_{AB}}{2n}\)</span></p></li>
<li><p><span class="math inline">\(p_B = \frac{2n_{BB} + n_{BO} +
n_{AB}}{2n}\)</span></p></li>
<li><p><span class="math inline">\(p_O = \frac{n_{AO} + n_{BO} +
2n_{OO}}{2n}\)</span></p></li>
</ul>
<p>(Can derive with Lagrange multipliers)</p>
</div>
<div id="in-code" class="slide section level2">
<h1>In code</h1>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&#39; @param theta_0: A list with elements pA, pB, and pO</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&#39; @param observed: A list with elements nA, nB, nAB, nO</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>allele_em <span class="ot">&lt;-</span> <span class="cf">function</span>(theta_start, observed, niter) {</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> theta_start</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    theta_iterates <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>niter) {</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        complete_data <span class="ot">&lt;-</span> <span class="fu">e_step</span>(theta, observed)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        theta <span class="ot">&lt;-</span> <span class="fu">m_step</span>(theta, complete_data)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        theta_iterates[[i]] <span class="ot">&lt;-</span> theta</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(theta_iterates)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&#39; @param theta: A list with elements pA, pO, and pO</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&#39; @param observed: A list with elements nA, nB, nAB, nO</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>e_step <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, observed) {</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    complete_data <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    complete_data[[<span class="st">&quot;nAA&quot;</span>]] <span class="ot">&lt;-</span> observed[[<span class="st">&quot;nA&quot;</span>]] <span class="sc">*</span> theta[[<span class="st">&quot;pA&quot;</span>]]<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (theta[[<span class="st">&quot;pA&quot;</span>]]<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> theta[[<span class="st">&quot;pA&quot;</span>]] <span class="sc">*</span> theta[[<span class="st">&quot;pO&quot;</span>]])</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    complete_data[[<span class="st">&quot;nBB&quot;</span>]] <span class="ot">&lt;-</span> observed[[<span class="st">&quot;nB&quot;</span>]] <span class="sc">*</span> theta[[<span class="st">&quot;pB&quot;</span>]]<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (theta[[<span class="st">&quot;pB&quot;</span>]]<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> theta[[<span class="st">&quot;pB&quot;</span>]] <span class="sc">*</span> theta[[<span class="st">&quot;pO&quot;</span>]])</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    complete_data[[<span class="st">&quot;nAO&quot;</span>]] <span class="ot">&lt;-</span> observed[[<span class="st">&quot;nA&quot;</span>]] <span class="sc">*</span> <span class="dv">2</span> <span class="sc">*</span> theta[[<span class="st">&quot;pA&quot;</span>]] <span class="sc">*</span> theta[[<span class="st">&quot;pO&quot;</span>]] <span class="sc">/</span> (theta[[<span class="st">&quot;pA&quot;</span>]]<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> theta[[<span class="st">&quot;pA&quot;</span>]] <span class="sc">*</span> theta[[<span class="st">&quot;pO&quot;</span>]])</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    complete_data[[<span class="st">&quot;nBO&quot;</span>]] <span class="ot">&lt;-</span> observed[[<span class="st">&quot;nB&quot;</span>]] <span class="sc">*</span> <span class="dv">2</span> <span class="sc">*</span> theta[[<span class="st">&quot;pB&quot;</span>]] <span class="sc">*</span> theta[[<span class="st">&quot;pO&quot;</span>]] <span class="sc">/</span> (theta[[<span class="st">&quot;pB&quot;</span>]]<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> theta[[<span class="st">&quot;pB&quot;</span>]] <span class="sc">*</span> theta[[<span class="st">&quot;pO&quot;</span>]])</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    complete_data[[<span class="st">&quot;nAB&quot;</span>]] <span class="ot">&lt;-</span> observed[[<span class="st">&quot;nAB&quot;</span>]]</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    complete_data[[<span class="st">&quot;nOO&quot;</span>]] <span class="ot">&lt;-</span> observed[[<span class="st">&quot;nOO&quot;</span>]]</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(complete_data)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&#39; @param complete_data: A list with elements nAA, nBB, nAO, nBO, nAB, nOO</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&#39; @param theta: A list with elements pA, pB, and pO</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>m_step <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, complete_data) {</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    theta_new <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">unlist</span>(complete_data))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    theta_new[[<span class="st">&quot;pA&quot;</span>]] <span class="ot">&lt;-</span> (<span class="dv">2</span> <span class="sc">*</span> complete_data[[<span class="st">&quot;nAA&quot;</span>]] <span class="sc">+</span> complete_data[[<span class="st">&quot;nAO&quot;</span>]] <span class="sc">+</span> complete_data[[<span class="st">&quot;nAB&quot;</span>]]) <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> n)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    theta_new[[<span class="st">&quot;pB&quot;</span>]] <span class="ot">&lt;-</span> (<span class="dv">2</span> <span class="sc">*</span> complete_data[[<span class="st">&quot;nBB&quot;</span>]] <span class="sc">+</span> complete_data[[<span class="st">&quot;nBO&quot;</span>]] <span class="sc">+</span> complete_data[[<span class="st">&quot;nAB&quot;</span>]]) <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> n)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    theta_new[[<span class="st">&quot;pO&quot;</span>]]<span class="ot">&lt;-</span> (complete_data[[<span class="st">&quot;nAO&quot;</span>]] <span class="sc">+</span> complete_data[[<span class="st">&quot;nBO&quot;</span>]] <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> complete_data[[<span class="st">&quot;nOO&quot;</span>]]) <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> n)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(theta_new)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>observed <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">nA =</span> <span class="dv">186</span>, <span class="at">nB =</span> <span class="dv">38</span>, <span class="at">nAB =</span> <span class="dv">13</span>, <span class="at">nOO =</span> <span class="dv">284</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>theta_start <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">pA =</span> .<span class="dv">3</span>, <span class="at">pB =</span> .<span class="dv">2</span>, <span class="at">pO =</span> .<span class="dv">5</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">allele_em</span>(theta_start, observed, <span class="at">niter =</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span> <span class="fu">bind_rows</span>()</span></code></pre></div>
<pre><code>## # A tibble: 5 × 3
##      pA     pB    pO
##   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 0.232 0.0550 0.713
## 2 0.216 0.0503 0.734
## 3 0.214 0.0502 0.736
## 4 0.214 0.0501 0.736
## 5 0.214 0.0501 0.736</code></pre>
</div>
<div id="some-notes" class="slide section level2">
<h1>Some notes</h1>
<ul class="incremental">
<li><p>We can prove that EM is an ascent algorithm</p></li>
<li><p>Tends to converge rather slowly</p></li>
<li><p>No guarantee of getting a global maximum of the likelihood (but
sometimes this is a good thing…)</p></li>
</ul>
<p>When is this useful?</p>
<ul class="incremental">
<li><p>When it is easy to get distributions over the missing variables
given the current parameter estimates.</p></li>
<li><p>When the problem simplifies after introduction of the missing
variables.</p></li>
<li><p>These two conditions often hold in latent variable models, which
also tend not to be convex.</p></li>
</ul>
</div>
</body>
</html>
