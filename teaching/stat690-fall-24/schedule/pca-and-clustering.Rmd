% Clustering and PCA
% Julia Fukuyama

```{r setup, echo = FALSE}
library(knitr)
opts_chunk$set(fig.cap="", fig.width = 5, fig.height = 3, dpi=175, fig.path="lecture-23-fig/", warning = FALSE)
library(ggplot2)
library(ggrepel)
set.seed(0)
```

## PCA and Clustering

PCA

> - Intuition

> - Math

> - Example

Clustering

> - k-means algorithm

> - Example

## A contrived example

Suppose we have a 3-dimensional object that we want to investigate.

Due to some strange set of constraints, we are only allowed to see its shadow.

Without knowing anything about the object, what sort of a shadow would you expect to be most useful?

-----

![](camel1.png)

-----

![](camel2.png)

-----

It seems like the most useful shadow is the biggest one: If we were only allowed one view, we would ask for the one that took up the most space.

It's easy to come up with examples where this isn't the most useful, but it's a good place to start if you don't know anything about the data.

The same will be true of PCA.

## PCA as variance maximization

In PCA, we have a data matrix $\mathbf X \in \mathbb R^{n \times p}$ with centered columns.

We think of the rows of $\mathbf X$ as points in $p$-dimensional space, and we want to project the points down into a lower-dimensional space so we can visualize them.

There are a lot of different ways we could do this, but in PCA we want to find the projection that maximizes the variance of the projected points.


## Example

```{r, eval = FALSE}
library(devtools)
install_github("vqv/ggbiplot")
```
```{r}
library(ade4)
library(ggbiplot)
library(viridis)
```

-----

Olympic dataset

```{r}
data(olympic)
summary(olympic$tab)
```

-----

```{r}
oly_noscale = prcomp(olympic$tab, scale. = FALSE)
oly_noscale$rotation[,1:2]
```


## PCA Biplots

Two types of biplots, _form_ and _covariance_. Let's start off with the _form_ biplot.

> - For each sample we plot a point with the sample scores along the principal axes.

> - For each variable, we make an arrow with the variable weights along the principal axes.

This allows us to read off the sample scores along the principal axes and the variable weights along the principal axes.


## PCA biplot for the olympic data

```{r}
## scale = 0 means form biplot
ggbiplot(oly_noscale, scale = 0) + ylim(c(-17, 10))
```


## Covariance biplot

The _covariance_ biplot is very subtly different from the form biplot, the difference being how the singular values are allocated.

In this form of the biplot, angles between biplot axes give us approximations of the covariances between the variables.


## Covariance biplot

```{r}
## scale = 1 means covariance biplot
ggbiplot(oly_noscale, scale = 1)
```



## Correlation-based PCA

The example above suggests that doing PCA when the variables are on different scales isn't always that useful.

When the variables are on different scales and measure different things, we often standardize the columns of $\mathbf X$ (divide each column by its standard deviation) before performing PCA.

This is sometimes referred to as correlation-based PCA, as opposed to covariance-based PCA.

## Correlation-based PCA biplot

Analogous to the covariance-based PCA biplot:

> - Sample points represent the scores on the principal axes.

> - Variable points represent variable weights on the principal axes.

Relationships between the variables:

> - Projections of biplot axes onto each other now describe correlations between variables instead of covariances.

-----

Covariance PCA biplot based on correlations, with some extra information:

```{r}
oly_scale = prcomp(olympic$tab, scale. = TRUE)
## scale = 1 means covariance biplot
ggbiplot(oly_scale, scale = 1)
```

How would you interpret the first principal component?

------

Covariance PCA biplot based on correlations, with some extra information:

```{r}
## scale = 1 means covariance biplot
ggbiplot(oly_scale, scale = 1) +
    geom_point(aes(color = olympic$score)) +
    scale_color_viridis("Score")
```



## Summing up

Interpretation of PCA:

> - Visualization of the elements of the matrix

> - Visualization of the correlation/covariance among the variables

Two kinds of PCA:

> - Covariance-based PCA: columns of the data matrix are centered but not scaled, implemented with `scale. = FALSE` in `prcomp` (the default)

> - Correlation-based PCA: columns of the data matrix are centered and scaled, implemented with `scale. = TRUE` in `prcomp`

Two kinds of PCA biplot:

> - Form biplot: More informative about the relationships between the samples, implemented with `scale = 0` in `ggbiplot`

> - Covariance biplot: More informative about relationships between variables, implemented with `scale = 1` in `ggbiplot`

## K-means clustering

Notation: $C_1, \ldots, C_K$ denote sets containing the indices of the observations in each cluster. These sets form a partition of the observations, that is:
1. $C_1 \cup C_2 \cup \cdots \cup C_K = \{1,\ldots, n\}$.
2. $C_k \cap C_{k'} = \emptyset$ for all $k \ne k'$.

The $K$-means problem is then to find $C_1,\ldots, C_k$ that solves the problem
$$
\text{minimize}_{C_1,\ldots, C_K} \left\{ \sum_{k=1}^K W(C_k) \right\}
$$
where
$$
W(C_k) = \frac{1}{|C_k|} \sum_{i, i' \in C_k} \sum_{j=1}^p (x_{ij} - x_{i'j})^2
$$

## The $K$-means clustering algorithm

![](k-means-algorithm.png)

## The $K$-means clustering algorithm in pictures

![](k-means-alg-pic.png)

## Main issue: How to choose the number of clusters?

Options:

> - Elbow method (look for a "kink" in the plot of the within-cluster sum of squares)

> - Gap statistic (look at the difference between the within-cluster sum of squares and what the within-cluster sum of squares would be if the samples were distributed uniformly.)

> - Silhouette plot (plot of how well clustered each point is, see [here](https://en.wikipedia.org/wiki/Silhouette_(clustering)))

## Example

Load packagies and data, remove NAs, scale all the variables in the data frame.
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

df <- USArrests
df <- na.omit(df)
df <- scale(df)
```

-----

Using the `kmeans` function to cluster, look at the output using `fviz_cluster`.

```{r}
k2 <- kmeans(df, centers = 2, nstart = 25)
str(k2)
fviz_cluster(k2, data = df)
```

-----

Look at different clustering solutions.

```{r}
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

----

Look at the within-cluster sum of squares:
```{r}
set.seed(123)
fviz_nbclust(df, kmeans, method = "wss")
```

-----


What does the gap statistic tell us about the number of clusters?
```{r}
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

-----


Look at the silhouette plots:
```{r, fig.width=8, fig.height=5}
sil2 <- silhouette(k2$cluster, dist(df))
p1 <- fviz_silhouette(sil2)
sil3 <- silhouette(k3$cluster, dist(df))
p2 <- fviz_silhouette(sil3)
sil4 <- silhouette(k4$cluster, dist(df))
p3 <- fviz_silhouette(sil4)
sil5 <- silhouette(k5$cluster, dist(df))
p4 <- fviz_silhouette(sil5)

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

-----

Look at what the average silhouette width tells us about the number of clusters.

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")
```
