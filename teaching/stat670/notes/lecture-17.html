<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Julia Fukuyama" />
  <meta name="date" content="2018-10-16" />
  <title>Stat 470/670 Lecture 17: Multi-Dimensional Scaling</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Stat 470/670 Lecture 17: Multi-Dimensional Scaling</h1>
  <p class="author">
Julia Fukuyama
  </p>
  <p class="date">October 16, 2018</p>
</div>
<div id="logistics" class="slide section level2">
<h1>Logistics</h1>
<p>How the rest of the semester should go:</p>
<ul>
<li><p>Mini project 1 grading will be done by Thursday.</p></li>
<li><p>Homework 5 assigned today, due next Friday, October 26.</p></li>
<li><p>There will be one more homework after homework 5.</p></li>
<li><p>Mini project 2 assigned Thursday, draft due Tuesday October 30, final due Friday November 9.</p></li>
<li><p>I'll post guidelines for the final projects by the end of the week, and I'll ask you to submit project proposals by November 2. The presentations, which can describe partial results, will be after Thanksgiving, and the final writeup will be due December 7.</p></li>
</ul>
</div>
<div id="midterm-evaluations" class="slide section level2">
<h1>Midterm evaluations</h1>
<p>Thank you for the feedback!</p>
<ul>
<li><p>Many of you requested more detailed slides</p></li>
<li><p>Several people wanted more coding/lab sessions</p></li>
<li><p>Several people requested more readings</p></li>
</ul>
<p>Questions for you:</p>
<ul>
<li><p>Do you like it when I do live demonstrations in R?</p></li>
<li><p>Would you like time in class to try out the material on your own/on the mini project data/on your final projects?</p></li>
</ul>
</div>
<div id="setup-for-multi-dimensional-scaling" class="slide section level2">
<h1>Setup for multi-dimensional scaling</h1>
<p>Instead of measurements on variables, like in PCA, we have distances between the samples.</p>
<p>The distances can be what was measured initially, or the distance could be constructed by the analyst from other variables that were measured directly.</p>
<p>In multi-dimensional scaling, the goal is to make a map of the samples in a low-dimensional space (probably 2-dimensional space) so that the distances in that map match the distances between the samples as closely as possible.</p>
</div>
<div id="some-examples-of-inputs-for-multi-dimensional-scaling" class="slide section level2">
<h1>Some examples of inputs for multi-dimensional scaling</h1>
<ul>
<li><p>Subjective ratings of dissimilarities between objects</p></li>
<li><p>Distances between politicians based on voting records</p></li>
<li><p>Travel times between cities</p></li>
</ul>
</div>
<div id="fake-data-1" class="slide section level2">
<h1>Fake Data 1</h1>
<pre class="sourceCode r"><code class="sourceCode r">D =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="kw">sqrt</span>(<span class="dv">2</span>),
             <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>,
             <span class="kw">sqrt</span>(<span class="dv">2</span>), <span class="dv">1</span>, <span class="dv">0</span>),
    <span class="dt">nrow =</span> <span class="dv">3</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
<span class="kw">round</span>(D, <span class="dt">digits =</span> <span class="dv">2</span>)</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,] 0.00    1 1.41
## [2,] 1.00    0 1.00
## [3,] 1.41    1 0.00</code></pre>
<p>Here <code>D</code> is a distance matrix, and the <span class="math">\((i,j)\)</span> element of <code>D</code> tells us the distance between sample <span class="math">\(i\)</span> and sample <span class="math">\(j\)</span>.</p>
<p>How would you position these samples in space so that the distances between them matched the distances in <code>D</code>?</p>
</div>
<div id="embedding-into-euclidean-space" class="slide section level2">
<h1>Embedding into Euclidean space</h1>
<p>In multi-dimensional scaling, we want to find an <em>embedding</em> of the samples into Euclidean space so that the distances between the embedded points match the distances between the samples as closely as possible.</p>
<p>This sounds fancy, but all it means is that we create a set of coordinates and assign each sample a value along each coordinate so that the distances between the samples match the input distances.</p>
</div>
<div class="slide section level2">

<p>How does this work on our fake data?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cmdscale</span>(D)</code></pre>
<pre><code>##               [,1]       [,2]
## [1,]  7.071068e-01  0.2357023
## [2,] -1.110223e-16 -0.4714045
## [3,] -7.071068e-01  0.2357023</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mds_points =<span class="st"> </span><span class="kw">cmdscale</span>(D)
<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(mds_points)) +
<span class="st">    </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">x =</span> X1, <span class="dt">y =</span> X2, <span class="dt">label =</span> <span class="dv">1</span>:<span class="dv">3</span>)) +<span class="st"> </span><span class="kw">coord_fixed</span>()</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-2-1.png" />
</div>
</div>
<div class="slide section level2">

<p>We can check that the distances match:</p>
<pre class="sourceCode r"><code class="sourceCode r">## The dist function computes distances (Euclidean
## by default) between the rows of a data frame
<span class="kw">dist</span>(mds_points)</code></pre>
<pre><code>##          1        2
## 2 1.000000         
## 3 1.414214 1.000000</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## compare with D, the input distances
D</code></pre>
<pre><code>##          [,1] [,2]     [,3]
## [1,] 0.000000    1 1.414214
## [2,] 1.000000    0 1.000000
## [3,] 1.414214    1 0.000000</code></pre>
</div>
<div id="classical-multi-dimensional-scaling" class="slide section level2">
<h1>Classical Multi-Dimensional Scaling</h1>
<p>One solution to the multi-dimensional scaling problem is given by classical multi-dimensional scaling.</p>
<p>Let <span class="math">\(\mathbf D \in \mathbb R^{n \times n}\)</span> be a matrix where <span class="math">\(\mathbf D_{ij}\)</span> contains the square of the distance between sample <span class="math">\(i\)</span> and sample <span class="math">\(j\)</span>.</p>
<p>Let <span class="math">\(\mathbf H \in \mathbb R^{n \times n}\)</span> be the centering matrix, <span class="math">\(\mathbf H = \mathbf I - \frac{1}{n} \mathbf 1 \mathbf 1^T\)</span>.</p>
<p>Create the doubly-centered distance matrix <span class="math">\(\mathbf B = -\frac{1}{2} \mathbf H \mathbf D \mathbf H\)</span>, and let <span class="math">\(\mathbf U \mathbf \Lambda \mathbf U^T\)</span> be the singular value decomposition of <span class="math">\(\mathbf B\)</span>.</p>
<p>Then the <span class="math">\(k\)</span>-dimensional solution to the multi-dimensional scaling problem is obtained by taking <span class="math">\(\mathbf U_{(k)} \mathbf \Lambda_{(k)}^{1/2}\)</span>.</p>
</div>
<div class="slide section level2">

<p>Idea behind this solution:</p>
<p>Suppose the distance really did come from a matrix <span class="math">\(\mathbf X \in \mathbb R^{n \times k}\)</span>, where we computed the Euclidean distances between the rows of <span class="math">\(\mathbf X\)</span>. For definiteness, assume that the columns of <span class="math">\(\mathbf X\)</span> are centered.</p>
<p>Then it turns out (linear algebra exercise: verify this) that <span class="math">\((\mathbf H \mathbf X) (\mathbf H \mathbf X)^T = \mathbf B\)</span>.</p>
<p>The top <span class="math">\(k\)</span> left singular vectors of <span class="math">\(\mathbf X\)</span> (which is the same as <span class="math">\(\mathbf H \mathbf X\)</span> because <span class="math">\(\mathbf X\)</span> already has centered columns) will therefore give the optimal representation of the true embedded points that we got the distances from.</p>
<p>The singular vectors of <span class="math">\(\mathbf H \mathbf X\)</span> are the same as the singular vectors of <span class="math">\(\mathbf B\)</span>, so if we start off with <span class="math">\(\mathbf B\)</span> instead of <span class="math">\(\mathbf X\)</span>, we can still get the optimal low-dimensional embedding by taking the top singular vectors of <span class="math">\(\mathbf B\)</span>.</p>
</div>
<div id="checking-the-quality-of-the-mds-solution" class="slide section level2">
<h1>Checking the quality of the MDS solution</h1>
<p>Just as in PCA and with the SVD, we have a measure of the quality of the approximation.</p>
<p>In classical multi-dimensional scaling, these are given by the eigenvalues of <span class="math">\(\mathbf B\)</span>, and plotting the eigenvalues tells us how how much of the &quot;variance&quot; is explained by the multi-dimensional scaling axes.</p>
<p>If we can represent the distances perfectly with an embedding into <span class="math">\(k\)</span>-dimensional space, the top <span class="math">\(k\)</span> eigenvalues will be non-zero and the remainder will be zero.</p>
<p>We can check this on our fake data, where we constructed the distances so that they could be exactly represented in two-dimensional space.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cmdscale</span>(D, <span class="dt">eig =</span> <span class="ot">TRUE</span>)$eig</code></pre>
<pre><code>## [1] 1.000000e+00 3.333333e-01 2.220446e-16</code></pre>
</div>
<div class="slide section level2">

<p>In general, you won't be able to get an exact representation in a number of dimensions that's easy to visualize, but you will want to know how well you're doing with the number of dimensions you take.</p>
<p>We use the eigenvalues to make a scree plot, analogous to the PCA scree plot, to measure the quality of the embedding</p>
<p>Major difference between MDS and PCA:.</p>
<ul>
<li><p>The eigenvalues can be negative.</p></li>
<li><p>Negative eigenvalues mean that there is no embedding of the points so that the Euclidean distances between them exactly match the input distances, and the size of the negative eigenvalues indicate how severe the problem is.</p></li>
<li><p>Not that important, but the terminology is that if you see negative eigenvalues, it means that your distances are <em>non-Euclidean</em>.</p></li>
</ul>
</div>
<div id="example-of-non-embeddable-set-of-points" class="slide section level2">
<h1>Example of non-embeddable set of points</h1>
<pre class="sourceCode r"><code class="sourceCode r">D =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, .<span class="dv">1</span>,
             <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">5</span>,
             <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">5</span>,
             .<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">0</span>),
    <span class="dt">nrow =</span> <span class="dv">4</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
D</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]  0.0    1    1  0.1
## [2,]  1.0    0    1  5.0
## [3,]  1.0    1    0  5.0
## [4,]  0.1    5    5  0.0</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mds_points =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cmdscale</span>(D))
<span class="kw">dist</span>(mds_points)</code></pre>
<pre><code>##          1        2        3
## 2 2.436166                  
## 3 2.436166 1.000000         
## 4 2.605269 5.014562 5.014562</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(mds_points) +
<span class="st">    </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">x =</span> X1, <span class="dt">y =</span> X2, <span class="dt">label =</span> <span class="dv">1</span>:<span class="dv">4</span>)) +<span class="st"> </span><span class="kw">coord_fixed</span>()</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-5-1.png" />
</div>
</div>
<div class="slide section level2">

<p>As promised, this shows up in negative eigenvalues:</p>
<pre class="sourceCode r"><code class="sourceCode r">mds_eig =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">eig =</span> <span class="kw">cmdscale</span>(D, <span class="dt">eig =</span> <span class="ot">TRUE</span>)$eig, <span class="dt">index =</span> <span class="dv">1</span>:<span class="dv">4</span>)
mds_eig</code></pre>
<pre><code>##         eig index
## 1 16.987227     1
## 2  0.500000     2
## 3  0.000000     3
## 4 -4.234727     4</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(mds_eig) +<span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> index, <span class="dt">y =</span> eig)) +<span class="st"> </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>)</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-6-1.png" />
</div>
</div>
<div id="contrived-example-1-state-locations" class="slide section level2">
<h1>Contrived Example 1: State locations</h1>
<p>R contains data on state locations, including one called <code>state.center</code> that gives the latitude and longitude between</p>
<p>What happens if we compute distances between the centers of the states and run multi-dimensional scaling on those distances?</p>
<pre class="sourceCode r"><code class="sourceCode r">state_locations =<span class="st"> </span><span class="kw">data.frame</span>(state.center)
state_distances =<span class="st"> </span><span class="kw">dist</span>(state_locations, <span class="dt">method =</span> <span class="st">&quot;euclidean&quot;</span>)
state_mds =<span class="st"> </span><span class="kw">cmdscale</span>(state_distances, <span class="dt">eig =</span> <span class="ot">TRUE</span>, <span class="dt">k =</span> <span class="dv">2</span>)</code></pre>
</div>
<div class="slide section level2">

<p>Before we get to the MDS plot, let's look at the scree plot to see the quality of the MDS solution.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">eig =</span> state_mds$eig, <span class="dt">index =</span> <span class="dv">1</span>:<span class="dv">50</span>)) +
<span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> index, <span class="dt">y =</span> eig)) +
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Scree plot for MDS on distances between states&quot;</span>)</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-8-1.png" />
</div>
<p>Why do we only get non-zero eigenvalues for the first two MDS axes?</p>
</div>
<div class="slide section level2">

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(state_mds$points), <span class="kw">aes</span>(<span class="dt">x =</span> X1, <span class="dt">y =</span> X2, <span class="dt">label =</span> state.name)) +
<span class="st">    </span><span class="kw">geom_text_repel</span>() +<span class="st"> </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">coord_fixed</span>()</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-9-1.png" />
</div>
<p>When we plot the MDS solution, we get a map!</p>
<p>The states all have the correct relative locations, but the north-south axis is going the wrong way.</p>
<p>This is just due to an indeterminacy in the solution: the singular value decomposition is only determined up to a sign change for the singular vectors.</p>
<p>More heuristically, since we only provide MDS with distances, we can only expect it to give us good approximations to the distances between the samples, we can't expect it to know about north and south.</p>
</div>
<div id="non-metric-mds" class="slide section level2">
<h1>Non-metric MDS</h1>
<p>Non-metric MDS is a robust alternative to classical MDS, and it is used when we want a map that preserves relative distances instead of absolute distances.</p>
<p>The idea is that we want to find a an embedding of the points into a lower-dimensional space <em>and</em> a monotonic transformation of the embedded distances so that the transformed distances recapitulate the input distances as well as possible.</p>
<p>Notes:</p>
<ul>
<li><p>NMDS is more resistant to outliers than classical MDS: if one point has a very large distance from all the others, the first classical MDS axis will tend to separate that point from the others and not be informative about the remaining distances.</p></li>
<li><p>Unlike classical MDS, NMDS does not give nested solutions: if we do NMDS with 2 axes, the first axis will not be equal to the NMDS solution with 1 axis.</p></li>
<li><p>There is no notion of percentage of variation explained by individual axes as in classical MDS.</p></li>
</ul>
</div>
<div id="implementation-of-nmds" class="slide section level2">
<h1>Implementation of NMDS</h1>
<p>Let <span class="math">\(d_i\)</span> contain the input distances, and let <span class="math">\(f\)</span> a monotone increasing function.</p>
<p>Note that since <span class="math">\(f\)</span> is monotone, <span class="math">\(d_i &lt; d_j\)</span> implies that <span class="math">\(f(d_i) &lt; f(d_j)\)</span>, and so the <em>relative</em> distances between the points are preserved under <span class="math">\(f\)</span>.</p>
<p>In NMDS, we want to minimize the stress function, defined as <span class="math">\[
\text{STRESS}^2 = \frac{\sum_i (f(\tilde d_i) - d_i)^2}{\sum_j d_j^2}
\]</span> where <span class="math">\(d\)</span> represents the input distances, and <span class="math">\(\tilde d\)</span> represent the distances between the embedded points.</p>
<p>The NMDS algorithm is as follows:</p>
<ul>
<li><p>Find a random embedding of the samples, e. g. by sampling from a normal distribution.</p></li>
<li><p>Calculate the distances <span class="math">\(\tilde d\)</span> between the embedded sample points.</p></li>
<li><p>Find the optimal monotonic transformation of the distances <span class="math">\(f\)</span> so that <span class="math">\(f(\tilde d)\)</span> matches <span class="math">\(d\)</span> as closely as possible.</p></li>
<li><p>Find the embedding of the samples such that the distances between the embedded points matches <span class="math">\(f(d)\)</span> as closely as possible.</p></li>
<li><p>Compare the stress to some criterion. If the change in stress is small enough then exit the algorithm, otherwise return to to step 2.</p></li>
</ul>
</div>
<div id="example-on-colors" class="slide section level2">
<h1>Example on colors</h1>
<p>In a psychology study (Ekman, Gosta. 1954. &quot;Dimensions of Color Vision.&quot; The Journal of Psychology 38 (2). Taylor &amp; Francis: 467â€“74.), the investigator asked subjects to rate similarities between colors.</p>
<p>These were combined to give overall measure of similarities between colors, and the results are in <code>ekman.txt</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">ekm =<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;../../datasets/ekman.txt&quot;</span>, <span class="dt">header=</span><span class="ot">TRUE</span>)
<span class="kw">rownames</span>(ekm) =<span class="st"> </span><span class="kw">colnames</span>(ekm)
## the elements in ekm are similarities, but we
## need dissimilarities instead. We can create
## dissimilarities by taking the complement of
## the similarities and setting the diagonal to zero.
ekm_dist =<span class="st"> </span><span class="dv">1</span> -<span class="st"> </span>ekm -<span class="st"> </span><span class="kw">diag</span>(<span class="dv">1</span>, <span class="kw">ncol</span>(ekm))
ekm_dist[<span class="dv">1</span>:<span class="dv">5</span>, <span class="dv">1</span>:<span class="dv">5</span>]</code></pre>
<pre><code>##      w434 w445 w465 w472 w490
## w434 0.00 0.14 0.58 0.58 0.82
## w445 0.14 0.00 0.50 0.56 0.78
## w465 0.58 0.50 0.00 0.19 0.53
## w472 0.58 0.56 0.19 0.00 0.46
## w490 0.82 0.78 0.53 0.46 0.00</code></pre>
</div>
<div class="slide section level2">

<p>Let's try classical MDS first:</p>
<pre class="sourceCode r"><code class="sourceCode r">ekm_mds =<span class="st"> </span><span class="kw">cmdscale</span>(ekm_dist, <span class="dt">eig =</span> <span class="ot">TRUE</span>)
## we can make a scree plot giving
## the fraction of variance explained
<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">eig =</span> ekm_mds$eig,
                  <span class="dt">index =</span> <span class="dv">1</span>:<span class="kw">length</span>(ekm_mds$eig))) +
<span class="st">                      </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> index, <span class="dt">y =</span> eig))</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-11-1.png" />
</div>
<p>Note that the negative eigenvalues at the end indicate that the dissimilarities cannot be exactly represented in Euclidean space, but the values are not that large and so we aren't too concerned.</p>
<p>The top two eigenvalues are quite large, indicating that a two-dimensional MDS solution does a reasonable job at recapitulating the dissimilarities between the samples.</p>
</div>
<div class="slide section level2">

<p>And finally the plot:</p>
<pre class="sourceCode r"><code class="sourceCode r">## Here we&#39;re changing variable names and
## adding some additional information to
## the data frame we will use to plot the
## MDS solution
ekm_points =<span class="st"> </span>ekm_mds$points[,<span class="dv">1</span>:<span class="dv">2</span>] %&gt;%
<span class="st">    </span>as_tibble %&gt;%
<span class="st">    </span><span class="kw">setNames</span>(<span class="kw">paste0</span>(<span class="st">&quot;MDS&quot;</span>, <span class="dv">1</span>:<span class="dv">2</span>)) %&gt;%
<span class="st">    </span><span class="kw">mutate</span>(
        <span class="dt">name =</span> <span class="kw">rownames</span>(ekm),
        <span class="dt">rgb =</span> photobiology::<span class="kw">w_length2rgb</span>(<span class="kw">as.numeric</span>(<span class="kw">sub</span>(<span class="st">&quot;w&quot;</span>, <span class="st">&quot;&quot;</span>, name))))
<span class="kw">ggplot</span>(ekm_points, <span class="kw">aes</span>(<span class="dt">x =</span> MDS1, <span class="dt">y =</span> MDS2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col =</span> ekm_points$rgb, <span class="dt">size =</span> <span class="dv">2</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_text_repel</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name)) +<span class="st"> </span><span class="kw">coord_fixed</span>()</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-12-1.png" />
</div>
</div>
<div id="color-data-with-nmds" class="slide section level2">
<h1>Color data with NMDS</h1>
<p>Remember that NMDS is a randomized algorithm, so each run can in principle give a different solution.</p>
<p>The <code>metaMDS</code> function (in the package <code>vegan</code>) repeats the NMDS algorithm many times (20 by default) and looks for the best solution among the results.</p>
<p>The output here tells us that for each run of the algorithm, the stresses are about the same, suggesting that the corresponding solutions are the same.</p>
<p>The part of the output that says <code>Procrustes: rmse 1.060348e-06  max resid 1.960065e-06</code> is checking for similarity between the solutions directly: <code>rmse</code> and <code>max resid</code> describe the discrepancies between the solution and the previous best solution.</p>
<p>Here we see that the algorithm converges ot the same solution every time.</p>
<pre class="sourceCode r"><code class="sourceCode r">ekm_nmds =<span class="st"> </span><span class="kw">metaMDS</span>(ekm_dist, <span class="dt">k =</span> <span class="dv">2</span>, <span class="dt">autotransform =</span> <span class="ot">FALSE</span>)</code></pre>
<pre><code>## Run 0 stress 0.02310251 
## Run 1 stress 0.02310251 
## ... Procrustes: rmse 2.102867e-06  max resid 3.320966e-06 
## ... Similar to previous best
## Run 2 stress 0.02310251 
## ... Procrustes: rmse 7.088863e-06  max resid 1.145484e-05 
## ... Similar to previous best
## Run 3 stress 0.02310251 
## ... Procrustes: rmse 6.438299e-06  max resid 1.040306e-05 
## ... Similar to previous best
## Run 4 stress 0.02310251 
## ... New best solution
## ... Procrustes: rmse 4.681247e-06  max resid 7.620905e-06 
## ... Similar to previous best
## Run 5 stress 0.02310251 
## ... New best solution
## ... Procrustes: rmse 3.633134e-06  max resid 6.162556e-06 
## ... Similar to previous best
## Run 6 stress 0.02310251 
## ... Procrustes: rmse 3.021893e-06  max resid 5.156862e-06 
## ... Similar to previous best
## Run 7 stress 0.02310251 
## ... New best solution
## ... Procrustes: rmse 1.146055e-06  max resid 2.470557e-06 
## ... Similar to previous best
## Run 8 stress 0.02310251 
## ... New best solution
## ... Procrustes: rmse 9.659437e-07  max resid 2.036998e-06 
## ... Similar to previous best
## Run 9 stress 0.02310251 
## ... Procrustes: rmse 4.787886e-06  max resid 8.130221e-06 
## ... Similar to previous best
## Run 10 stress 0.02310251 
## ... Procrustes: rmse 5.616317e-07  max resid 8.747772e-07 
## ... Similar to previous best
## Run 11 stress 0.02310251 
## ... Procrustes: rmse 5.527651e-07  max resid 1.104713e-06 
## ... Similar to previous best
## Run 12 stress 0.02310251 
## ... Procrustes: rmse 2.855771e-06  max resid 4.601952e-06 
## ... Similar to previous best
## Run 13 stress 0.02310251 
## ... Procrustes: rmse 1.506129e-06  max resid 2.440185e-06 
## ... Similar to previous best
## Run 14 stress 0.02310251 
## ... Procrustes: rmse 6.926222e-06  max resid 1.15127e-05 
## ... Similar to previous best
## Run 15 stress 0.02310251 
## ... Procrustes: rmse 8.158106e-07  max resid 1.29539e-06 
## ... Similar to previous best
## Run 16 stress 0.02310251 
## ... Procrustes: rmse 1.780811e-06  max resid 2.655272e-06 
## ... Similar to previous best
## Run 17 stress 0.02310251 
## ... Procrustes: rmse 6.859623e-07  max resid 1.288504e-06 
## ... Similar to previous best
## Run 18 stress 0.02310251 
## ... Procrustes: rmse 5.879151e-07  max resid 8.94391e-07 
## ... Similar to previous best
## Run 19 stress 0.02310251 
## ... Procrustes: rmse 4.007526e-06  max resid 6.695112e-06 
## ... Similar to previous best
## Run 20 stress 0.02310251 
## ... Procrustes: rmse 3.797161e-06  max resid 5.754955e-06 
## ... Similar to previous best
## *** Solution reached</code></pre>
</div>
<div class="slide section level2">

<p>We would like an analog of the scree plot so that we can evaluate how many dimensions to use.</p>
<p>One way to do this is to compute the stress function for each number of dimensions and plot that.</p>
<pre class="sourceCode r"><code class="sourceCode r">## since the algorithm is random, it would be
## better to do this many times for each value
## of k and take the average
stresses =<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span>:<span class="dv">5</span>, function(k) <span class="kw">metaMDS</span>(ekm_dist, <span class="dt">k =</span> k, <span class="dt">autotransform =</span> <span class="ot">FALSE</span>)$stress)</code></pre>
<pre><code>## Run 0 stress 0.2721258 
## Run 1 stress 0.5212407 
## Run 2 stress 0.4856468 
## Run 3 stress 0.490962 
## Run 4 stress 0.2567348 
## ... New best solution
## ... Procrustes: rmse 0.05564529  max resid 0.1384309 
## Run 5 stress 0.2687508 
## Run 6 stress 0.4533083 
## Run 7 stress 0.4373748 
## Run 8 stress 0.4843117 
## Run 9 stress 0.5071186 
## Run 10 stress 0.5291604 
## Run 11 stress 0.4866729 
## Run 12 stress 0.4983676 
## Run 13 stress 0.4942408 
## Run 14 stress 0.4915839 
## Run 15 stress 0.4584145 
## Run 16 stress 0.5006447 
## Run 17 stress 0.5165007 
## Run 18 stress 0.5246208 
## Run 19 stress 0.5003296 
## Run 20 stress 0.5081357 
## *** No convergence -- monoMDS stopping criteria:
##      1: stress ratio &gt; sratmax
##     19: scale factor of the gradient &lt; sfgrmin
## Run 0 stress 0.02310251 
## Run 1 stress 0.02310251 
## ... Procrustes: rmse 1.603649e-06  max resid 2.608348e-06 
## ... Similar to previous best
## Run 2 stress 0.02310251 
## ... New best solution
## ... Procrustes: rmse 1.283596e-06  max resid 2.203181e-06 
## ... Similar to previous best
## Run 3 stress 0.02310251 
## ... Procrustes: rmse 9.753636e-07  max resid 1.832654e-06 
## ... Similar to previous best
## Run 4 stress 0.2925031 
## Run 5 stress 0.02310251 
## ... Procrustes: rmse 5.337933e-06  max resid 8.633325e-06 
## ... Similar to previous best
## Run 6 stress 0.02310251 
## ... Procrustes: rmse 1.579965e-06  max resid 3.45673e-06 
## ... Similar to previous best
## Run 7 stress 0.02310251 
## ... Procrustes: rmse 3.284343e-06  max resid 5.161338e-06 
## ... Similar to previous best
## Run 8 stress 0.02310251 
## ... Procrustes: rmse 2.555243e-06  max resid 4.933159e-06 
## ... Similar to previous best
## Run 9 stress 0.02310251 
## ... New best solution
## ... Procrustes: rmse 1.01295e-06  max resid 2.103083e-06 
## ... Similar to previous best
## Run 10 stress 0.02310251 
## ... Procrustes: rmse 2.232036e-06  max resid 4.119736e-06 
## ... Similar to previous best
## Run 11 stress 0.02310251 
## ... Procrustes: rmse 1.663214e-06  max resid 3.072049e-06 
## ... Similar to previous best
## Run 12 stress 0.02310251 
## ... Procrustes: rmse 2.237823e-06  max resid 4.174616e-06 
## ... Similar to previous best
## Run 13 stress 0.02310251 
## ... New best solution
## ... Procrustes: rmse 1.327781e-06  max resid 2.668128e-06 
## ... Similar to previous best
## Run 14 stress 0.02310251 
## ... Procrustes: rmse 3.930095e-06  max resid 6.673939e-06 
## ... Similar to previous best
## Run 15 stress 0.02310251 
## ... Procrustes: rmse 1.227083e-06  max resid 2.181569e-06 
## ... Similar to previous best
## Run 16 stress 0.02310251 
## ... Procrustes: rmse 1.61893e-06  max resid 2.611836e-06 
## ... Similar to previous best
## Run 17 stress 0.02310251 
## ... Procrustes: rmse 4.812507e-06  max resid 7.680302e-06 
## ... Similar to previous best
## Run 18 stress 0.02310251 
## ... New best solution
## ... Procrustes: rmse 9.04642e-07  max resid 1.366657e-06 
## ... Similar to previous best
## Run 19 stress 0.02310251 
## ... Procrustes: rmse 2.685811e-06  max resid 5.192665e-06 
## ... Similar to previous best
## Run 20 stress 0.02310251 
## ... Procrustes: rmse 4.933418e-06  max resid 7.938065e-06 
## ... Similar to previous best
## *** Solution reached
## Run 0 stress 0.01253576 
## Run 1 stress 0.01703842 
## Run 2 stress 0.1085466 
## Run 3 stress 0.01715139 
## Run 4 stress 0.01483193 
## Run 5 stress 0.0156638 
## Run 6 stress 0.01253745 
## ... Procrustes: rmse 0.001812855  max resid 0.003286622 
## ... Similar to previous best
## Run 7 stress 0.01253767 
## ... Procrustes: rmse 0.001890592  max resid 0.003426268 
## ... Similar to previous best
## Run 8 stress 0.01678396 
## Run 9 stress 0.01715178 
## Run 10 stress 0.01529429 
## Run 11 stress 0.0152931 
## Run 12 stress 0.01253797 
## ... Procrustes: rmse 0.0005013934  max resid 0.0008736871 
## ... Similar to previous best
## Run 13 stress 0.01619717 
## Run 14 stress 0.01515287 
## Run 15 stress 0.01617234 
## Run 16 stress 0.012539 
## ... Procrustes: rmse 0.00219784  max resid 0.003643609 
## ... Similar to previous best
## Run 17 stress 0.01604329 
## Run 18 stress 0.01777524 
## Run 19 stress 0.01404241 
## Run 20 stress 0.01244572 
## ... New best solution
## ... Procrustes: rmse 0.01867244  max resid 0.04686828 
## *** No convergence -- monoMDS stopping criteria:
##      7: no. of iterations &gt;= maxit
##     13: stress ratio &gt; sratmax
## Run 0 stress 0.006273768 
## Run 1 stress 0.004990382 
## ... New best solution
## ... Procrustes: rmse 0.1007591  max resid 0.1849666 
## Run 2 stress 0.00487154 
## ... New best solution
## ... Procrustes: rmse 0.01068611  max resid 0.02011363 
## Run 3 stress 0.006313356 
## Run 4 stress 0.005815049 
## Run 5 stress 0.005724101 
## Run 6 stress 0.01109444 
## Run 7 stress 0.003030431 
## ... New best solution
## ... Procrustes: rmse 0.04301309  max resid 0.07967849 
## Run 8 stress 0.003766835 
## Run 9 stress 0.002812255 
## ... New best solution
## ... Procrustes: rmse 0.007655836  max resid 0.01192655 
## Run 10 stress 0.002872299 
## ... Procrustes: rmse 0.01055606  max resid 0.01956895 
## Run 11 stress 0.006120003 
## Run 12 stress 0.1316894 
## Run 13 stress 0.003452073 
## Run 14 stress 0.005912479 
## Run 15 stress 0.006849582 
## Run 16 stress 0.003751898 
## Run 17 stress 0.006483368 
## Run 18 stress 0.003709185 
## Run 19 stress 0.004488317 
## Run 20 stress 0.002733593 
## ... New best solution
## ... Procrustes: rmse 0.008442853  max resid 0.01632713 
## *** No convergence -- monoMDS stopping criteria:
##     19: no. of iterations &gt;= maxit
##      1: stress ratio &gt; sratmax
## Run 0 stress 0.0009367835 
## Run 1 stress 0.002131138 
## Run 2 stress 0.003907969 
## Run 3 stress 0.001706481 
## Run 4 stress 0.005466837 
## Run 5 stress 0.002919591 
## Run 6 stress 0.00258284 
## Run 7 stress 0.003337378 
## Run 8 stress 0.005895784 
## Run 9 stress 0.001076621 
## ... Procrustes: rmse 0.09829673  max resid 0.2129623 
## Run 10 stress 0.002311725 
## Run 11 stress 0.002533747 
## Run 12 stress 0.001741355 
## Run 13 stress 0.002134076 
## Run 14 stress 0.001557201 
## Run 15 stress 0.0006962816 
## ... New best solution
## ... Procrustes: rmse 0.08041192  max resid 0.1217456 
## Run 16 stress 0.001892428 
## Run 17 stress 0.002917626 
## Run 18 stress 0.001702046 
## Run 19 stress 0.002460906 
## Run 20 stress 0.002818138 
## *** No convergence -- monoMDS stopping criteria:
##     20: no. of iterations &gt;= maxit</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## note about above, autotransform is a parameter
## that is specific to ecology data, we want to
## set it to FALSE
<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(stresses, <span class="dt">k =</span> <span class="dv">1</span>:<span class="dv">5</span>)) +<span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> stresses))</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-14-1.png" />
</div>
<p>Here we see that the stress drops substantially going from 1 to 2 dimensions, and then doesn't go down that much once we increase the number of dimensions further.</p>
<p>This indicates to us that the two-dimensional solution is likely to be pretty good, just as it was with classical MDS.</p>
</div>
<div class="slide section level2">

<p>And the plot:</p>
<pre class="sourceCode r"><code class="sourceCode r">nmds_points =<span class="st"> </span>ekm_nmds$points[,<span class="dv">1</span>:<span class="dv">2</span>] %&gt;%
<span class="st">    </span>as_tibble %&gt;%
<span class="st">    </span><span class="kw">setNames</span>(<span class="kw">paste0</span>(<span class="st">&quot;NMDS&quot;</span>, <span class="dv">1</span>:<span class="dv">2</span>)) %&gt;%
<span class="st">    </span><span class="kw">bind_cols</span>(<span class="kw">select</span>(ekm_points, rgb, name))
<span class="kw">ggplot</span>(nmds_points, <span class="kw">aes</span>(<span class="dt">x =</span> NMDS1, <span class="dt">y =</span> NMDS2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col =</span> ekm_points$rgb, <span class="dt">size =</span> <span class="dv">2</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_text_repel</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name)) +<span class="st"> </span><span class="kw">coord_fixed</span>()</code></pre>
<div class="figure">
<img src="lecture-17-fig/unnamed-chunk-15-1.png" />
</div>
</div>
</body>
</html>
