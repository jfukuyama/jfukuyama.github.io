<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Julia Fukuyama" />
  <title>Stat 470/670 Lecture 14: Model-building with a moderate number of variables</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Stat 470/670 Lecture 14: Model-building with a
moderate number of variables</h1>
  <p class="author">
Julia Fukuyama
  </p>
</div>
<div id="model-building" class="slide section level2">
<h1>Model building</h1>
<ul>
<li>Over the next couple of weeks we’ll get back to building models, and
we’ll look at models for different kinds of responses (binary, count,
categorical).</li>
</ul>
<ul>
<li>Today we’ll look at model-building with a moderate number of
variables. Today will be linear models, but the ideas carry over to
logistic regression and the generalized linear models we’ll talk about
later.</li>
</ul>
<ul>
<li>Note on terminology: for statisticians, “linear model” means that
your response variable follows a normal distribution. “Generalized
linear models” will be models for which the response variable follows a
different sort of distribution.</li>
</ul>
</div>
<div id="data-prostate-cancer" class="slide section level2">
<h1>Data: Prostate Cancer</h1>
<p>We have a data set containing clinical data on patients who were
about to receive a radical prostatectomy. The relevant variables
are:</p>
<p>Variables about sizes:</p>
<ul>
<li><code>lcavol</code>: log cancer volume</li>
</ul>
<ul>
<li><code>lweight</code>: log prostate weight</li>
</ul>
<ul>
<li><code>lbph</code>: log of the amount of benign prostatic
hyperplasia</li>
</ul>
<p>Variables about the individual:</p>
<ul>
<li><code>age</code>: Age in years</li>
</ul>
<p>Variables measured by the pathologist:</p>
<ul>
<li><code>svi</code>: Seminal vesicle invasion, a measure of how
advanced the cancer is.</li>
</ul>
<ul>
<li><code>lcp</code>: log of capsular penetration</li>
</ul>
<ul>
<li><code>gleason</code>: A numeric vector giving the <a
href="https://www.pcf.org/about-prostate-cancer/diagnosis-staging-prostate-cancer/gleason-score-isup-grade/">Gleason
score</a>. In theory can range from 2-10, in practice ranges from
6-10.</li>
</ul>
<ul>
<li><code>pgg45</code>: Percent of cells with Gleason score 4 or 5.</li>
</ul>
<p>The “response” variable:</p>
<ul>
<li><code>lpsa</code>: log of the concentration of prostate-stimulating
antigen.</li>
</ul>
<p>We are primarily interested in <code>lpsa</code>, which is used as a
marker for prostate cancer. We would like to know whether and how it is
related to the other variables we have available to us.</p>
</div>
<div id="first-we-look-at-the-data" class="slide section level2">
<h1>First we look at the data</h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>prostate <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">&quot;../../datasets/prostate.txt&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(prostate)</span></code></pre></div>
<pre><code>##      lcavol           lweight           age             lbph        
##  Min.   :-1.3471   Min.   :2.375   Min.   :41.00   Min.   :-1.3863  
##  1st Qu.: 0.5128   1st Qu.:3.376   1st Qu.:60.00   1st Qu.:-1.3863  
##  Median : 1.4469   Median :3.623   Median :65.00   Median : 0.3001  
##  Mean   : 1.3500   Mean   :3.629   Mean   :63.87   Mean   : 0.1004  
##  3rd Qu.: 2.1270   3rd Qu.:3.876   3rd Qu.:68.00   3rd Qu.: 1.5581  
##  Max.   : 3.8210   Max.   :4.780   Max.   :79.00   Max.   : 2.3263  
##       svi              lcp             gleason          pgg45       
##  Min.   :0.0000   Min.   :-1.3863   Min.   :6.000   Min.   :  0.00  
##  1st Qu.:0.0000   1st Qu.:-1.3863   1st Qu.:6.000   1st Qu.:  0.00  
##  Median :0.0000   Median :-0.7985   Median :7.000   Median : 15.00  
##  Mean   :0.2165   Mean   :-0.1794   Mean   :6.753   Mean   : 24.38  
##  3rd Qu.:0.0000   3rd Qu.: 1.1787   3rd Qu.:7.000   3rd Qu.: 40.00  
##  Max.   :1.0000   Max.   : 2.9042   Max.   :9.000   Max.   :100.00  
##       lpsa           train        
##  Min.   :-0.4308   Mode :logical  
##  1st Qu.: 1.7317   FALSE:30       
##  Median : 2.5915   TRUE :67       
##  Mean   : 2.4784                  
##  3rd Qu.: 3.0564                  
##  Max.   : 5.5829</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>prostate <span class="ot">=</span> prostate <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span><span class="st">&quot;train&quot;</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(prostate, <span class="at">progress =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="lecture-14-fig/unnamed-chunk-1-1.png" /></p>
<p>From the <code>ggpairs</code> plot, we see that the distributions of
the variables are reasonably symmetrical, not that skewed, and that
there is at least some relationship between most of the variables and
<code>lpsa</code>.</p>
</div>
<div class="slide section level2">

<p>And a side note: this data set has already been transformed for you.
Many of the variables are logged versions of what were presumably the
raw measurements. This is actually a really important step: models with
the logged variables perform substantially better than models with the
raw variables, as we can see if we un-transform:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>prostate_unlogged <span class="ot">=</span> prostate <span class="sc">%&gt;%</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">cavol =</span> <span class="fu">exp</span>(lcavol), <span class="at">weight =</span> <span class="fu">exp</span>(lweight), <span class="at">bph =</span> <span class="fu">exp</span>(lbph), <span class="at">cp =</span> <span class="fu">exp</span>(lcp)) <span class="sc">%&gt;%</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span><span class="st">&quot;lcavol&quot;</span>, <span class="sc">-</span><span class="st">&quot;lweight&quot;</span>, <span class="sc">-</span><span class="st">&quot;lbph&quot;</span>, <span class="sc">-</span><span class="st">&quot;lcp&quot;</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(lpsa <span class="sc">~</span> ., <span class="at">data =</span> prostate))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = lpsa ~ ., data = prostate)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.76644 -0.35510 -0.00328  0.38087  1.55770 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.181561   1.320568   0.137  0.89096    
## lcavol       0.564341   0.087833   6.425 6.55e-09 ***
## lweight      0.622020   0.200897   3.096  0.00263 ** 
## age         -0.021248   0.011084  -1.917  0.05848 .  
## lbph         0.096713   0.057913   1.670  0.09848 .  
## svi          0.761673   0.241176   3.158  0.00218 ** 
## lcp         -0.106051   0.089868  -1.180  0.24115    
## gleason      0.049228   0.155341   0.317  0.75207    
## pgg45        0.004458   0.004365   1.021  0.31000    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6995 on 88 degrees of freedom
## Multiple R-squared:  0.6634, Adjusted R-squared:  0.6328 
## F-statistic: 21.68 on 8 and 88 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(lpsa <span class="sc">~</span> ., <span class="at">data =</span> prostate_unlogged))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = lpsa ~ ., data = prostate_unlogged)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.78876 -0.40705 -0.00634  0.46725  1.82792 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.759219   1.210444   0.627  0.53214    
## age         -0.008545   0.012087  -0.707  0.48144    
## svi          0.769166   0.270646   2.842  0.00557 ** 
## gleason      0.130183   0.166701   0.781  0.43694    
## pgg45        0.005412   0.004567   1.185  0.23927    
## cavol        0.073661   0.014479   5.087 2.03e-06 ***
## weight       0.012511   0.005322   2.351  0.02095 *  
## bph          0.056512   0.034547   1.636  0.10546    
## cp          -0.040047   0.034216  -1.170  0.24499    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.764 on 88 degrees of freedom
## Multiple R-squared:  0.5985, Adjusted R-squared:  0.562 
## F-statistic:  16.4 on 8 and 88 DF,  p-value: 1.329e-14</code></pre>
</div>
<div class="slide section level2">

<p>If we had started off with the raw data, we would have seen that we
should log-transform some of the variables by looking at their marginal
distributions: the variables that were transformed started off quite
skewed, and the transformation got rid of the skewness.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(prostate_unlogged, <span class="at">progress =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="lecture-14-fig/unnamed-chunk-3-1.png" /></p>
</div>
<div id="linear-model-with-all-the-predictors"
class="slide section level2">
<h1>Linear model with all the predictors</h1>
<p>As a first step, we can fit a linear model with all the predictors
and look at the results. We see that a lot of the coefficients are
within the margin of error of zero, which suggests to us that a model
with fewer predictors would do better.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>prostate_lm <span class="ot">=</span> <span class="fu">lm</span>(lpsa <span class="sc">~</span> ., <span class="at">data =</span> prostate)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>prostate_coefs <span class="ot">=</span> <span class="fu">tidy</span>(prostate_lm, <span class="at">conf.int =</span> <span class="cn">TRUE</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(prostate_coefs[<span class="sc">-</span><span class="dv">1</span>, ], <span class="fu">aes</span>(<span class="at">x =</span> estimate, <span class="at">y =</span> term, <span class="at">xmin =</span> conf.low, <span class="at">xmax =</span> conf.high)) <span class="sc">+</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_errorbarh</span>() <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="lecture-14-fig/unnamed-chunk-4-1.png" /></p>
</div>
<div id="automatic-ways-of-choosing-subsets-of-variables"
class="slide section level2">
<h1>Automatic ways of choosing subsets of variables</h1>
<p>There are a lot of ways of doing variable selection for linear
models.</p>
<ul>
<li>Forward stepwise regression: Predictors are added to the model one
at a time, stopping when adding a new predictor doesn’t seem to help.
Computationally efficient.</li>
</ul>
<ul>
<li>Backward stepwise regression: Predictors are added to or subtracted
from the model one at a time, stopping when subtracting an existing
predictor seems to hurt too much (for backward stepwise). Works if you
have fewer predictors than observations. Computationally efficient.</li>
</ul>
<ul>
<li>Best subsets: For every potential number of predictors used in the
model, find the combination of predictors that does the best.
Computationally intensive.</li>
</ul>
<p>With eight predictors and a laptop, we can actually just look through
all the subsets and see which model performs best.</p>
<p>This is what the <code>regsubsets</code> in the package
<code>leaps</code> does for you.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>prostate_best_subset <span class="ot">=</span> <span class="fu">regsubsets</span>(lpsa <span class="sc">~</span> ., <span class="at">data =</span> prostate)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(prostate_best_subset)<span class="sc">$</span>which</span></code></pre></div>
<pre><code>##   (Intercept) lcavol lweight   age  lbph   svi   lcp gleason pgg45
## 1        TRUE   TRUE   FALSE FALSE FALSE FALSE FALSE   FALSE FALSE
## 2        TRUE   TRUE    TRUE FALSE FALSE FALSE FALSE   FALSE FALSE
## 3        TRUE   TRUE    TRUE FALSE FALSE  TRUE FALSE   FALSE FALSE
## 4        TRUE   TRUE    TRUE FALSE  TRUE  TRUE FALSE   FALSE FALSE
## 5        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE   FALSE FALSE
## 6        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE   FALSE  TRUE
## 7        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE   FALSE  TRUE
## 8        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE    TRUE  TRUE</code></pre>
<p>This shows us that the best one-predictor model uses
<code>lcavol</code>, the best two-predictor model uses
<code>lcavol</code> and <code>lweight</code>, the best three-predictor
model uses <code>lcavol</code>, <code>lweight</code>, and
<code>svi</code>, and so on.</p>
<p>We can use this set of models as guidance for what variables to
include, and build up an interpretable model using some of the tools
we’ve seen earlier in the course.</p>
</div>
<div class="slide section level2">

<p><code>regsubsets</code> also does forward and backward stepwise
regression. In this case they all give the same results.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>prostate_forward <span class="ot">=</span> <span class="fu">regsubsets</span>(lpsa <span class="sc">~</span> ., <span class="at">data =</span> prostate, <span class="at">method =</span> <span class="st">&quot;forward&quot;</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(prostate_forward)<span class="sc">$</span>which</span></code></pre></div>
<pre><code>##   (Intercept) lcavol lweight   age  lbph   svi   lcp gleason pgg45
## 1        TRUE   TRUE   FALSE FALSE FALSE FALSE FALSE   FALSE FALSE
## 2        TRUE   TRUE    TRUE FALSE FALSE FALSE FALSE   FALSE FALSE
## 3        TRUE   TRUE    TRUE FALSE FALSE  TRUE FALSE   FALSE FALSE
## 4        TRUE   TRUE    TRUE FALSE  TRUE  TRUE FALSE   FALSE FALSE
## 5        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE   FALSE FALSE
## 6        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE   FALSE  TRUE
## 7        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE   FALSE  TRUE
## 8        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE    TRUE  TRUE</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>prostate_backward <span class="ot">=</span> <span class="fu">regsubsets</span>(lpsa <span class="sc">~</span> ., <span class="at">data =</span> prostate, <span class="at">method =</span> <span class="st">&quot;backward&quot;</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(prostate_backward)<span class="sc">$</span>which</span></code></pre></div>
<pre><code>##   (Intercept) lcavol lweight   age  lbph   svi   lcp gleason pgg45
## 1        TRUE   TRUE   FALSE FALSE FALSE FALSE FALSE   FALSE FALSE
## 2        TRUE   TRUE    TRUE FALSE FALSE FALSE FALSE   FALSE FALSE
## 3        TRUE   TRUE    TRUE FALSE FALSE  TRUE FALSE   FALSE FALSE
## 4        TRUE   TRUE    TRUE FALSE  TRUE  TRUE FALSE   FALSE FALSE
## 5        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE   FALSE FALSE
## 6        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE   FALSE  TRUE
## 7        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE   FALSE  TRUE
## 8        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE    TRUE  TRUE</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">all</span>(<span class="fu">summary</span>(prostate_backward)<span class="sc">$</span>which <span class="sc">==</span> <span class="fu">summary</span>(prostate_forward)<span class="sc">$</span>which)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">all</span>(<span class="fu">summary</span>(prostate_backward)<span class="sc">$</span>which <span class="sc">==</span> <span class="fu">summary</span>(prostate_best_subset)<span class="sc">$</span>which)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div class="slide section level2">

<p>We start off looking at the best one-predictor model, with
<code>lcavol</code> predicting <code>lpsa</code>.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(prostate, <span class="fu">aes</span>(<span class="at">x =</span> lcavol, <span class="at">y =</span> lpsa)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39;</code></pre>
<p><img src="lecture-14-fig/unnamed-chunk-7-1.png" /></p>
<p>We see that the relationship is pretty close to linear (a line would
go through the entire confidence band of the smoother), and so we’re ok
with using a linear function of <code>lcavol</code> to predict
<code>lpsa</code>.</p>
<p>If there had been a major non-linearity here, we would have wanted to
ditch the linear modeling approach and do something non-parametric,
maybe loess.</p>
</div>
<div class="slide section level2">

<p>Then we can move to the two-predictor model, and see what the
relationship between <code>lcavol</code>, <code>lweight</code>, and
<code>lpsa</code> looks like.</p>
<p>We can make a coplot to examine the relationship between
<code>lpsa</code> and <code>lcavol</code>, with <code>lweight</code>as
the given variable</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(prostate, <span class="fu">aes</span>(<span class="at">x =</span> lcavol, <span class="at">y =</span> lpsa)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">span =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_grid</span>(<span class="sc">~</span><span class="fu">cut_number</span>(lweight, <span class="at">n =</span> <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39;</code></pre>
<p><img src="lecture-14-fig/unnamed-chunk-8-1.png" /></p>
<p>Here it seems like there is some non-linearity, but only for the
observations with a high value of <code>lweight</code>.</p>
<p>If we had a lot more observations, this might prompt us to move to
loess, but since the non-linearity is based on just a few points, we
want to see first whether the non-linearity shows up in other graphs as
well.</p>
</div>
<div class="slide section level2">

<p>The best three-predictor model identified by <code>leaps</code>
included <code>svi</code> in addition to <code>lcavol</code> and
<code>lweight</code>, so we next look at those four variables
together.</p>
<p><code>svi</code> is binary and there are only 21 cases where
<code>svi</code> is equal to 1, we can’t make a lot of facets and we
don’t trust curves that much.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(prostate<span class="sc">$</span>svi)</span></code></pre></div>
<pre><code>## 
##  0  1 
## 76 21</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>prostate<span class="sc">$</span>svi <span class="ot">=</span> <span class="fu">recode</span>(prostate<span class="sc">$</span>svi, <span class="st">`</span><span class="at">1</span><span class="st">`</span> <span class="ot">=</span> <span class="st">&quot;Yes&quot;</span>, <span class="st">`</span><span class="at">0</span><span class="st">`</span> <span class="ot">=</span> <span class="st">&quot;No&quot;</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(prostate, <span class="fu">aes</span>(<span class="at">x =</span> lcavol, <span class="at">y =</span> lpsa, <span class="at">group =</span> svi, <span class="at">color =</span> svi)) <span class="sc">+</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> <span class="fu">facet_wrap</span>(<span class="sc">~</span> <span class="fu">cut_number</span>(lweight, <span class="at">n =</span> <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="lecture-14-fig/unnamed-chunk-9-1.png" /></p>
<p>The different slopes suggest an interaction, but again, we don’t have
that much data.</p>
</div>
<div class="slide section level2">

<p>Next we try looking at the predictors from the four-predictor model.
This is pushing the limits of the number of variables we can look at all
at once, but we will try.</p>
<p>We can look at the relationship between <code>lpsa</code>,
<code>lcavol</code>, and <code>svi</code>, with <code>lweight</code> and
<code>lbph</code> as the given variables.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(prostate, <span class="fu">aes</span>(<span class="at">x =</span> lcavol, <span class="at">y =</span> lpsa, <span class="at">group =</span> svi, <span class="at">color =</span> svi)) <span class="sc">+</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        <span class="fu">facet_grid</span>(<span class="fu">cut_number</span>(lweight, <span class="at">n =</span> <span class="dv">2</span>) <span class="sc">~</span> <span class="fu">cut</span>(lbph, <span class="at">breaks =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>)))</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="lecture-14-fig/unnamed-chunk-10-1.png" /></p>
<p>Based on this visualization, we might not be confident about an
interaction: the blue lines vary in slope, but there’s based on very
small samples. The red lines have different heights but are similar in
slope.</p>
</div>
<div class="slide section level2">

<p>Automatic ways of choosing model sizes:</p>
<ul>
<li><a
href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a>:
<span class="math inline">\(2k - 2 \log(\hat L)\)</span>, where <span
class="math inline">\(k\)</span> is the number of parameters in the
model and <span class="math inline">\(\hat L\)</span> is the likelihood
at the fitted parameter values.</li>
</ul>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Mallows%27s_Cp">Mallows’
Cp</a>: Equivalent to AIC for linear regression.</li>
</ul>
<ul>
<li><a
href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a>:
<span class="math inline">\(k \log n - 2 \log(\hat L)\)</span>, where
<span class="math inline">\(k\)</span> is the number of parameters,
<span class="math inline">\(n\)</span> is the number of observations,
and <span class="math inline">\(\hat L\)</span> is the likelihood at the
fitted parameter values.</li>
</ul>
<p>With each of these, we choose the model with the lowest value of the
selection criterion.</p>
</div>
<div class="slide section level2">

<p>In EDA we’re not always required to find a “best” model, and even if
we were we can decide on what best means subjectively.</p>
<p>If you wanted to fit a linear model with <code>lcavol</code>,
<code>lweight</code>, and <code>svi</code> as predictors plus
interactions, you’re free to do so and then call that “best” because of
the complexity you can get out of a relatively small number of
variables.</p>
<p>We can also check what the model selection criteria tell us about how
many variables to use:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>cp_df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">value  =</span> <span class="fu">summary</span>(prostate_best_subset)<span class="sc">$</span>cp,</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">n_params =</span> <span class="fu">seq_along</span>(<span class="fu">summary</span>(prostate_best_subset)<span class="sc">$</span>cp),</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">type =</span> <span class="st">&quot;Cp&quot;</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>bic_df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">value =</span> <span class="fu">summary</span>(prostate_best_subset)<span class="sc">$</span>bic,</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">n_params =</span> <span class="fu">seq_along</span>(<span class="fu">summary</span>(prostate_best_subset)<span class="sc">$</span>bic),</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">type =</span> <span class="st">&quot;BIC&quot;</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>model_selection_criterion_df <span class="ot">=</span> <span class="fu">rbind</span>(cp_df, bic_df)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(model_selection_criterion_df, <span class="fu">aes</span>(<span class="at">x =</span> n_params, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">facet_wrap</span>(<span class="sc">~</span> type, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>)</span></code></pre></div>
<p><img src="lecture-14-fig/unnamed-chunk-11-1.png" /></p>
<p>Even though the lowest value of Cp is for 5 predictors (intercept + 4
variables), the Cp value for 3 predictors is very close to the value for
5 predictors and BIC has a strong minimum at 5 predictors. The two plots
suggest that if we don’t believe we need any interactions, we should use
a model with an intercept and two predictors.</p>
</div>
<div id="final-model" class="slide section level2">
<h1>Final model</h1>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>lm_final <span class="ot">=</span> <span class="fu">lm</span>(lpsa <span class="sc">~</span> lcavol <span class="sc">+</span> lweight, <span class="at">data =</span> prostate)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>final_fits <span class="ot">=</span> <span class="fu">augment</span>(lm_final)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>prostate_with_lweight_bins <span class="ot">=</span> prostate <span class="sc">%&gt;%</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">lweight_bins =</span> <span class="fu">cut_number</span>(lweight, <span class="at">n =</span> <span class="dv">3</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">separate</span>(lweight_bins, <span class="at">into =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="st">&quot;lo&quot;</span>, <span class="st">&quot;hi&quot;</span>, <span class="cn">NA</span>), <span class="at">remove =</span> <span class="cn">FALSE</span>, <span class="at">sep =</span> <span class="st">&quot;</span><span class="sc">\\</span><span class="st">[|</span><span class="sc">\\</span><span class="st">(|</span><span class="sc">\\</span><span class="st">)|</span><span class="sc">\\</span><span class="st">]|,&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">bin_mean =</span> (<span class="fu">as.numeric</span>(lo) <span class="sc">+</span> <span class="fu">as.numeric</span>(hi)) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>prostate_grid_final <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">lweight =</span> <span class="fu">unique</span>(prostate_with_lweight_bins<span class="sc">$</span>bin_mean),</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lcavol =</span> <span class="fu">quantile</span>(prostate<span class="sc">$</span>lcavol)) <span class="sc">%&gt;%</span> <span class="fu">data.frame</span>()</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>prostate_fits_on_grid <span class="ot">=</span> <span class="fu">augment</span>(lm_final, <span class="at">newdata =</span> prostate_grid_final)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>prostate_fits_on_grid <span class="ot">=</span> <span class="fu">merge</span>(prostate_fits_on_grid,</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">unique</span>(prostate_with_lweight_bins[,<span class="fu">c</span>(<span class="st">&quot;lweight_bins&quot;</span>, <span class="st">&quot;bin_mean&quot;</span>)]),</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>                             <span class="at">by.x =</span> <span class="st">&quot;lweight&quot;</span>, <span class="at">by.y =</span> <span class="st">&quot;bin_mean&quot;</span>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(prostate_with_lweight_bins, <span class="fu">aes</span>(<span class="at">x =</span> lcavol, <span class="at">y =</span> lpsa, <span class="at">color =</span> lweight_bins)) <span class="sc">+</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> .fitted), <span class="at">data =</span> prostate_fits_on_grid) <span class="sc">+</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Log Cancer Volume&quot;</span>) <span class="sc">+</span> <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Log PSA&quot;</span>) <span class="sc">+</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_discrete</span>(<span class="st">&quot;Log Weight&quot;</span>)</span></code></pre></div>
<p><img src="lecture-14-fig/unnamed-chunk-12-1.png" /></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(prostate_with_lweight_bins, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">exp</span>(lcavol), <span class="at">y =</span> <span class="fu">exp</span>(lpsa), <span class="at">color =</span> lweight_bins)) <span class="sc">+</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">exp</span>(.fitted)), <span class="at">data =</span> prostate_fits_on_grid) <span class="sc">+</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Cancer Volume&quot;</span>) <span class="sc">+</span> <span class="fu">scale_y_continuous</span>(<span class="st">&quot;PSA&quot;</span>) <span class="sc">+</span> <span class="fu">scale_color_discrete</span>(<span class="st">&quot;Log Weight&quot;</span>)</span></code></pre></div>
<p><img src="lecture-14-fig/unnamed-chunk-12-2.png" /></p>
</div>
<div id="overall" class="slide section level2">
<h1>Overall</h1>
<ul>
<li>There are a couple of automatic ways of choosing the best subsets of
predictors and the best model sizes.</li>
</ul>
<ul>
<li>These can be used automatically, or they can be used to guide the
order in which we investigate whether and how to include variables.</li>
</ul>
<ul>
<li>Transformations of the predictors are very important.</li>
</ul>
<ul>
<li>Best subsets/forward stepwise/backward stepwise don’t include
variables in the same order as the marginal correlations between the
predictors and response. Don’t just rely on correlations to decide on
which variables to include!</li>
</ul>
</div>
</body>
</html>
